{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "lab4_solutions.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/annnnnnnnnnie/ICMachineLearningLab/blob/master/lab4_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MBmOdXoenFw"
   },
   "source": [
    "# Lab 4: Simple Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3iS23ADfo4w"
   },
   "source": [
    "## Version history\n",
    "\n",
    "| Date | Author | Description |\n",
    "|:----:|:------:|:------------|\n",
    "2021-02-03 | Josiah Wang | First version | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EbbkgqOgZK_"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this lab exercise is for you to gain some experience implementing and training a simple linear regression model from scratch. This will help you improve your understanding of linear regression and machine learning optimisation. \n",
    "\n",
    "By the end of this lab exercise, you will have \n",
    "- implemented a simple linear regression model \n",
    "- defined and implemented a loss function\n",
    "- optimised the parameters of your model using gradient descent\n",
    "\n",
    "There will be a bit less coding required on your side in this exercise compared to previous exercises. The aim is for you to try to really understand linear regression at implementation level to complement the lectures, which will help you in future weeks as you move on to Neural Networks in your coursework assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0iaMjnIWEpe"
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "In this tutorial, we will focus on the **regression task**. For simplicity, we will implement a *simple linear regression* model with one input variable and one output variable. More specifically, our task is to predict the value of $y$ given the input $x$.\n",
    "\n",
    "Let us develop our simple linear regressor with a simple toy example to make sure that our model works correctly. You can later apply it to a bigger dataset if desired."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "4w2N6AZsYJUd",
    "outputId": "c89729a9-2ad9-4798-ed18-3545b030f651"
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# toy dataset\n",
    "x_train = np.array([1.0, 1.2, 2.0, 3.5, 4.0, 5.0])\n",
    "y_train = np.array([3.1, 3.5, 5.0, 7.9, 9.1, 10.9])\n",
    "x_test = np.array([2.5, 3.0, 4.5])\n",
    "y_test = np.array([6.0, 7.0, 10.1])\n",
    "\n",
    "# plot toy data\n",
    "plt.scatter(x_train, y_train, c=\"blue\")\n",
    "plt.scatter(x_test, y_test, c=\"red\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YagIkbxvaC-b"
   },
   "source": [
    "### Model\n",
    "\n",
    "As you should be able to see from the plot, the toy dataset can almost be perfectly modelled with a straight line. The model should also be able to pretty accurately predict the value of $y$ of the test set.\n",
    "\n",
    "Assuming you still remember your high school Maths, a straight line is represented as $y = wx + b$, where $w$ is the slope and $b$ the intercept/bias. Our objective is to find the line that best fits our training data. More specifically, we want our regressor to automatically learn the parameters $w$ and $b$ such that we can accurately predict the real-valued label ${\\hat y}$ given an example $x$. The objective is to get ${\\hat y}$ to be as close as possible to the values of $y$ of the training data (and presumably the true $y$).\n",
    "\n",
    "Let us now build our simple linear regression model. Complete the `forward()` method of the `SimpleLinearRegression` class below to return the value of the output $y$ given an input `x` and the current weight `w` and bias `b` of the model. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLNpOSjXUQrY",
    "outputId": "6db3b496-66fc-4234-d412-5f882dc59879"
   },
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "class SimpleLinearRegression:\n",
    "    def __init__(self, random_generator=default_rng()):\n",
    "        # initialise the slope with a random value drawn from a standard normal \n",
    "        # distribution (mean=0, stddev=1)\n",
    "        self.w = random_generator.standard_normal()\n",
    "\n",
    "        # initialise bias to 0 \n",
    "        self.b = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Perform forward pass given an input x\n",
    "\n",
    "        Args:\n",
    "            x (float): input instance\n",
    "\n",
    "        Returns:\n",
    "            float: the output of the model given the current weights\n",
    "        \"\"\"\n",
    "\n",
    "        return self.w * x + self.b\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        \"\"\" Placeholder for later\"\"\"\n",
    "        pass\n",
    "\n",
    "    def gradient(self, x, y):\n",
    "        \"\"\" Placeholder for later\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "## Quick test: This should return 8\n",
    "model = SimpleLinearRegression()\n",
    "model.w = 3\n",
    "model.b = 2\n",
    "x = 2\n",
    "y_hat = model.forward(x)\n",
    "print(y_hat) # should print 8"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9mtsqFXBNNa"
   },
   "source": [
    "### Loss function\n",
    "\n",
    "From the plot and the numbers, you may have manually worked out that the 'best' line would be $\\hat{y} \\approx 2x+1$, that is, the optimal parameter values are $w \\approx 2$ and $b \\approx 1$.\n",
    "\n",
    "What constitutes the 'best' line? We will first have to define what 'best' actually means. Intuitively, the 'best' line would be the one that goes through all training points as closely as possible.\n",
    "\n",
    "To enable our model to automatically learn what the parameter values of the 'best' line are from training examples, we will have to formally define that we mean by 'best'. We define this via a *loss function* (or cost function). For this tutorial, we will use the loss function as in the lectures - the **sum of squared differences** between the predicted label vs. the ground truth label across the training instances. \n",
    "\n",
    "$$L = \\frac{1}{2} \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 = \\frac{1}{2} \\sum_{i=1}^{N} \\left(wx^{(i)} + b - y^{(i)}\\right)^2$$\n",
    "\n",
    "Our objective is to select the parameters $\\theta = \\{ w, b \\}$ that **minimise** the loss (or error).\n",
    "\n",
    "$$\\theta = argmin_{\\theta} \\, L$$\n",
    "\n",
    "To make things easy, let us implement the loss function for a **single** instance $x$:\n",
    "\n",
    "$$L^{(i)} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$$\n",
    "\n",
    "Complete the `loss()` method of `SimpleLinearRegression` below to return the individual loss for an instance `x`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZM0MAgiE2UZ",
    "outputId": "30f00efd-aeda-440f-fcf1-e1955ba39d73"
   },
   "source": [
    "# Loss method for SimpleLinearRegression\n",
    "def loss(self, x, y):\n",
    "    \"\"\" Compute the loss for an input x\n",
    "\n",
    "    Args:\n",
    "        x (float): input instance\n",
    "        y (float): ground truth output\n",
    "\n",
    "    Returns:\n",
    "        float: the model loss for an instance x\n",
    "    \"\"\"\n",
    "\n",
    "    y_hat = self.forward(x)\n",
    "    return (y_hat - y)**2\n",
    "\n",
    "\n",
    "# A quick hack to bind this function as the SimpleLinearRegression.loss() method\n",
    "SimpleLinearRegression.loss = loss\n",
    "\n",
    "\n",
    "## Quick test: This should return 0.25\n",
    "model = SimpleLinearRegression()\n",
    "model.w = 3\n",
    "model.b = 2\n",
    "x = 2.0\n",
    "y = 8.5\n",
    "test_loss = model.loss(x, y)\n",
    "print(test_loss) # should print 0.25"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxwg9EPiGu2l"
   },
   "source": [
    "### Optimisation by brute force search\n",
    "\n",
    "Now, how do we get the model to automatically figure out the optimal parameters from training data? Remember that the optimal parameter values are the ones that minimise the loss function. A naive approach would be to compute the loss for different combinations of $w$ and $b$ and selecting the combination that results in the smallest loss.\n",
    "\n",
    "The code below will search for $w$ between $0$ and $4$, and for $b$ between $0$ and $2$ to find the best combination of $w$ and $b$. Examine the code, and try to understand what it is doing, then run the code."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyV8OCZSIdrj",
    "outputId": "823e15e6-9ec5-4fc3-f146-b11a760f86e1"
   },
   "source": [
    "model = SimpleLinearRegression()\n",
    "\n",
    "# to store all losses for later use\n",
    "losses = []\n",
    "\n",
    "# the parameters to search\n",
    "weights = np.arange(0, 4.1, 0.2) \n",
    "biases = np.arange(0, 2.1, 0.2)\n",
    "\n",
    "# for storing the loss in a matrix for visualisation later\n",
    "loss_matrix = np.zeros((len(weights), len(biases)))\n",
    "\n",
    "# compute loss for each (w,b) combination\n",
    "for i, w in enumerate(weights):\n",
    "    for j, b in enumerate(biases):\n",
    "        print(f\"(w={w:.1f}, b={b:.1f})\")\n",
    "        \n",
    "        # setup weights of model\n",
    "        model.w = w\n",
    "        model.b = b\n",
    "\n",
    "        sum_loss = 0\n",
    "        # for each example\n",
    "        for (x, y) in zip(x_train, y_train):\n",
    "            # compute the loss for this example\n",
    "            single_loss = model.loss(x, y)\n",
    "\n",
    "            # and add it to the sum\n",
    "            sum_loss += single_loss\n",
    "\n",
    "            # print out the values just to make sure everything is working correctly\n",
    "            y_hat = model.forward(x)\n",
    "            print(f\"    x: {x}, y: {y}, y_hat: {y_hat:.1f}, loss: {single_loss:.2f}\")\n",
    "\n",
    "        # print out the sum of individual losses\n",
    "        # I multiplied by 0.5 to be consistent with the equation earlier, \n",
    "        # but this is not necessary in practice as this is a constant\n",
    "        print(f\"    Loss = {(0.5 * sum_loss):.4f}\\n\")\n",
    "\n",
    "        # store the losses and the corresponding (w,b) for later use\n",
    "        losses.append((0.5*sum_loss, w, b))\n",
    "\n",
    "        # store the losses in a matrix form for visualisation later\n",
    "        loss_matrix[i,j] = 0.5 * sum_loss\n",
    "\n",
    "# find combination with minimum loss\n",
    "(min_loss, best_w, best_b) = min(losses, key=lambda x:x[0])\n",
    "print(\"BEST:\")\n",
    "print(f\"w={best_w}, b={best_b}, loss={min_loss:.4f}\")"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(w=0.0, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.0, loss: 9.61\n",
      "    x: 1.2, y: 3.5, y_hat: 0.0, loss: 12.25\n",
      "    x: 2.0, y: 5.0, y_hat: 0.0, loss: 25.00\n",
      "    x: 3.5, y: 7.9, y_hat: 0.0, loss: 62.41\n",
      "    x: 4.0, y: 9.1, y_hat: 0.0, loss: 82.81\n",
      "    x: 5.0, y: 10.9, y_hat: 0.0, loss: 118.81\n",
      "    Loss = 155.4450\n",
      "\n",
      "(w=0.0, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.2, loss: 8.41\n",
      "    x: 1.2, y: 3.5, y_hat: 0.2, loss: 10.89\n",
      "    x: 2.0, y: 5.0, y_hat: 0.2, loss: 23.04\n",
      "    x: 3.5, y: 7.9, y_hat: 0.2, loss: 59.29\n",
      "    x: 4.0, y: 9.1, y_hat: 0.2, loss: 79.21\n",
      "    x: 5.0, y: 10.9, y_hat: 0.2, loss: 114.49\n",
      "    Loss = 147.6650\n",
      "\n",
      "(w=0.0, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.4, loss: 9.61\n",
      "    x: 2.0, y: 5.0, y_hat: 0.4, loss: 21.16\n",
      "    x: 3.5, y: 7.9, y_hat: 0.4, loss: 56.25\n",
      "    x: 4.0, y: 9.1, y_hat: 0.4, loss: 75.69\n",
      "    x: 5.0, y: 10.9, y_hat: 0.4, loss: 110.25\n",
      "    Loss = 140.1250\n",
      "\n",
      "(w=0.0, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 0.6, loss: 8.41\n",
      "    x: 2.0, y: 5.0, y_hat: 0.6, loss: 19.36\n",
      "    x: 3.5, y: 7.9, y_hat: 0.6, loss: 53.29\n",
      "    x: 4.0, y: 9.1, y_hat: 0.6, loss: 72.25\n",
      "    x: 5.0, y: 10.9, y_hat: 0.6, loss: 106.09\n",
      "    Loss = 132.8250\n",
      "\n",
      "(w=0.0, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.8, loss: 7.29\n",
      "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
      "    x: 3.5, y: 7.9, y_hat: 0.8, loss: 50.41\n",
      "    x: 4.0, y: 9.1, y_hat: 0.8, loss: 68.89\n",
      "    x: 5.0, y: 10.9, y_hat: 0.8, loss: 102.01\n",
      "    Loss = 125.7650\n",
      "\n",
      "(w=0.0, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.25\n",
      "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
      "    x: 3.5, y: 7.9, y_hat: 1.0, loss: 47.61\n",
      "    x: 4.0, y: 9.1, y_hat: 1.0, loss: 65.61\n",
      "    x: 5.0, y: 10.9, y_hat: 1.0, loss: 98.01\n",
      "    Loss = 118.9450\n",
      "\n",
      "(w=0.0, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.29\n",
      "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 1.2, loss: 44.89\n",
      "    x: 4.0, y: 9.1, y_hat: 1.2, loss: 62.41\n",
      "    x: 5.0, y: 10.9, y_hat: 1.2, loss: 94.09\n",
      "    Loss = 112.3650\n",
      "\n",
      "(w=0.0, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.41\n",
      "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 1.4, loss: 42.25\n",
      "    x: 4.0, y: 9.1, y_hat: 1.4, loss: 59.29\n",
      "    x: 5.0, y: 10.9, y_hat: 1.4, loss: 90.25\n",
      "    Loss = 106.0250\n",
      "\n",
      "(w=0.0, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.61\n",
      "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 1.6, loss: 39.69\n",
      "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
      "    x: 5.0, y: 10.9, y_hat: 1.6, loss: 86.49\n",
      "    Loss = 99.9250\n",
      "\n",
      "(w=0.0, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.89\n",
      "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 1.8, loss: 37.21\n",
      "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
      "    x: 5.0, y: 10.9, y_hat: 1.8, loss: 82.81\n",
      "    Loss = 94.0650\n",
      "\n",
      "(w=0.0, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.25\n",
      "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 2.0, loss: 34.81\n",
      "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
      "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
      "    Loss = 88.4450\n",
      "\n",
      "(w=0.2, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.2, loss: 8.41\n",
      "    x: 1.2, y: 3.5, y_hat: 0.2, loss: 10.63\n",
      "    x: 2.0, y: 5.0, y_hat: 0.4, loss: 21.16\n",
      "    x: 3.5, y: 7.9, y_hat: 0.7, loss: 51.84\n",
      "    x: 4.0, y: 9.1, y_hat: 0.8, loss: 68.89\n",
      "    x: 5.0, y: 10.9, y_hat: 1.0, loss: 98.01\n",
      "    Loss = 129.4688\n",
      "\n",
      "(w=0.2, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.4, loss: 9.36\n",
      "    x: 2.0, y: 5.0, y_hat: 0.6, loss: 19.36\n",
      "    x: 3.5, y: 7.9, y_hat: 0.9, loss: 49.00\n",
      "    x: 4.0, y: 9.1, y_hat: 1.0, loss: 65.61\n",
      "    x: 5.0, y: 10.9, y_hat: 1.2, loss: 94.09\n",
      "    Loss = 122.3568\n",
      "\n",
      "(w=0.2, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 0.6, loss: 8.18\n",
      "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
      "    x: 3.5, y: 7.9, y_hat: 1.1, loss: 46.24\n",
      "    x: 4.0, y: 9.1, y_hat: 1.2, loss: 62.41\n",
      "    x: 5.0, y: 10.9, y_hat: 1.4, loss: 90.25\n",
      "    Loss = 115.4848\n",
      "\n",
      "(w=0.2, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.8, loss: 7.08\n",
      "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
      "    x: 3.5, y: 7.9, y_hat: 1.3, loss: 43.56\n",
      "    x: 4.0, y: 9.1, y_hat: 1.4, loss: 59.29\n",
      "    x: 5.0, y: 10.9, y_hat: 1.6, loss: 86.49\n",
      "    Loss = 108.8528\n",
      "\n",
      "(w=0.2, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.05\n",
      "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 1.5, loss: 40.96\n",
      "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
      "    x: 5.0, y: 10.9, y_hat: 1.8, loss: 82.81\n",
      "    Loss = 102.4608\n",
      "\n",
      "(w=0.2, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.11\n",
      "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 1.7, loss: 38.44\n",
      "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
      "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
      "    Loss = 96.3088\n",
      "\n",
      "(w=0.2, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.24\n",
      "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 1.9, loss: 36.00\n",
      "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
      "    x: 5.0, y: 10.9, y_hat: 2.2, loss: 75.69\n",
      "    Loss = 90.3968\n",
      "\n",
      "(w=0.2, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.46\n",
      "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 2.1, loss: 33.64\n",
      "    x: 4.0, y: 9.1, y_hat: 2.2, loss: 47.61\n",
      "    x: 5.0, y: 10.9, y_hat: 2.4, loss: 72.25\n",
      "    Loss = 84.7248\n",
      "\n",
      "(w=0.2, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.76\n",
      "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 2.3, loss: 31.36\n",
      "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
      "    x: 5.0, y: 10.9, y_hat: 2.6, loss: 68.89\n",
      "    Loss = 79.2928\n",
      "\n",
      "(w=0.2, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.13\n",
      "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 2.5, loss: 29.16\n",
      "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
      "    x: 5.0, y: 10.9, y_hat: 2.8, loss: 65.61\n",
      "    Loss = 74.1008\n",
      "\n",
      "(w=0.2, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.59\n",
      "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 2.7, loss: 27.04\n",
      "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
      "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
      "    Loss = 69.1488\n",
      "\n",
      "(w=0.4, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.5, loss: 9.12\n",
      "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
      "    x: 3.5, y: 7.9, y_hat: 1.4, loss: 42.25\n",
      "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
      "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
      "    Loss = 105.8802\n",
      "\n",
      "(w=0.4, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 0.7, loss: 7.95\n",
      "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
      "    x: 3.5, y: 7.9, y_hat: 1.6, loss: 39.69\n",
      "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
      "    x: 5.0, y: 10.9, y_hat: 2.2, loss: 75.69\n",
      "    Loss = 99.4362\n",
      "\n",
      "(w=0.4, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.9, loss: 6.86\n",
      "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 1.8, loss: 37.21\n",
      "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
      "    x: 5.0, y: 10.9, y_hat: 2.4, loss: 72.25\n",
      "    Loss = 93.2322\n",
      "\n",
      "(w=0.4, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 1.1, loss: 5.86\n",
      "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 2.0, loss: 34.81\n",
      "    x: 4.0, y: 9.1, y_hat: 2.2, loss: 47.61\n",
      "    x: 5.0, y: 10.9, y_hat: 2.6, loss: 68.89\n",
      "    Loss = 87.2682\n",
      "\n",
      "(w=0.4, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.3, loss: 4.93\n",
      "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 2.2, loss: 32.49\n",
      "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
      "    x: 5.0, y: 10.9, y_hat: 2.8, loss: 65.61\n",
      "    Loss = 81.5442\n",
      "\n",
      "(w=0.4, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.5, loss: 4.08\n",
      "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 2.4, loss: 30.25\n",
      "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
      "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
      "    Loss = 76.0602\n",
      "\n",
      "(w=0.4, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.31\n",
      "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 2.6, loss: 28.09\n",
      "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
      "    x: 5.0, y: 10.9, y_hat: 3.2, loss: 59.29\n",
      "    Loss = 70.8162\n",
      "\n",
      "(w=0.4, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.62\n",
      "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 2.8, loss: 26.01\n",
      "    x: 4.0, y: 9.1, y_hat: 3.0, loss: 37.21\n",
      "    x: 5.0, y: 10.9, y_hat: 3.4, loss: 56.25\n",
      "    Loss = 65.8122\n",
      "\n",
      "(w=0.4, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 2.02\n",
      "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.0, loss: 24.01\n",
      "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
      "    x: 5.0, y: 10.9, y_hat: 3.6, loss: 53.29\n",
      "    Loss = 61.0482\n",
      "\n",
      "(w=0.4, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.49\n",
      "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.2, loss: 22.09\n",
      "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
      "    x: 5.0, y: 10.9, y_hat: 3.8, loss: 50.41\n",
      "    Loss = 56.5242\n",
      "\n",
      "(w=0.4, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 1.04\n",
      "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 3.4, loss: 20.25\n",
      "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
      "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
      "    Loss = 52.2402\n",
      "\n",
      "(w=0.6, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 0.7, loss: 7.73\n",
      "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 2.1, loss: 33.64\n",
      "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
      "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
      "    Loss = 84.6792\n",
      "\n",
      "(w=0.6, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.9, loss: 6.66\n",
      "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 2.3, loss: 31.36\n",
      "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
      "    x: 5.0, y: 10.9, y_hat: 3.2, loss: 59.29\n",
      "    Loss = 78.9032\n",
      "\n",
      "(w=0.6, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 1.1, loss: 5.66\n",
      "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 2.5, loss: 29.16\n",
      "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
      "    x: 5.0, y: 10.9, y_hat: 3.4, loss: 56.25\n",
      "    Loss = 73.3672\n",
      "\n",
      "(w=0.6, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.3, loss: 4.75\n",
      "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 2.7, loss: 27.04\n",
      "    x: 4.0, y: 9.1, y_hat: 3.0, loss: 37.21\n",
      "    x: 5.0, y: 10.9, y_hat: 3.6, loss: 53.29\n",
      "    Loss = 68.0712\n",
      "\n",
      "(w=0.6, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.5, loss: 3.92\n",
      "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 2.9, loss: 25.00\n",
      "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
      "    x: 5.0, y: 10.9, y_hat: 3.8, loss: 50.41\n",
      "    Loss = 63.0152\n",
      "\n",
      "(w=0.6, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.17\n",
      "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 3.1, loss: 23.04\n",
      "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
      "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
      "    Loss = 58.1992\n",
      "\n",
      "(w=0.6, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.50\n",
      "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.3, loss: 21.16\n",
      "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
      "    x: 5.0, y: 10.9, y_hat: 4.2, loss: 44.89\n",
      "    Loss = 53.6232\n",
      "\n",
      "(w=0.6, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 1.90\n",
      "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.5, loss: 19.36\n",
      "    x: 4.0, y: 9.1, y_hat: 3.8, loss: 28.09\n",
      "    x: 5.0, y: 10.9, y_hat: 4.4, loss: 42.25\n",
      "    Loss = 49.2872\n",
      "\n",
      "(w=0.6, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.39\n",
      "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 3.7, loss: 17.64\n",
      "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
      "    x: 5.0, y: 10.9, y_hat: 4.6, loss: 39.69\n",
      "    Loss = 45.1912\n",
      "\n",
      "(w=0.6, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 0.96\n",
      "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 3.9, loss: 16.00\n",
      "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
      "    x: 5.0, y: 10.9, y_hat: 4.8, loss: 37.21\n",
      "    Loss = 41.3352\n",
      "\n",
      "(w=0.6, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.61\n",
      "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 4.1, loss: 14.44\n",
      "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
      "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
      "    Loss = 37.7192\n",
      "\n",
      "(w=0.8, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.45\n",
      "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 2.8, loss: 26.01\n",
      "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
      "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
      "    Loss = 65.8658\n",
      "\n",
      "(w=0.8, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.48\n",
      "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 3.0, loss: 24.01\n",
      "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
      "    x: 5.0, y: 10.9, y_hat: 4.2, loss: 44.89\n",
      "    Loss = 60.7578\n",
      "\n",
      "(w=0.8, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.58\n",
      "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 3.2, loss: 22.09\n",
      "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
      "    x: 5.0, y: 10.9, y_hat: 4.4, loss: 42.25\n",
      "    Loss = 55.8898\n",
      "\n",
      "(w=0.8, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.76\n",
      "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 3.4, loss: 20.25\n",
      "    x: 4.0, y: 9.1, y_hat: 3.8, loss: 28.09\n",
      "    x: 5.0, y: 10.9, y_hat: 4.6, loss: 39.69\n",
      "    Loss = 51.2618\n",
      "\n",
      "(w=0.8, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 3.03\n",
      "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.6, loss: 18.49\n",
      "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
      "    x: 5.0, y: 10.9, y_hat: 4.8, loss: 37.21\n",
      "    Loss = 46.8738\n",
      "\n",
      "(w=0.8, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.37\n",
      "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.8, loss: 16.81\n",
      "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
      "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
      "    Loss = 42.7258\n",
      "\n",
      "(w=0.8, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.80\n",
      "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 4.0, loss: 15.21\n",
      "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
      "    x: 5.0, y: 10.9, y_hat: 5.2, loss: 32.49\n",
      "    Loss = 38.8178\n",
      "\n",
      "(w=0.8, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.30\n",
      "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 4.2, loss: 13.69\n",
      "    x: 4.0, y: 9.1, y_hat: 4.6, loss: 20.25\n",
      "    x: 5.0, y: 10.9, y_hat: 5.4, loss: 30.25\n",
      "    Loss = 35.1498\n",
      "\n",
      "(w=0.8, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.88\n",
      "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 4.4, loss: 12.25\n",
      "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
      "    x: 5.0, y: 10.9, y_hat: 5.6, loss: 28.09\n",
      "    Loss = 31.7218\n",
      "\n",
      "(w=0.8, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.55\n",
      "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 4.6, loss: 10.89\n",
      "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
      "    x: 5.0, y: 10.9, y_hat: 5.8, loss: 26.01\n",
      "    Loss = 28.5338\n",
      "\n",
      "(w=0.8, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.29\n",
      "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 4.8, loss: 9.61\n",
      "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
      "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
      "    Loss = 25.5858\n",
      "\n",
      "(w=1.0, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.29\n",
      "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 3.5, loss: 19.36\n",
      "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
      "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
      "    Loss = 49.4400\n",
      "\n",
      "(w=1.0, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.41\n",
      "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 3.7, loss: 17.64\n",
      "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
      "    x: 5.0, y: 10.9, y_hat: 5.2, loss: 32.49\n",
      "    Loss = 45.0000\n",
      "\n",
      "(w=1.0, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.61\n",
      "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.9, loss: 16.00\n",
      "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
      "    x: 5.0, y: 10.9, y_hat: 5.4, loss: 30.25\n",
      "    Loss = 40.8000\n",
      "\n",
      "(w=1.0, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.89\n",
      "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 4.1, loss: 14.44\n",
      "    x: 4.0, y: 9.1, y_hat: 4.6, loss: 20.25\n",
      "    x: 5.0, y: 10.9, y_hat: 5.6, loss: 28.09\n",
      "    Loss = 36.8400\n",
      "\n",
      "(w=1.0, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.25\n",
      "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 4.3, loss: 12.96\n",
      "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
      "    x: 5.0, y: 10.9, y_hat: 5.8, loss: 26.01\n",
      "    Loss = 33.1200\n",
      "\n",
      "(w=1.0, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.69\n",
      "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 4.5, loss: 11.56\n",
      "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
      "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
      "    Loss = 29.6400\n",
      "\n",
      "(w=1.0, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.21\n",
      "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 4.7, loss: 10.24\n",
      "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
      "    x: 5.0, y: 10.9, y_hat: 6.2, loss: 22.09\n",
      "    Loss = 26.4000\n",
      "\n",
      "(w=1.0, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.81\n",
      "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 4.9, loss: 9.00\n",
      "    x: 4.0, y: 9.1, y_hat: 5.4, loss: 13.69\n",
      "    x: 5.0, y: 10.9, y_hat: 6.4, loss: 20.25\n",
      "    Loss = 23.4000\n",
      "\n",
      "(w=1.0, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.49\n",
      "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 5.1, loss: 7.84\n",
      "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
      "    x: 5.0, y: 10.9, y_hat: 6.6, loss: 18.49\n",
      "    Loss = 20.6400\n",
      "\n",
      "(w=1.0, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.25\n",
      "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 5.3, loss: 6.76\n",
      "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
      "    x: 5.0, y: 10.9, y_hat: 6.8, loss: 16.81\n",
      "    Loss = 18.1200\n",
      "\n",
      "(w=1.0, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.09\n",
      "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 5.5, loss: 5.76\n",
      "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
      "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
      "    Loss = 15.8400\n",
      "\n",
      "(w=1.2, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.24\n",
      "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 4.2, loss: 13.69\n",
      "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
      "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
      "    Loss = 35.4018\n",
      "\n",
      "(w=1.2, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.46\n",
      "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 4.4, loss: 12.25\n",
      "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
      "    x: 5.0, y: 10.9, y_hat: 6.2, loss: 22.09\n",
      "    Loss = 31.6298\n",
      "\n",
      "(w=1.2, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.76\n",
      "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 4.6, loss: 10.89\n",
      "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
      "    x: 5.0, y: 10.9, y_hat: 6.4, loss: 20.25\n",
      "    Loss = 28.0978\n",
      "\n",
      "(w=1.2, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.13\n",
      "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 4.8, loss: 9.61\n",
      "    x: 4.0, y: 9.1, y_hat: 5.4, loss: 13.69\n",
      "    x: 5.0, y: 10.9, y_hat: 6.6, loss: 18.49\n",
      "    Loss = 24.8058\n",
      "\n",
      "(w=1.2, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.59\n",
      "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 5.0, loss: 8.41\n",
      "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
      "    x: 5.0, y: 10.9, y_hat: 6.8, loss: 16.81\n",
      "    Loss = 21.7538\n",
      "\n",
      "(w=1.2, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.12\n",
      "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 5.2, loss: 7.29\n",
      "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
      "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
      "    Loss = 18.9418\n",
      "\n",
      "(w=1.2, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.74\n",
      "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 5.4, loss: 6.25\n",
      "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
      "    x: 5.0, y: 10.9, y_hat: 7.2, loss: 13.69\n",
      "    Loss = 16.3698\n",
      "\n",
      "(w=1.2, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.44\n",
      "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 5.6, loss: 5.29\n",
      "    x: 4.0, y: 9.1, y_hat: 6.2, loss: 8.41\n",
      "    x: 5.0, y: 10.9, y_hat: 7.4, loss: 12.25\n",
      "    Loss = 14.0378\n",
      "\n",
      "(w=1.2, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.21\n",
      "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 5.8, loss: 4.41\n",
      "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
      "    x: 5.0, y: 10.9, y_hat: 7.6, loss: 10.89\n",
      "    Loss = 11.9458\n",
      "\n",
      "(w=1.2, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.07\n",
      "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 6.0, loss: 3.61\n",
      "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
      "    x: 5.0, y: 10.9, y_hat: 7.8, loss: 9.61\n",
      "    Loss = 10.0938\n",
      "\n",
      "(w=1.2, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 6.2, loss: 2.89\n",
      "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
      "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
      "    Loss = 8.4818\n",
      "\n",
      "(w=1.4, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.31\n",
      "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 4.9, loss: 9.00\n",
      "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
      "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
      "    Loss = 23.7512\n",
      "\n",
      "(w=1.4, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.62\n",
      "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 5.1, loss: 7.84\n",
      "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
      "    x: 5.0, y: 10.9, y_hat: 7.2, loss: 13.69\n",
      "    Loss = 20.6472\n",
      "\n",
      "(w=1.4, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 2.02\n",
      "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 5.3, loss: 6.76\n",
      "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
      "    x: 5.0, y: 10.9, y_hat: 7.4, loss: 12.25\n",
      "    Loss = 17.7832\n",
      "\n",
      "(w=1.4, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.49\n",
      "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 5.5, loss: 5.76\n",
      "    x: 4.0, y: 9.1, y_hat: 6.2, loss: 8.41\n",
      "    x: 5.0, y: 10.9, y_hat: 7.6, loss: 10.89\n",
      "    Loss = 15.1592\n",
      "\n",
      "(w=1.4, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 1.04\n",
      "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 5.7, loss: 4.84\n",
      "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
      "    x: 5.0, y: 10.9, y_hat: 7.8, loss: 9.61\n",
      "    Loss = 12.7752\n",
      "\n",
      "(w=1.4, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.67\n",
      "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 5.9, loss: 4.00\n",
      "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
      "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
      "    Loss = 10.6312\n",
      "\n",
      "(w=1.4, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.38\n",
      "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 6.1, loss: 3.24\n",
      "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
      "    x: 5.0, y: 10.9, y_hat: 8.2, loss: 7.29\n",
      "    Loss = 8.7272\n",
      "\n",
      "(w=1.4, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.18\n",
      "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 6.3, loss: 2.56\n",
      "    x: 4.0, y: 9.1, y_hat: 7.0, loss: 4.41\n",
      "    x: 5.0, y: 10.9, y_hat: 8.4, loss: 6.25\n",
      "    Loss = 7.0632\n",
      "\n",
      "(w=1.4, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.05\n",
      "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 6.5, loss: 1.96\n",
      "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
      "    x: 5.0, y: 10.9, y_hat: 8.6, loss: 5.29\n",
      "    Loss = 5.6392\n",
      "\n",
      "(w=1.4, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 6.7, loss: 1.44\n",
      "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
      "    x: 5.0, y: 10.9, y_hat: 8.8, loss: 4.41\n",
      "    Loss = 4.4552\n",
      "\n",
      "(w=1.4, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.03\n",
      "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 6.9, loss: 1.00\n",
      "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
      "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
      "    Loss = 3.5112\n",
      "\n",
      "(w=1.6, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.50\n",
      "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 5.6, loss: 5.29\n",
      "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
      "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
      "    Loss = 14.4882\n",
      "\n",
      "(w=1.6, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 1.90\n",
      "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 5.8, loss: 4.41\n",
      "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
      "    x: 5.0, y: 10.9, y_hat: 8.2, loss: 7.29\n",
      "    Loss = 12.0522\n",
      "\n",
      "(w=1.6, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.39\n",
      "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 6.0, loss: 3.61\n",
      "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
      "    x: 5.0, y: 10.9, y_hat: 8.4, loss: 6.25\n",
      "    Loss = 9.8562\n",
      "\n",
      "(w=1.6, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 0.96\n",
      "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 6.2, loss: 2.89\n",
      "    x: 4.0, y: 9.1, y_hat: 7.0, loss: 4.41\n",
      "    x: 5.0, y: 10.9, y_hat: 8.6, loss: 5.29\n",
      "    Loss = 7.9002\n",
      "\n",
      "(w=1.6, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.61\n",
      "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 6.4, loss: 2.25\n",
      "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
      "    x: 5.0, y: 10.9, y_hat: 8.8, loss: 4.41\n",
      "    Loss = 6.1842\n",
      "\n",
      "(w=1.6, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.34\n",
      "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 6.6, loss: 1.69\n",
      "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
      "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
      "    Loss = 4.7082\n",
      "\n",
      "(w=1.6, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.14\n",
      "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 6.8, loss: 1.21\n",
      "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
      "    x: 5.0, y: 10.9, y_hat: 9.2, loss: 2.89\n",
      "    Loss = 3.4722\n",
      "\n",
      "(w=1.6, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.03\n",
      "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 7.0, loss: 0.81\n",
      "    x: 4.0, y: 9.1, y_hat: 7.8, loss: 1.69\n",
      "    x: 5.0, y: 10.9, y_hat: 9.4, loss: 2.25\n",
      "    Loss = 2.4762\n",
      "\n",
      "(w=1.6, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 7.2, loss: 0.49\n",
      "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
      "    x: 5.0, y: 10.9, y_hat: 9.6, loss: 1.69\n",
      "    Loss = 1.7202\n",
      "\n",
      "(w=1.6, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.05\n",
      "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
      "    x: 3.5, y: 7.9, y_hat: 7.4, loss: 0.25\n",
      "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
      "    x: 5.0, y: 10.9, y_hat: 9.8, loss: 1.21\n",
      "    Loss = 1.2042\n",
      "\n",
      "(w=1.6, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.18\n",
      "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 7.6, loss: 0.09\n",
      "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
      "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
      "    Loss = 0.9282\n",
      "\n",
      "(w=1.8, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.80\n",
      "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 6.3, loss: 2.56\n",
      "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
      "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
      "    Loss = 7.6128\n",
      "\n",
      "(w=1.8, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.30\n",
      "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 6.5, loss: 1.96\n",
      "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
      "    x: 5.0, y: 10.9, y_hat: 9.2, loss: 2.89\n",
      "    Loss = 5.8448\n",
      "\n",
      "(w=1.8, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.88\n",
      "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 6.7, loss: 1.44\n",
      "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
      "    x: 5.0, y: 10.9, y_hat: 9.4, loss: 2.25\n",
      "    Loss = 4.3168\n",
      "\n",
      "(w=1.8, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.55\n",
      "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 6.9, loss: 1.00\n",
      "    x: 4.0, y: 9.1, y_hat: 7.8, loss: 1.69\n",
      "    x: 5.0, y: 10.9, y_hat: 9.6, loss: 1.69\n",
      "    Loss = 3.0288\n",
      "\n",
      "(w=1.8, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.29\n",
      "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 7.1, loss: 0.64\n",
      "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
      "    x: 5.0, y: 10.9, y_hat: 9.8, loss: 1.21\n",
      "    Loss = 1.9808\n",
      "\n",
      "(w=1.8, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.12\n",
      "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 7.3, loss: 0.36\n",
      "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
      "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
      "    Loss = 1.1728\n",
      "\n",
      "(w=1.8, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.02\n",
      "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 7.5, loss: 0.16\n",
      "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
      "    x: 5.0, y: 10.9, y_hat: 10.2, loss: 0.49\n",
      "    Loss = 0.6048\n",
      "\n",
      "(w=1.8, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
      "    x: 3.5, y: 7.9, y_hat: 7.7, loss: 0.04\n",
      "    x: 4.0, y: 9.1, y_hat: 8.6, loss: 0.25\n",
      "    x: 5.0, y: 10.9, y_hat: 10.4, loss: 0.25\n",
      "    Loss = 0.2768\n",
      "\n",
      "(w=1.8, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.07\n",
      "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 7.9, loss: 0.00\n",
      "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
      "    x: 5.0, y: 10.9, y_hat: 10.6, loss: 0.09\n",
      "    Loss = 0.1888\n",
      "\n",
      "(w=1.8, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.21\n",
      "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 8.1, loss: 0.04\n",
      "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
      "    x: 5.0, y: 10.9, y_hat: 10.8, loss: 0.01\n",
      "    Loss = 0.3408\n",
      "\n",
      "(w=1.8, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.44\n",
      "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 8.3, loss: 0.16\n",
      "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
      "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
      "    Loss = 0.7328\n",
      "\n",
      "(w=2.0, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.21\n",
      "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 7.0, loss: 0.81\n",
      "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
      "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
      "    Loss = 3.1250\n",
      "\n",
      "(w=2.0, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.81\n",
      "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 7.2, loss: 0.49\n",
      "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
      "    x: 5.0, y: 10.9, y_hat: 10.2, loss: 0.49\n",
      "    Loss = 2.0250\n",
      "\n",
      "(w=2.0, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.49\n",
      "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 7.4, loss: 0.25\n",
      "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
      "    x: 5.0, y: 10.9, y_hat: 10.4, loss: 0.25\n",
      "    Loss = 1.1650\n",
      "\n",
      "(w=2.0, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.25\n",
      "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 7.6, loss: 0.09\n",
      "    x: 4.0, y: 9.1, y_hat: 8.6, loss: 0.25\n",
      "    x: 5.0, y: 10.9, y_hat: 10.6, loss: 0.09\n",
      "    Loss = 0.5450\n",
      "\n",
      "(w=2.0, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.09\n",
      "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 7.8, loss: 0.01\n",
      "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
      "    x: 5.0, y: 10.9, y_hat: 10.8, loss: 0.01\n",
      "    Loss = 0.1650\n",
      "\n",
      "(w=2.0, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.01\n",
      "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
      "    x: 3.5, y: 7.9, y_hat: 8.0, loss: 0.01\n",
      "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
      "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
      "    Loss = 0.0250\n",
      "\n",
      "(w=2.0, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.01\n",
      "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 8.2, loss: 0.09\n",
      "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
      "    x: 5.0, y: 10.9, y_hat: 11.2, loss: 0.09\n",
      "    Loss = 0.1250\n",
      "\n",
      "(w=2.0, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.09\n",
      "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 8.4, loss: 0.25\n",
      "    x: 4.0, y: 9.1, y_hat: 9.4, loss: 0.09\n",
      "    x: 5.0, y: 10.9, y_hat: 11.4, loss: 0.25\n",
      "    Loss = 0.4650\n",
      "\n",
      "(w=2.0, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.25\n",
      "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 8.6, loss: 0.49\n",
      "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
      "    x: 5.0, y: 10.9, y_hat: 11.6, loss: 0.49\n",
      "    Loss = 1.0450\n",
      "\n",
      "(w=2.0, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.49\n",
      "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 8.8, loss: 0.81\n",
      "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
      "    x: 5.0, y: 10.9, y_hat: 11.8, loss: 0.81\n",
      "    Loss = 1.8650\n",
      "\n",
      "(w=2.0, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.81\n",
      "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 9.0, loss: 1.21\n",
      "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
      "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
      "    Loss = 2.9250\n",
      "\n",
      "(w=2.2, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.74\n",
      "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 7.7, loss: 0.04\n",
      "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
      "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
      "    Loss = 1.0248\n",
      "\n",
      "(w=2.2, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.44\n",
      "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 7.9, loss: 0.00\n",
      "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
      "    x: 5.0, y: 10.9, y_hat: 11.2, loss: 0.09\n",
      "    Loss = 0.5928\n",
      "\n",
      "(w=2.2, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.21\n",
      "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 8.1, loss: 0.04\n",
      "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
      "    x: 5.0, y: 10.9, y_hat: 11.4, loss: 0.25\n",
      "    Loss = 0.4008\n",
      "\n",
      "(w=2.2, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.07\n",
      "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
      "    x: 3.5, y: 7.9, y_hat: 8.3, loss: 0.16\n",
      "    x: 4.0, y: 9.1, y_hat: 9.4, loss: 0.09\n",
      "    x: 5.0, y: 10.9, y_hat: 11.6, loss: 0.49\n",
      "    Loss = 0.4488\n",
      "\n",
      "(w=2.2, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 8.5, loss: 0.36\n",
      "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
      "    x: 5.0, y: 10.9, y_hat: 11.8, loss: 0.81\n",
      "    Loss = 0.7368\n",
      "\n",
      "(w=2.2, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.02\n",
      "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 8.7, loss: 0.64\n",
      "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
      "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
      "    Loss = 1.2648\n",
      "\n",
      "(w=2.2, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.12\n",
      "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 8.9, loss: 1.00\n",
      "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
      "    x: 5.0, y: 10.9, y_hat: 12.2, loss: 1.69\n",
      "    Loss = 2.0328\n",
      "\n",
      "(w=2.2, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.29\n",
      "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 9.1, loss: 1.44\n",
      "    x: 4.0, y: 9.1, y_hat: 10.2, loss: 1.21\n",
      "    x: 5.0, y: 10.9, y_hat: 12.4, loss: 2.25\n",
      "    Loss = 3.0408\n",
      "\n",
      "(w=2.2, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.55\n",
      "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 9.3, loss: 1.96\n",
      "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
      "    x: 5.0, y: 10.9, y_hat: 12.6, loss: 2.89\n",
      "    Loss = 4.2888\n",
      "\n",
      "(w=2.2, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.88\n",
      "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 9.5, loss: 2.56\n",
      "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
      "    x: 5.0, y: 10.9, y_hat: 12.8, loss: 3.61\n",
      "    Loss = 5.7768\n",
      "\n",
      "(w=2.2, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.30\n",
      "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 9.7, loss: 3.24\n",
      "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
      "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
      "    Loss = 7.5048\n",
      "\n",
      "(w=2.4, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.38\n",
      "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 8.4, loss: 0.25\n",
      "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
      "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
      "    Loss = 1.3122\n",
      "\n",
      "(w=2.4, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.18\n",
      "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
      "    x: 3.5, y: 7.9, y_hat: 8.6, loss: 0.49\n",
      "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
      "    x: 5.0, y: 10.9, y_hat: 12.2, loss: 1.69\n",
      "    Loss = 1.5482\n",
      "\n",
      "(w=2.4, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.05\n",
      "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 8.8, loss: 0.81\n",
      "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
      "    x: 5.0, y: 10.9, y_hat: 12.4, loss: 2.25\n",
      "    Loss = 2.0242\n",
      "\n",
      "(w=2.4, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 9.0, loss: 1.21\n",
      "    x: 4.0, y: 9.1, y_hat: 10.2, loss: 1.21\n",
      "    x: 5.0, y: 10.9, y_hat: 12.6, loss: 2.89\n",
      "    Loss = 2.7402\n",
      "\n",
      "(w=2.4, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.03\n",
      "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 9.2, loss: 1.69\n",
      "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
      "    x: 5.0, y: 10.9, y_hat: 12.8, loss: 3.61\n",
      "    Loss = 3.6962\n",
      "\n",
      "(w=2.4, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.14\n",
      "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 9.4, loss: 2.25\n",
      "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
      "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
      "    Loss = 4.8922\n",
      "\n",
      "(w=2.4, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.34\n",
      "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 9.6, loss: 2.89\n",
      "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
      "    x: 5.0, y: 10.9, y_hat: 13.2, loss: 5.29\n",
      "    Loss = 6.3282\n",
      "\n",
      "(w=2.4, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.61\n",
      "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 9.8, loss: 3.61\n",
      "    x: 4.0, y: 9.1, y_hat: 11.0, loss: 3.61\n",
      "    x: 5.0, y: 10.9, y_hat: 13.4, loss: 6.25\n",
      "    Loss = 8.0042\n",
      "\n",
      "(w=2.4, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 0.96\n",
      "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 10.0, loss: 4.41\n",
      "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
      "    x: 5.0, y: 10.9, y_hat: 13.6, loss: 7.29\n",
      "    Loss = 9.9202\n",
      "\n",
      "(w=2.4, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.39\n",
      "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 10.2, loss: 5.29\n",
      "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
      "    x: 5.0, y: 10.9, y_hat: 13.8, loss: 8.41\n",
      "    Loss = 12.0762\n",
      "\n",
      "(w=2.4, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 1.90\n",
      "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 10.4, loss: 6.25\n",
      "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
      "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
      "    Loss = 14.4722\n",
      "\n",
      "(w=2.6, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.14\n",
      "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 9.1, loss: 1.44\n",
      "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
      "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
      "    Loss = 3.9872\n",
      "\n",
      "(w=2.6, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.03\n",
      "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 9.3, loss: 1.96\n",
      "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
      "    x: 5.0, y: 10.9, y_hat: 13.2, loss: 5.29\n",
      "    Loss = 4.8912\n",
      "\n",
      "(w=2.6, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 9.5, loss: 2.56\n",
      "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
      "    x: 5.0, y: 10.9, y_hat: 13.4, loss: 6.25\n",
      "    Loss = 6.0352\n",
      "\n",
      "(w=2.6, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.05\n",
      "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 9.7, loss: 3.24\n",
      "    x: 4.0, y: 9.1, y_hat: 11.0, loss: 3.61\n",
      "    x: 5.0, y: 10.9, y_hat: 13.6, loss: 7.29\n",
      "    Loss = 7.4192\n",
      "\n",
      "(w=2.6, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.18\n",
      "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 9.9, loss: 4.00\n",
      "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
      "    x: 5.0, y: 10.9, y_hat: 13.8, loss: 8.41\n",
      "    Loss = 9.0432\n",
      "\n",
      "(w=2.6, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.38\n",
      "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 10.1, loss: 4.84\n",
      "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
      "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
      "    Loss = 10.9072\n",
      "\n",
      "(w=2.6, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.67\n",
      "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 10.3, loss: 5.76\n",
      "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
      "    x: 5.0, y: 10.9, y_hat: 14.2, loss: 10.89\n",
      "    Loss = 13.0112\n",
      "\n",
      "(w=2.6, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 1.04\n",
      "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 10.5, loss: 6.76\n",
      "    x: 4.0, y: 9.1, y_hat: 11.8, loss: 7.29\n",
      "    x: 5.0, y: 10.9, y_hat: 14.4, loss: 12.25\n",
      "    Loss = 15.3552\n",
      "\n",
      "(w=2.6, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.49\n",
      "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 10.7, loss: 7.84\n",
      "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
      "    x: 5.0, y: 10.9, y_hat: 14.6, loss: 13.69\n",
      "    Loss = 17.9392\n",
      "\n",
      "(w=2.6, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 2.02\n",
      "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 10.9, loss: 9.00\n",
      "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
      "    x: 5.0, y: 10.9, y_hat: 14.8, loss: 15.21\n",
      "    Loss = 20.7632\n",
      "\n",
      "(w=2.6, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.62\n",
      "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 11.1, loss: 10.24\n",
      "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
      "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
      "    Loss = 23.8272\n",
      "\n",
      "(w=2.8, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.02\n",
      "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 9.8, loss: 3.61\n",
      "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
      "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
      "    Loss = 9.0498\n",
      "\n",
      "(w=2.8, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 10.0, loss: 4.41\n",
      "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
      "    x: 5.0, y: 10.9, y_hat: 14.2, loss: 10.89\n",
      "    Loss = 10.6218\n",
      "\n",
      "(w=2.8, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.07\n",
      "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 10.2, loss: 5.29\n",
      "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
      "    x: 5.0, y: 10.9, y_hat: 14.4, loss: 12.25\n",
      "    Loss = 12.4338\n",
      "\n",
      "(w=2.8, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.21\n",
      "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 10.4, loss: 6.25\n",
      "    x: 4.0, y: 9.1, y_hat: 11.8, loss: 7.29\n",
      "    x: 5.0, y: 10.9, y_hat: 14.6, loss: 13.69\n",
      "    Loss = 14.4858\n",
      "\n",
      "(w=2.8, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.44\n",
      "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 10.6, loss: 7.29\n",
      "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
      "    x: 5.0, y: 10.9, y_hat: 14.8, loss: 15.21\n",
      "    Loss = 16.7778\n",
      "\n",
      "(w=2.8, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.74\n",
      "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 10.8, loss: 8.41\n",
      "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
      "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
      "    Loss = 19.3098\n",
      "\n",
      "(w=2.8, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.12\n",
      "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 11.0, loss: 9.61\n",
      "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
      "    x: 5.0, y: 10.9, y_hat: 15.2, loss: 18.49\n",
      "    Loss = 22.0818\n",
      "\n",
      "(w=2.8, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.59\n",
      "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 11.2, loss: 10.89\n",
      "    x: 4.0, y: 9.1, y_hat: 12.6, loss: 12.25\n",
      "    x: 5.0, y: 10.9, y_hat: 15.4, loss: 20.25\n",
      "    Loss = 25.0938\n",
      "\n",
      "(w=2.8, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.13\n",
      "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 11.4, loss: 12.25\n",
      "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
      "    x: 5.0, y: 10.9, y_hat: 15.6, loss: 22.09\n",
      "    Loss = 28.3458\n",
      "\n",
      "(w=2.8, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.76\n",
      "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 11.6, loss: 13.69\n",
      "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
      "    x: 5.0, y: 10.9, y_hat: 15.8, loss: 24.01\n",
      "    Loss = 31.8378\n",
      "\n",
      "(w=2.8, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.46\n",
      "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 11.8, loss: 15.21\n",
      "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
      "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
      "    Loss = 35.5698\n",
      "\n",
      "(w=3.0, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.01\n",
      "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 10.5, loss: 6.76\n",
      "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
      "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
      "    Loss = 16.5000\n",
      "\n",
      "(w=3.0, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.09\n",
      "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 10.7, loss: 7.84\n",
      "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
      "    x: 5.0, y: 10.9, y_hat: 15.2, loss: 18.49\n",
      "    Loss = 18.7400\n",
      "\n",
      "(w=3.0, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.25\n",
      "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 10.9, loss: 9.00\n",
      "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
      "    x: 5.0, y: 10.9, y_hat: 15.4, loss: 20.25\n",
      "    Loss = 21.2200\n",
      "\n",
      "(w=3.0, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.49\n",
      "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 11.1, loss: 10.24\n",
      "    x: 4.0, y: 9.1, y_hat: 12.6, loss: 12.25\n",
      "    x: 5.0, y: 10.9, y_hat: 15.6, loss: 22.09\n",
      "    Loss = 23.9400\n",
      "\n",
      "(w=3.0, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.81\n",
      "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 11.3, loss: 11.56\n",
      "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
      "    x: 5.0, y: 10.9, y_hat: 15.8, loss: 24.01\n",
      "    Loss = 26.9000\n",
      "\n",
      "(w=3.0, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.21\n",
      "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 11.5, loss: 12.96\n",
      "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
      "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
      "    Loss = 30.1000\n",
      "\n",
      "(w=3.0, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.69\n",
      "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 11.7, loss: 14.44\n",
      "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
      "    x: 5.0, y: 10.9, y_hat: 16.2, loss: 28.09\n",
      "    Loss = 33.5400\n",
      "\n",
      "(w=3.0, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.25\n",
      "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 11.9, loss: 16.00\n",
      "    x: 4.0, y: 9.1, y_hat: 13.4, loss: 18.49\n",
      "    x: 5.0, y: 10.9, y_hat: 16.4, loss: 30.25\n",
      "    Loss = 37.2200\n",
      "\n",
      "(w=3.0, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.89\n",
      "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 12.1, loss: 17.64\n",
      "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
      "    x: 5.0, y: 10.9, y_hat: 16.6, loss: 32.49\n",
      "    Loss = 41.1400\n",
      "\n",
      "(w=3.0, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.61\n",
      "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 12.3, loss: 19.36\n",
      "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
      "    x: 5.0, y: 10.9, y_hat: 16.8, loss: 34.81\n",
      "    Loss = 45.3000\n",
      "\n",
      "(w=3.0, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.41\n",
      "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 12.5, loss: 21.16\n",
      "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
      "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
      "    Loss = 49.7000\n",
      "\n",
      "(w=3.2, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.12\n",
      "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 11.2, loss: 10.89\n",
      "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
      "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
      "    Loss = 26.3378\n",
      "\n",
      "(w=3.2, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.29\n",
      "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 11.4, loss: 12.25\n",
      "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
      "    x: 5.0, y: 10.9, y_hat: 16.2, loss: 28.09\n",
      "    Loss = 29.2458\n",
      "\n",
      "(w=3.2, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.55\n",
      "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 11.6, loss: 13.69\n",
      "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
      "    x: 5.0, y: 10.9, y_hat: 16.4, loss: 30.25\n",
      "    Loss = 32.3938\n",
      "\n",
      "(w=3.2, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.88\n",
      "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 11.8, loss: 15.21\n",
      "    x: 4.0, y: 9.1, y_hat: 13.4, loss: 18.49\n",
      "    x: 5.0, y: 10.9, y_hat: 16.6, loss: 32.49\n",
      "    Loss = 35.7818\n",
      "\n",
      "(w=3.2, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.30\n",
      "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 12.0, loss: 16.81\n",
      "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
      "    x: 5.0, y: 10.9, y_hat: 16.8, loss: 34.81\n",
      "    Loss = 39.4098\n",
      "\n",
      "(w=3.2, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.80\n",
      "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 12.2, loss: 18.49\n",
      "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
      "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
      "    Loss = 43.2778\n",
      "\n",
      "(w=3.2, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.37\n",
      "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 12.4, loss: 20.25\n",
      "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
      "    x: 5.0, y: 10.9, y_hat: 17.2, loss: 39.69\n",
      "    Loss = 47.3858\n",
      "\n",
      "(w=3.2, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 3.03\n",
      "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 12.6, loss: 22.09\n",
      "    x: 4.0, y: 9.1, y_hat: 14.2, loss: 26.01\n",
      "    x: 5.0, y: 10.9, y_hat: 17.4, loss: 42.25\n",
      "    Loss = 51.7338\n",
      "\n",
      "(w=3.2, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.76\n",
      "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 12.8, loss: 24.01\n",
      "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
      "    x: 5.0, y: 10.9, y_hat: 17.6, loss: 44.89\n",
      "    Loss = 56.3218\n",
      "\n",
      "(w=3.2, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.58\n",
      "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 13.0, loss: 26.01\n",
      "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
      "    x: 5.0, y: 10.9, y_hat: 17.8, loss: 47.61\n",
      "    Loss = 61.1498\n",
      "\n",
      "(w=3.2, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.48\n",
      "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 13.2, loss: 28.09\n",
      "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
      "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
      "    Loss = 66.2178\n",
      "\n",
      "(w=3.4, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.34\n",
      "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 11.9, loss: 16.00\n",
      "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
      "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
      "    Loss = 38.5632\n",
      "\n",
      "(w=3.4, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.61\n",
      "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 12.1, loss: 17.64\n",
      "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
      "    x: 5.0, y: 10.9, y_hat: 17.2, loss: 39.69\n",
      "    Loss = 42.1392\n",
      "\n",
      "(w=3.4, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 0.96\n",
      "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 12.3, loss: 19.36\n",
      "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
      "    x: 5.0, y: 10.9, y_hat: 17.4, loss: 42.25\n",
      "    Loss = 45.9552\n",
      "\n",
      "(w=3.4, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.39\n",
      "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 12.5, loss: 21.16\n",
      "    x: 4.0, y: 9.1, y_hat: 14.2, loss: 26.01\n",
      "    x: 5.0, y: 10.9, y_hat: 17.6, loss: 44.89\n",
      "    Loss = 50.0112\n",
      "\n",
      "(w=3.4, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 1.90\n",
      "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 12.7, loss: 23.04\n",
      "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
      "    x: 5.0, y: 10.9, y_hat: 17.8, loss: 47.61\n",
      "    Loss = 54.3072\n",
      "\n",
      "(w=3.4, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.50\n",
      "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 12.9, loss: 25.00\n",
      "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
      "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
      "    Loss = 58.8432\n",
      "\n",
      "(w=3.4, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.3, loss: 3.17\n",
      "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 13.1, loss: 27.04\n",
      "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
      "    x: 5.0, y: 10.9, y_hat: 18.2, loss: 53.29\n",
      "    Loss = 63.6192\n",
      "\n",
      "(w=3.4, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.5, loss: 3.92\n",
      "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 13.3, loss: 29.16\n",
      "    x: 4.0, y: 9.1, y_hat: 15.0, loss: 34.81\n",
      "    x: 5.0, y: 10.9, y_hat: 18.4, loss: 56.25\n",
      "    Loss = 68.6352\n",
      "\n",
      "(w=3.4, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 5.7, loss: 4.75\n",
      "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 13.5, loss: 31.36\n",
      "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
      "    x: 5.0, y: 10.9, y_hat: 18.6, loss: 59.29\n",
      "    Loss = 73.8912\n",
      "\n",
      "(w=3.4, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 5.9, loss: 5.66\n",
      "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 13.7, loss: 33.64\n",
      "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
      "    x: 5.0, y: 10.9, y_hat: 18.8, loss: 62.41\n",
      "    Loss = 79.3872\n",
      "\n",
      "(w=3.4, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 6.1, loss: 6.66\n",
      "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 13.9, loss: 36.00\n",
      "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
      "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
      "    Loss = 85.1232\n",
      "\n",
      "(w=3.6, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.67\n",
      "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 12.6, loss: 22.09\n",
      "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
      "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
      "    Loss = 53.1762\n",
      "\n",
      "(w=3.6, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 1.04\n",
      "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 12.8, loss: 24.01\n",
      "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
      "    x: 5.0, y: 10.9, y_hat: 18.2, loss: 53.29\n",
      "    Loss = 57.4202\n",
      "\n",
      "(w=3.6, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.49\n",
      "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 13.0, loss: 26.01\n",
      "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
      "    x: 5.0, y: 10.9, y_hat: 18.4, loss: 56.25\n",
      "    Loss = 61.9042\n",
      "\n",
      "(w=3.6, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 2.02\n",
      "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 13.2, loss: 28.09\n",
      "    x: 4.0, y: 9.1, y_hat: 15.0, loss: 34.81\n",
      "    x: 5.0, y: 10.9, y_hat: 18.6, loss: 59.29\n",
      "    Loss = 66.6282\n",
      "\n",
      "(w=3.6, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.62\n",
      "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 13.4, loss: 30.25\n",
      "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
      "    x: 5.0, y: 10.9, y_hat: 18.8, loss: 62.41\n",
      "    Loss = 71.5922\n",
      "\n",
      "(w=3.6, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.3, loss: 3.31\n",
      "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 13.6, loss: 32.49\n",
      "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
      "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
      "    Loss = 76.7962\n",
      "\n",
      "(w=3.6, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.5, loss: 4.08\n",
      "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 13.8, loss: 34.81\n",
      "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
      "    x: 5.0, y: 10.9, y_hat: 19.2, loss: 68.89\n",
      "    Loss = 82.2402\n",
      "\n",
      "(w=3.6, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 5.7, loss: 4.93\n",
      "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 14.0, loss: 37.21\n",
      "    x: 4.0, y: 9.1, y_hat: 15.8, loss: 44.89\n",
      "    x: 5.0, y: 10.9, y_hat: 19.4, loss: 72.25\n",
      "    Loss = 87.9242\n",
      "\n",
      "(w=3.6, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 5.9, loss: 5.86\n",
      "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 14.2, loss: 39.69\n",
      "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
      "    x: 5.0, y: 10.9, y_hat: 19.6, loss: 75.69\n",
      "    Loss = 93.8482\n",
      "\n",
      "(w=3.6, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 6.1, loss: 6.86\n",
      "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
      "    x: 3.5, y: 7.9, y_hat: 14.4, loss: 42.25\n",
      "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
      "    x: 5.0, y: 10.9, y_hat: 19.8, loss: 79.21\n",
      "    Loss = 100.0122\n",
      "\n",
      "(w=3.6, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 6.3, loss: 7.95\n",
      "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
      "    x: 3.5, y: 7.9, y_hat: 14.6, loss: 44.89\n",
      "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
      "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
      "    Loss = 106.4162\n",
      "\n",
      "(w=3.8, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.12\n",
      "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 13.3, loss: 29.16\n",
      "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
      "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
      "    Loss = 70.1768\n",
      "\n",
      "(w=3.8, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.59\n",
      "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 13.5, loss: 31.36\n",
      "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
      "    x: 5.0, y: 10.9, y_hat: 19.2, loss: 68.89\n",
      "    Loss = 75.0888\n",
      "\n",
      "(w=3.8, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.13\n",
      "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 13.7, loss: 33.64\n",
      "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
      "    x: 5.0, y: 10.9, y_hat: 19.4, loss: 72.25\n",
      "    Loss = 80.2408\n",
      "\n",
      "(w=3.8, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.76\n",
      "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 13.9, loss: 36.00\n",
      "    x: 4.0, y: 9.1, y_hat: 15.8, loss: 44.89\n",
      "    x: 5.0, y: 10.9, y_hat: 19.6, loss: 75.69\n",
      "    Loss = 85.6328\n",
      "\n",
      "(w=3.8, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.46\n",
      "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 14.1, loss: 38.44\n",
      "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
      "    x: 5.0, y: 10.9, y_hat: 19.8, loss: 79.21\n",
      "    Loss = 91.2648\n",
      "\n",
      "(w=3.8, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.24\n",
      "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 14.3, loss: 40.96\n",
      "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
      "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
      "    Loss = 97.1368\n",
      "\n",
      "(w=3.8, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.11\n",
      "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 14.5, loss: 43.56\n",
      "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
      "    x: 5.0, y: 10.9, y_hat: 20.2, loss: 86.49\n",
      "    Loss = 103.2488\n",
      "\n",
      "(w=3.8, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 6.0, loss: 6.05\n",
      "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
      "    x: 3.5, y: 7.9, y_hat: 14.7, loss: 46.24\n",
      "    x: 4.0, y: 9.1, y_hat: 16.6, loss: 56.25\n",
      "    x: 5.0, y: 10.9, y_hat: 20.4, loss: 90.25\n",
      "    Loss = 109.6008\n",
      "\n",
      "(w=3.8, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 6.2, loss: 7.08\n",
      "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
      "    x: 3.5, y: 7.9, y_hat: 14.9, loss: 49.00\n",
      "    x: 4.0, y: 9.1, y_hat: 16.8, loss: 59.29\n",
      "    x: 5.0, y: 10.9, y_hat: 20.6, loss: 94.09\n",
      "    Loss = 116.1928\n",
      "\n",
      "(w=3.8, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 6.4, loss: 8.18\n",
      "    x: 2.0, y: 5.0, y_hat: 9.4, loss: 19.36\n",
      "    x: 3.5, y: 7.9, y_hat: 15.1, loss: 51.84\n",
      "    x: 4.0, y: 9.1, y_hat: 17.0, loss: 62.41\n",
      "    x: 5.0, y: 10.9, y_hat: 20.8, loss: 98.01\n",
      "    Loss = 123.0248\n",
      "\n",
      "(w=3.8, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.8, loss: 7.29\n",
      "    x: 1.2, y: 3.5, y_hat: 6.6, loss: 9.36\n",
      "    x: 2.0, y: 5.0, y_hat: 9.6, loss: 21.16\n",
      "    x: 3.5, y: 7.9, y_hat: 15.3, loss: 54.76\n",
      "    x: 4.0, y: 9.1, y_hat: 17.2, loss: 65.61\n",
      "    x: 5.0, y: 10.9, y_hat: 21.0, loss: 102.01\n",
      "    Loss = 130.0968\n",
      "\n",
      "(w=4.0, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.69\n",
      "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 14.0, loss: 37.21\n",
      "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
      "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
      "    Loss = 89.5650\n",
      "\n",
      "(w=4.0, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.25\n",
      "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 14.2, loss: 39.69\n",
      "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
      "    x: 5.0, y: 10.9, y_hat: 20.2, loss: 86.49\n",
      "    Loss = 95.1450\n",
      "\n",
      "(w=4.0, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.89\n",
      "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 14.4, loss: 42.25\n",
      "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
      "    x: 5.0, y: 10.9, y_hat: 20.4, loss: 90.25\n",
      "    Loss = 100.9650\n",
      "\n",
      "(w=4.0, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.61\n",
      "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 14.6, loss: 44.89\n",
      "    x: 4.0, y: 9.1, y_hat: 16.6, loss: 56.25\n",
      "    x: 5.0, y: 10.9, y_hat: 20.6, loss: 94.09\n",
      "    Loss = 107.0250\n",
      "\n",
      "(w=4.0, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.41\n",
      "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 14.8, loss: 47.61\n",
      "    x: 4.0, y: 9.1, y_hat: 16.8, loss: 59.29\n",
      "    x: 5.0, y: 10.9, y_hat: 20.8, loss: 98.01\n",
      "    Loss = 113.3250\n",
      "\n",
      "(w=4.0, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.29\n",
      "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
      "    x: 3.5, y: 7.9, y_hat: 15.0, loss: 50.41\n",
      "    x: 4.0, y: 9.1, y_hat: 17.0, loss: 62.41\n",
      "    x: 5.0, y: 10.9, y_hat: 21.0, loss: 102.01\n",
      "    Loss = 119.8650\n",
      "\n",
      "(w=4.0, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 6.0, loss: 6.25\n",
      "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
      "    x: 3.5, y: 7.9, y_hat: 15.2, loss: 53.29\n",
      "    x: 4.0, y: 9.1, y_hat: 17.2, loss: 65.61\n",
      "    x: 5.0, y: 10.9, y_hat: 21.2, loss: 106.09\n",
      "    Loss = 126.6450\n",
      "\n",
      "(w=4.0, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 6.2, loss: 7.29\n",
      "    x: 2.0, y: 5.0, y_hat: 9.4, loss: 19.36\n",
      "    x: 3.5, y: 7.9, y_hat: 15.4, loss: 56.25\n",
      "    x: 4.0, y: 9.1, y_hat: 17.4, loss: 68.89\n",
      "    x: 5.0, y: 10.9, y_hat: 21.4, loss: 110.25\n",
      "    Loss = 133.6650\n",
      "\n",
      "(w=4.0, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 6.4, loss: 8.41\n",
      "    x: 2.0, y: 5.0, y_hat: 9.6, loss: 21.16\n",
      "    x: 3.5, y: 7.9, y_hat: 15.6, loss: 59.29\n",
      "    x: 4.0, y: 9.1, y_hat: 17.6, loss: 72.25\n",
      "    x: 5.0, y: 10.9, y_hat: 21.6, loss: 114.49\n",
      "    Loss = 140.9250\n",
      "\n",
      "(w=4.0, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.8, loss: 7.29\n",
      "    x: 1.2, y: 3.5, y_hat: 6.6, loss: 9.61\n",
      "    x: 2.0, y: 5.0, y_hat: 9.8, loss: 23.04\n",
      "    x: 3.5, y: 7.9, y_hat: 15.8, loss: 62.41\n",
      "    x: 4.0, y: 9.1, y_hat: 17.8, loss: 75.69\n",
      "    x: 5.0, y: 10.9, y_hat: 21.8, loss: 118.81\n",
      "    Loss = 148.4250\n",
      "\n",
      "(w=4.0, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 6.0, loss: 8.41\n",
      "    x: 1.2, y: 3.5, y_hat: 6.8, loss: 10.89\n",
      "    x: 2.0, y: 5.0, y_hat: 10.0, loss: 25.00\n",
      "    x: 3.5, y: 7.9, y_hat: 16.0, loss: 65.61\n",
      "    x: 4.0, y: 9.1, y_hat: 18.0, loss: 79.21\n",
      "    x: 5.0, y: 10.9, y_hat: 22.0, loss: 123.21\n",
      "    Loss = 156.1650\n",
      "\n",
      "BEST:\n",
      "w=2.0, b=1.0, loss=0.0250\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdvNc6WQOwhv"
   },
   "source": [
    "Plotting the loss values as a surface graph gives you a picture of the \"optimisation landscape\" for the parameter values. The loss is minimum at $w=2$, $b=1$ (it might be hard to see this clearly from the 3D diagram, but you can trust the numbers)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "LVruAU86OwIA",
    "outputId": "322f196c-e31e-4357-a532-aeab73240aef"
   },
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "# enable 3D\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    " # generate combinations of weights and biases\n",
    "(w_list, b_list) = np.meshgrid(weights, biases)\n",
    "\n",
    "# plot loss across weights and bias values\n",
    "surf = ax.plot_surface(w_list.T, b_list.T, loss_matrix,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('b')\n",
    "ax.set_zlabel('Loss')\n",
    "plt.show()"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Annnn\\AppData\\Local\\Temp/ipykernel_2188/3762650947.py:4: MatplotlibDeprecationWarning: Calling gca() with keyword arguments was deprecated in Matplotlib 3.4. Starting two minor releases later, gca() will take no keyword arguments. The gca() function should only be used to get the current axes, or if no axes exist, create new axes with default keyword arguments. To create a new axes with non-default arguments, use plt.axes() or plt.subplot().\n",
      "  ax = fig.gca(projection='3d')\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nWYSrkBcWzs"
   },
   "source": [
    "### Optimisation with derivatives\n",
    "\n",
    "Obviously, the brute force search method above is really computationally expensive (very slow!), especially when there are more parameters and more values to search. A more efficient approach is to start from a random set of parameter values, and then try to \"move down\" the loss graph to the point with a minimum loss. So, in the plot above, you can start at some random $w$ and $b$, and try to move towards the minimum point.\n",
    "\n",
    "The problem is: from where you are, you actually do not know where the minimum point is. You only know that there *is* a minimum point. The question is: how do you know towards which direction you should move (and how far) so that you can get to the minimum point as fast as possible? The answer is downwards (obviously!) and where the slope is steepest. \n",
    "\n",
    "Fortunately, calculus saves us from guessing, and provides us a way to compute the direction of the slope at any point. This is called the *derivative* (or gradient). Intuitively, the derivative $\\frac{\\partial L}{\\partial w}$ tells us by how much $L$ changes when $w$ changes. Similarly, $\\frac{\\partial L}{\\partial b}$ tells us by how much $L$ changes when $b$ changes. So if we move in these directions, we hope that we can ultimately reach a minimum point.\n",
    "\n",
    "If you are well-versed with calculus, you can compute $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ by hand. Otherwise, you can get help from a derivative calculator (e.g. https://www.derivative-calculator.net)\n",
    "\n",
    "As presented in the lectures, the partial derivatives of the loss function with respect to $w$ and to $b$ are:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w} = \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x$ \n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$ \n",
    "\n",
    "Thus, for a *single* point, the partial derivatives are:\n",
    "\n",
    "$\\frac{\\partial L^{(i)}}{\\partial w} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x$ \n",
    "\n",
    "$\\frac{\\partial L^{(i)}}{\\partial b} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$ \n",
    "\n",
    "Now, complete the `gradient()` method for `SimpleLinearRegression` below to compute the partial derivatives wrt $w$ and $b$ at a given point. To make life easier, just compute and return both $\\frac{\\partial L^{(i)}}{\\partial w}$ and $\\frac{\\partial L^{(i)}}{\\partial b}$ at the same time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBXmI71Ewc9e",
    "outputId": "412ed58f-f115-4964-80ce-594568b0cb37"
   },
   "source": [
    "def gradient(self, x, y):\n",
    "    \"\"\" Compute partial derivatives wrt w and b\n",
    "\n",
    "    Args:\n",
    "        x (float): input instance\n",
    "        y (float): ground truth output\n",
    "\n",
    "    Returns:\n",
    "        tuple: (float, float)\n",
    "            - the first element will be dL/dw\n",
    "            - the second element will be dL/db\n",
    "    \"\"\"\n",
    "    y_hat = self.forward(x)\n",
    "    return ((y_hat - y) * x, (y_hat - y))\n",
    "\n",
    "# A quick hack to bind this function as the SimpleLinearRegression.gradient() method\n",
    "SimpleLinearRegression.gradient = gradient\n",
    "\n",
    "\n",
    "## Quick test: This should return (6.0, 3.0)\n",
    "model = SimpleLinearRegression()\n",
    "model.w = 3\n",
    "model.b = 2\n",
    "x = 2.0\n",
    "y = 5.0\n",
    "(dLdw, dLdb) = model.gradient(x, y)\n",
    "print(dLdw) # should print 6.0\n",
    "print(dLdb) # should print 3.0"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "3.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBw1pRVN-heL"
   },
   "source": [
    "So, in this example, $\\frac{\\partial L}{\\partial w}=6.0$ suggests that when $w$ increases by a very tiny amount, $L$ will increase by 6.0 times that amount.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCNctFEY0yiT"
   },
   "source": [
    "#### Gradient descent\n",
    "\n",
    "Now that we have our gradients, let us try to optimise the parameters $w$ and $b$ of our model to minimise the loss. We will use the gradient descent algorithm for this, as discussed in the lectures. \n",
    "\n",
    "You may reimplement the code provided in the lectures, replacing some of the code by reusing the `forward()`, `loss()` and `gradient()` methods of `SimpleLinearRegression` that you implemented earlier. This will help make your code more modular and readable, and help to improve your understanding of gradient descent at a more abstract level.\n",
    "\n",
    "You should be able to obtain $w \\approx 2$ and $b \\approx 1$ by the end of training if you have implemented everything correctly. Also experiment with the learning rate and the number of epochs and observe the effects."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLdqp4ESCpsR",
    "outputId": "2e246af4-ab8c-4e9a-bd69-2b28c3abb101"
   },
   "source": [
    "model = SimpleLinearRegression()\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    error = 0.0\n",
    "    grad_w = 0.0\n",
    "    grad_b = 0.0\n",
    "    for (x, y) in zip(x_train, y_train):\n",
    "        (dLdw, dLdb) = model.gradient(x, y)\n",
    "        grad_w += dLdw\n",
    "        grad_b += dLdb\n",
    "        error += model.loss(x, y)\n",
    "    model.w = model.w - learning_rate * grad_w\n",
    "    model.b = model.b - learning_rate * grad_b\n",
    "    print(f\"Epoch: {epoch}\\t w: {model.w:.2f}\\t b: {model.b:.2f}\\t L: {error:.4f}\")\n"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t w: 1.55\t b: 0.32\t L: 197.0305\n",
      "Epoch: 1\t w: 1.93\t b: 0.43\t L: 25.3799\n",
      "Epoch: 2\t w: 2.06\t b: 0.48\t L: 3.6881\n",
      "Epoch: 3\t w: 2.11\t b: 0.50\t L: 0.9371\n",
      "Epoch: 4\t w: 2.13\t b: 0.52\t L: 0.5787\n",
      "Epoch: 5\t w: 2.13\t b: 0.52\t L: 0.5228\n",
      "Epoch: 6\t w: 2.13\t b: 0.53\t L: 0.5054\n",
      "Epoch: 7\t w: 2.13\t b: 0.54\t L: 0.4931\n",
      "Epoch: 8\t w: 2.13\t b: 0.55\t L: 0.4816\n",
      "Epoch: 9\t w: 2.12\t b: 0.55\t L: 0.4706\n",
      "Epoch: 10\t w: 2.12\t b: 0.56\t L: 0.4598\n",
      "Epoch: 11\t w: 2.12\t b: 0.57\t L: 0.4492\n",
      "Epoch: 12\t w: 2.12\t b: 0.58\t L: 0.4390\n",
      "Epoch: 13\t w: 2.12\t b: 0.58\t L: 0.4289\n",
      "Epoch: 14\t w: 2.11\t b: 0.59\t L: 0.4191\n",
      "Epoch: 15\t w: 2.11\t b: 0.60\t L: 0.4096\n",
      "Epoch: 16\t w: 2.11\t b: 0.60\t L: 0.4003\n",
      "Epoch: 17\t w: 2.11\t b: 0.61\t L: 0.3912\n",
      "Epoch: 18\t w: 2.11\t b: 0.61\t L: 0.3824\n",
      "Epoch: 19\t w: 2.11\t b: 0.62\t L: 0.3737\n",
      "Epoch: 20\t w: 2.10\t b: 0.63\t L: 0.3653\n",
      "Epoch: 21\t w: 2.10\t b: 0.63\t L: 0.3571\n",
      "Epoch: 22\t w: 2.10\t b: 0.64\t L: 0.3490\n",
      "Epoch: 23\t w: 2.10\t b: 0.65\t L: 0.3412\n",
      "Epoch: 24\t w: 2.10\t b: 0.65\t L: 0.3336\n",
      "Epoch: 25\t w: 2.09\t b: 0.66\t L: 0.3261\n",
      "Epoch: 26\t w: 2.09\t b: 0.66\t L: 0.3189\n",
      "Epoch: 27\t w: 2.09\t b: 0.67\t L: 0.3118\n",
      "Epoch: 28\t w: 2.09\t b: 0.67\t L: 0.3049\n",
      "Epoch: 29\t w: 2.09\t b: 0.68\t L: 0.2981\n",
      "Epoch: 30\t w: 2.09\t b: 0.68\t L: 0.2915\n",
      "Epoch: 31\t w: 2.09\t b: 0.69\t L: 0.2851\n",
      "Epoch: 32\t w: 2.08\t b: 0.70\t L: 0.2788\n",
      "Epoch: 33\t w: 2.08\t b: 0.70\t L: 0.2727\n",
      "Epoch: 34\t w: 2.08\t b: 0.71\t L: 0.2668\n",
      "Epoch: 35\t w: 2.08\t b: 0.71\t L: 0.2610\n",
      "Epoch: 36\t w: 2.08\t b: 0.72\t L: 0.2553\n",
      "Epoch: 37\t w: 2.08\t b: 0.72\t L: 0.2498\n",
      "Epoch: 38\t w: 2.08\t b: 0.73\t L: 0.2443\n",
      "Epoch: 39\t w: 2.07\t b: 0.73\t L: 0.2391\n",
      "Epoch: 40\t w: 2.07\t b: 0.74\t L: 0.2339\n",
      "Epoch: 41\t w: 2.07\t b: 0.74\t L: 0.2289\n",
      "Epoch: 42\t w: 2.07\t b: 0.75\t L: 0.2240\n",
      "Epoch: 43\t w: 2.07\t b: 0.75\t L: 0.2193\n",
      "Epoch: 44\t w: 2.07\t b: 0.75\t L: 0.2146\n",
      "Epoch: 45\t w: 2.07\t b: 0.76\t L: 0.2101\n",
      "Epoch: 46\t w: 2.06\t b: 0.76\t L: 0.2056\n",
      "Epoch: 47\t w: 2.06\t b: 0.77\t L: 0.2013\n",
      "Epoch: 48\t w: 2.06\t b: 0.77\t L: 0.1971\n",
      "Epoch: 49\t w: 2.06\t b: 0.78\t L: 0.1930\n",
      "Epoch: 50\t w: 2.06\t b: 0.78\t L: 0.1890\n",
      "Epoch: 51\t w: 2.06\t b: 0.79\t L: 0.1851\n",
      "Epoch: 52\t w: 2.06\t b: 0.79\t L: 0.1812\n",
      "Epoch: 53\t w: 2.06\t b: 0.79\t L: 0.1775\n",
      "Epoch: 54\t w: 2.05\t b: 0.80\t L: 0.1739\n",
      "Epoch: 55\t w: 2.05\t b: 0.80\t L: 0.1703\n",
      "Epoch: 56\t w: 2.05\t b: 0.81\t L: 0.1669\n",
      "Epoch: 57\t w: 2.05\t b: 0.81\t L: 0.1635\n",
      "Epoch: 58\t w: 2.05\t b: 0.81\t L: 0.1602\n",
      "Epoch: 59\t w: 2.05\t b: 0.82\t L: 0.1570\n",
      "Epoch: 60\t w: 2.05\t b: 0.82\t L: 0.1539\n",
      "Epoch: 61\t w: 2.05\t b: 0.82\t L: 0.1508\n",
      "Epoch: 62\t w: 2.05\t b: 0.83\t L: 0.1478\n",
      "Epoch: 63\t w: 2.04\t b: 0.83\t L: 0.1449\n",
      "Epoch: 64\t w: 2.04\t b: 0.84\t L: 0.1421\n",
      "Epoch: 65\t w: 2.04\t b: 0.84\t L: 0.1393\n",
      "Epoch: 66\t w: 2.04\t b: 0.84\t L: 0.1366\n",
      "Epoch: 67\t w: 2.04\t b: 0.85\t L: 0.1340\n",
      "Epoch: 68\t w: 2.04\t b: 0.85\t L: 0.1314\n",
      "Epoch: 69\t w: 2.04\t b: 0.85\t L: 0.1289\n",
      "Epoch: 70\t w: 2.04\t b: 0.86\t L: 0.1264\n",
      "Epoch: 71\t w: 2.04\t b: 0.86\t L: 0.1241\n",
      "Epoch: 72\t w: 2.04\t b: 0.86\t L: 0.1217\n",
      "Epoch: 73\t w: 2.04\t b: 0.87\t L: 0.1195\n",
      "Epoch: 74\t w: 2.03\t b: 0.87\t L: 0.1172\n",
      "Epoch: 75\t w: 2.03\t b: 0.87\t L: 0.1151\n",
      "Epoch: 76\t w: 2.03\t b: 0.88\t L: 0.1130\n",
      "Epoch: 77\t w: 2.03\t b: 0.88\t L: 0.1109\n",
      "Epoch: 78\t w: 2.03\t b: 0.88\t L: 0.1089\n",
      "Epoch: 79\t w: 2.03\t b: 0.88\t L: 0.1069\n",
      "Epoch: 80\t w: 2.03\t b: 0.89\t L: 0.1050\n",
      "Epoch: 81\t w: 2.03\t b: 0.89\t L: 0.1032\n",
      "Epoch: 82\t w: 2.03\t b: 0.89\t L: 0.1014\n",
      "Epoch: 83\t w: 2.03\t b: 0.90\t L: 0.0996\n",
      "Epoch: 84\t w: 2.03\t b: 0.90\t L: 0.0978\n",
      "Epoch: 85\t w: 2.03\t b: 0.90\t L: 0.0962\n",
      "Epoch: 86\t w: 2.02\t b: 0.90\t L: 0.0945\n",
      "Epoch: 87\t w: 2.02\t b: 0.91\t L: 0.0929\n",
      "Epoch: 88\t w: 2.02\t b: 0.91\t L: 0.0913\n",
      "Epoch: 89\t w: 2.02\t b: 0.91\t L: 0.0898\n",
      "Epoch: 90\t w: 2.02\t b: 0.91\t L: 0.0883\n",
      "Epoch: 91\t w: 2.02\t b: 0.92\t L: 0.0869\n",
      "Epoch: 92\t w: 2.02\t b: 0.92\t L: 0.0854\n",
      "Epoch: 93\t w: 2.02\t b: 0.92\t L: 0.0841\n",
      "Epoch: 94\t w: 2.02\t b: 0.92\t L: 0.0827\n",
      "Epoch: 95\t w: 2.02\t b: 0.93\t L: 0.0814\n",
      "Epoch: 96\t w: 2.02\t b: 0.93\t L: 0.0801\n",
      "Epoch: 97\t w: 2.02\t b: 0.93\t L: 0.0788\n",
      "Epoch: 98\t w: 2.02\t b: 0.93\t L: 0.0776\n",
      "Epoch: 99\t w: 2.01\t b: 0.94\t L: 0.0764\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad3Q827dMohe"
   },
   "source": [
    "### Predictions\n",
    "\n",
    "Now that your model is trained, you can use it to predict some unknown test instances. Complete the code below to predict the output of the test set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTqEAz6GMoRH",
    "outputId": "4ba43f13-cd41-43cc-b37a-f2b9b8d66f3e"
   },
   "source": [
    "y_predictions = np.zeros((len(y_test),))\n",
    "for (i, x) in enumerate(x_test):\n",
    "    y_predictions[i] = model.forward(x)\n",
    "\n",
    "print(y_predictions)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.97412024  6.98158581 10.00398251]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2ANPLrkLVaY"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Finally, let us evaluate the linear regression model you developed. Unlike classification, we will need a different metric for regression. Recall from Lecture 3, a common evaluation metric for regression is the Mean Squared Error (MSE). We will use that for this tutorial.\n",
    "\n",
    "Complete the `mse()` function below to compute the MSE."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fvD2vNXLx6f",
    "outputId": "5bece384-789f-4ddf-eb6c-1d77a8f4c6ce"
   },
   "source": [
    "def mse(y_gold, y_prediction):\n",
    "    \"\"\" Compute the MSE given the ground truth and predictions\n",
    "\n",
    "    Args:\n",
    "        y_gold (np.ndarray): the correct ground truth values of y\n",
    "        y_prediction (np.ndarray): the predicted values of y\n",
    "\n",
    "    Returns:\n",
    "        float : MSE\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(y_gold) == len(y_prediction)  \n",
    "    \n",
    "    return np.square(y_gold - y_prediction).mean()\n",
    "\n",
    "\n",
    "# Compute the MSE on model predictions on our toy test data\n",
    "# You should be able to obtain a very small MSE rate\n",
    "print(mse(y_test, y_predictions))"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0034094007511908772\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6emtY9qqwGSe"
   },
   "source": [
    "## Iris Dataset (Extra exercise)\n",
    "\n",
    "Here is an extra optional exercise for you: try to get your simple linear regression model working on a (slightly) larger and noisier dataset. \n",
    "\n",
    "For this, we will convert the Iris dataset to use as a regression task. More specifically, our task is to predict the *petal width* of a flower (`y`) given the *sepal length* as input (`x`). The code below will give you the dataset in this format."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1yTkyHzwnUp",
    "outputId": "de45cb10-1319-4689-e531-5e5421f3e74f"
   },
   "source": [
    "import os\n",
    "\n",
    "# Download iris data if it does not exist\n",
    "if not os.path.exists(\"iris.data\"):\n",
    "    !wget -O iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
    "\n",
    "def read_dataset_as_regression(filepath):\n",
    "    \"\"\" Read in the dataset from the specified filepath\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The filepath to the dataset file\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple of (x, y), each being a numpy array. \n",
    "               - x is a numpy array with shape (N, ), \n",
    "                   where N is the number of instances\n",
    "               - y is a numpy array with shape (N, ), where each element is a \n",
    "                real-valued float, and N is the number of instances\n",
    "    \"\"\"\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for line in open(filepath):\n",
    "        if line.strip() != \"\": # handle empty rows in file\n",
    "            row = line.strip().split(\",\")\n",
    "            # extract columns 0 as x.\n",
    "            x.append(float(row[0])) \n",
    "\n",
    "            # extract column 3 as y\n",
    "            y.append(float(row[3]))\n",
    "\n",
    "    return (np.array(x), np.array(y))\n",
    "\n",
    "\n",
    "def split_dataset(x, y, test_proportion, random_generator=default_rng()):\n",
    "    \"\"\" Split dataset into training and test sets, according to the given \n",
    "        test set proportion.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
    "        y (np.ndarray): Output label, numpy array with shape (N,)\n",
    "        test_proprotion (float): the desired proportion of test examples \n",
    "                                 (0.0-1.0)\n",
    "        random_generator (np.random.Generator): A random generator\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple of (x_train, x_test, y_train, y_test) \n",
    "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
    "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
    "               - y_train (np.ndarray): Training labels, shape (N_train, )\n",
    "               - y_test (np.ndarray): Test labels, shape (N_test, )\n",
    "    \"\"\"\n",
    "\n",
    "    shuffled_indices = random_generator.permutation(len(x))\n",
    "    n_test = round(len(x) * test_proportion)\n",
    "    n_train = len(x) - n_test\n",
    "    x_train = x[shuffled_indices[:n_train]]\n",
    "    y_train = y[shuffled_indices[:n_train]]\n",
    "    x_test = x[shuffled_indices[n_train:]]\n",
    "    y_test = y[shuffled_indices[n_train:]]\n",
    "    return (x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "(x, y) = read_dataset_as_regression(\"iris.data\")\n",
    "print(x.shape)  # (150,) \n",
    "print(y.shape)  # (150,)\n",
    "\n",
    "seed = 60012\n",
    "rg = default_rng(seed)\n",
    "x_train, x_test, y_train, y_test = split_dataset(x, y, \n",
    "                                                 test_proportion=0.2, \n",
    "                                                 random_generator=rg)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n",
      "(150,)\n",
      "(120,)\n",
      "(30,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XM2yXt_0RMO"
   },
   "source": [
    "As usual, it's always a good idea to examine your data before you start:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "JTF3kSAV0ddn",
    "outputId": "287d6962-4fe0-493e-94b6-fe39b245aaab"
   },
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, c=\"blue\", edgecolor='k')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYm-2dB41JYI"
   },
   "source": [
    "### Model\n",
    "\n",
    "You can copy your `SimpleLinearRegression` class from earlier, with the`forward()`, `loss()` and `gradient()` methods that you implemented earlier. This time you can put them all together in the same place."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BfdsAHaV1IuR"
   },
   "source": [
    "class SimpleLinearRegression:\n",
    "    def __init__(self, random_generator=default_rng()):\n",
    "        self.w = random_generator.standard_normal()\n",
    "        self.b = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w * x + self.b\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        y_hat = self.forward(x)\n",
    "        return (y_hat - y)**2\n",
    "\n",
    "    def gradient(self, x, y):\n",
    "        y_hat = self.forward(x)\n",
    "        return ((y_hat - y) * x, (y_hat - y))"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6zXgIed3gTR"
   },
   "source": [
    "### Optimisation\n",
    "\n",
    "Again, you should not need to change much of your gradient descent implementation from earlier, so just copy it over. You may, however, need to tweak the learning rate and the number of epochs to obtain a reasonable output."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Txe_vse3iIH",
    "outputId": "64b5a790-eedc-4654-f445-0e8db7ee26a9"
   },
   "source": [
    "model = SimpleLinearRegression()\n",
    "\n",
    "learning_rate = 0.0001\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    error = 0.0\n",
    "    grad_w = 0.0\n",
    "    grad_b = 0.0\n",
    "    for (x, y) in zip(x_train, y_train):\n",
    "        (dLdw, dLdb) = model.gradient(x, y)\n",
    "        grad_w += dLdw\n",
    "        grad_b += dLdb\n",
    "        error += model.loss(x, y)\n",
    "    model.w = model.w - learning_rate * grad_w\n",
    "    model.b = model.b - learning_rate * grad_b\n",
    "    print(f\"Epoch: {epoch}\\t w: {model.w:.2f}\\t b: {model.b:.2f}\\t L: {error:.4f}\")\n"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t w: 0.38\t b: -0.02\t L: 376.9728\n",
      "Epoch: 1\t w: 0.31\t b: -0.03\t L: 152.8347\n",
      "Epoch: 2\t w: 0.27\t b: -0.04\t L: 80.3295\n",
      "Epoch: 3\t w: 0.25\t b: -0.04\t L: 56.8707\n",
      "Epoch: 4\t w: 0.24\t b: -0.05\t L: 49.2761\n",
      "Epoch: 5\t w: 0.23\t b: -0.05\t L: 46.8130\n",
      "Epoch: 6\t w: 0.23\t b: -0.05\t L: 46.0095\n",
      "Epoch: 7\t w: 0.23\t b: -0.05\t L: 45.7430\n",
      "Epoch: 8\t w: 0.23\t b: -0.05\t L: 45.6501\n",
      "Epoch: 9\t w: 0.23\t b: -0.05\t L: 45.6134\n",
      "Epoch: 10\t w: 0.23\t b: -0.05\t L: 45.5948\n",
      "Epoch: 11\t w: 0.23\t b: -0.05\t L: 45.5822\n",
      "Epoch: 12\t w: 0.23\t b: -0.05\t L: 45.5714\n",
      "Epoch: 13\t w: 0.23\t b: -0.06\t L: 45.5613\n",
      "Epoch: 14\t w: 0.23\t b: -0.06\t L: 45.5513\n",
      "Epoch: 15\t w: 0.23\t b: -0.06\t L: 45.5414\n",
      "Epoch: 16\t w: 0.23\t b: -0.06\t L: 45.5316\n",
      "Epoch: 17\t w: 0.23\t b: -0.06\t L: 45.5217\n",
      "Epoch: 18\t w: 0.23\t b: -0.06\t L: 45.5119\n",
      "Epoch: 19\t w: 0.23\t b: -0.06\t L: 45.5021\n",
      "Epoch: 20\t w: 0.23\t b: -0.06\t L: 45.4923\n",
      "Epoch: 21\t w: 0.23\t b: -0.06\t L: 45.4824\n",
      "Epoch: 22\t w: 0.23\t b: -0.06\t L: 45.4726\n",
      "Epoch: 23\t w: 0.23\t b: -0.06\t L: 45.4628\n",
      "Epoch: 24\t w: 0.23\t b: -0.06\t L: 45.4530\n",
      "Epoch: 25\t w: 0.23\t b: -0.06\t L: 45.4432\n",
      "Epoch: 26\t w: 0.23\t b: -0.06\t L: 45.4334\n",
      "Epoch: 27\t w: 0.23\t b: -0.07\t L: 45.4236\n",
      "Epoch: 28\t w: 0.23\t b: -0.07\t L: 45.4138\n",
      "Epoch: 29\t w: 0.23\t b: -0.07\t L: 45.4041\n",
      "Epoch: 30\t w: 0.23\t b: -0.07\t L: 45.3943\n",
      "Epoch: 31\t w: 0.23\t b: -0.07\t L: 45.3845\n",
      "Epoch: 32\t w: 0.23\t b: -0.07\t L: 45.3747\n",
      "Epoch: 33\t w: 0.23\t b: -0.07\t L: 45.3650\n",
      "Epoch: 34\t w: 0.23\t b: -0.07\t L: 45.3552\n",
      "Epoch: 35\t w: 0.23\t b: -0.07\t L: 45.3454\n",
      "Epoch: 36\t w: 0.23\t b: -0.07\t L: 45.3357\n",
      "Epoch: 37\t w: 0.23\t b: -0.07\t L: 45.3259\n",
      "Epoch: 38\t w: 0.23\t b: -0.07\t L: 45.3162\n",
      "Epoch: 39\t w: 0.23\t b: -0.07\t L: 45.3064\n",
      "Epoch: 40\t w: 0.23\t b: -0.07\t L: 45.2967\n",
      "Epoch: 41\t w: 0.23\t b: -0.07\t L: 45.2870\n",
      "Epoch: 42\t w: 0.23\t b: -0.08\t L: 45.2772\n",
      "Epoch: 43\t w: 0.23\t b: -0.08\t L: 45.2675\n",
      "Epoch: 44\t w: 0.23\t b: -0.08\t L: 45.2578\n",
      "Epoch: 45\t w: 0.23\t b: -0.08\t L: 45.2481\n",
      "Epoch: 46\t w: 0.23\t b: -0.08\t L: 45.2384\n",
      "Epoch: 47\t w: 0.23\t b: -0.08\t L: 45.2286\n",
      "Epoch: 48\t w: 0.23\t b: -0.08\t L: 45.2189\n",
      "Epoch: 49\t w: 0.23\t b: -0.08\t L: 45.2092\n",
      "Epoch: 50\t w: 0.23\t b: -0.08\t L: 45.1995\n",
      "Epoch: 51\t w: 0.23\t b: -0.08\t L: 45.1898\n",
      "Epoch: 52\t w: 0.23\t b: -0.08\t L: 45.1802\n",
      "Epoch: 53\t w: 0.23\t b: -0.08\t L: 45.1705\n",
      "Epoch: 54\t w: 0.23\t b: -0.08\t L: 45.1608\n",
      "Epoch: 55\t w: 0.23\t b: -0.08\t L: 45.1511\n",
      "Epoch: 56\t w: 0.23\t b: -0.09\t L: 45.1414\n",
      "Epoch: 57\t w: 0.23\t b: -0.09\t L: 45.1318\n",
      "Epoch: 58\t w: 0.23\t b: -0.09\t L: 45.1221\n",
      "Epoch: 59\t w: 0.23\t b: -0.09\t L: 45.1124\n",
      "Epoch: 60\t w: 0.23\t b: -0.09\t L: 45.1028\n",
      "Epoch: 61\t w: 0.23\t b: -0.09\t L: 45.0931\n",
      "Epoch: 62\t w: 0.23\t b: -0.09\t L: 45.0835\n",
      "Epoch: 63\t w: 0.23\t b: -0.09\t L: 45.0738\n",
      "Epoch: 64\t w: 0.23\t b: -0.09\t L: 45.0642\n",
      "Epoch: 65\t w: 0.23\t b: -0.09\t L: 45.0546\n",
      "Epoch: 66\t w: 0.23\t b: -0.09\t L: 45.0449\n",
      "Epoch: 67\t w: 0.23\t b: -0.09\t L: 45.0353\n",
      "Epoch: 68\t w: 0.23\t b: -0.09\t L: 45.0257\n",
      "Epoch: 69\t w: 0.23\t b: -0.09\t L: 45.0161\n",
      "Epoch: 70\t w: 0.23\t b: -0.09\t L: 45.0065\n",
      "Epoch: 71\t w: 0.23\t b: -0.10\t L: 44.9968\n",
      "Epoch: 72\t w: 0.23\t b: -0.10\t L: 44.9872\n",
      "Epoch: 73\t w: 0.23\t b: -0.10\t L: 44.9776\n",
      "Epoch: 74\t w: 0.23\t b: -0.10\t L: 44.9680\n",
      "Epoch: 75\t w: 0.23\t b: -0.10\t L: 44.9584\n",
      "Epoch: 76\t w: 0.23\t b: -0.10\t L: 44.9488\n",
      "Epoch: 77\t w: 0.23\t b: -0.10\t L: 44.9393\n",
      "Epoch: 78\t w: 0.23\t b: -0.10\t L: 44.9297\n",
      "Epoch: 79\t w: 0.23\t b: -0.10\t L: 44.9201\n",
      "Epoch: 80\t w: 0.23\t b: -0.10\t L: 44.9105\n",
      "Epoch: 81\t w: 0.23\t b: -0.10\t L: 44.9010\n",
      "Epoch: 82\t w: 0.23\t b: -0.10\t L: 44.8914\n",
      "Epoch: 83\t w: 0.23\t b: -0.10\t L: 44.8818\n",
      "Epoch: 84\t w: 0.23\t b: -0.10\t L: 44.8723\n",
      "Epoch: 85\t w: 0.23\t b: -0.10\t L: 44.8627\n",
      "Epoch: 86\t w: 0.23\t b: -0.11\t L: 44.8532\n",
      "Epoch: 87\t w: 0.23\t b: -0.11\t L: 44.8436\n",
      "Epoch: 88\t w: 0.23\t b: -0.11\t L: 44.8341\n",
      "Epoch: 89\t w: 0.23\t b: -0.11\t L: 44.8245\n",
      "Epoch: 90\t w: 0.23\t b: -0.11\t L: 44.8150\n",
      "Epoch: 91\t w: 0.23\t b: -0.11\t L: 44.8055\n",
      "Epoch: 92\t w: 0.23\t b: -0.11\t L: 44.7959\n",
      "Epoch: 93\t w: 0.23\t b: -0.11\t L: 44.7864\n",
      "Epoch: 94\t w: 0.23\t b: -0.11\t L: 44.7769\n",
      "Epoch: 95\t w: 0.24\t b: -0.11\t L: 44.7674\n",
      "Epoch: 96\t w: 0.24\t b: -0.11\t L: 44.7579\n",
      "Epoch: 97\t w: 0.24\t b: -0.11\t L: 44.7484\n",
      "Epoch: 98\t w: 0.24\t b: -0.11\t L: 44.7389\n",
      "Epoch: 99\t w: 0.24\t b: -0.11\t L: 44.7294\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P7tQX-L31LB"
   },
   "source": [
    "### Visualising your trained model\n",
    "\n",
    "You can visualise your model by plotting the line on the graph.\n",
    "\n",
    "We will also plot the test instances to get a rough idea of how well we expect it to perform."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "hUmFEg7s33Ve",
    "outputId": "63b0283a-ef11-4f03-bebc-e6fa9a48875e"
   },
   "source": [
    "# Plot training instances\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, c=\"blue\", edgecolor='k')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "# Draw the line representing the model\n",
    "xmin = x_train.min()\n",
    "ymin = model.forward(xmin)\n",
    "xmax = x_train.max()\n",
    "ymax = model.forward(xmax)\n",
    "plt.plot([xmin, xmax], [ymin, ymax], 'r-')\n",
    "\n",
    "# Plot test instances\n",
    "plt.scatter(x_test, y_test, c=\"red\", edgecolor='k')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XY37EHKe5JZT"
   },
   "source": [
    "### Predictions and evaluation\n",
    "\n",
    "Finally, predict the test instances given the model.\n",
    "\n",
    "Then evaluate the model with MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHx0gaAr5OfN",
    "outputId": "c7c987f7-ddac-434e-c7a1-6354343b7c59"
   },
   "source": [
    "y_predictions = np.zeros((len(y_test),))\n",
    "for (i, x) in enumerate(x_test):\n",
    "    y_predictions[i] = model.forward(x)\n",
    "\n",
    "print(y_predictions)\n",
    "print(y_test)\n",
    "\n",
    "print(mse(y_test, y_predictions))"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.43958686 1.36894637 1.20411855 1.74569567 1.0863844  1.18057172\n",
      " 1.62796151 1.0863844  1.0157439  1.41604003 1.48668052 1.25121221\n",
      " 1.34539954 1.10993123 1.34539954 1.03929073 0.92155658 1.51022736\n",
      " 1.32185271 1.46313369 1.20411855 1.32185271 1.32185271 1.06283756\n",
      " 1.53377419 0.89800975 1.0157439  1.15702489 0.99219707 0.94510341]\n",
      "[1.4 1.8 1.1 2.  0.2 1.1 1.9 0.4 0.1 1.8 2.3 2.4 1.8 0.2 2.3 0.1 0.2 2.3\n",
      " 1.4 1.7 2.  1.2 1.4 1.  1.4 0.1 0.3 0.4 0.2 0.3]\n",
      "0.4014772310206636\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNMajcyH7Vv5"
   },
   "source": [
    "## Summary\n",
    "\n",
    "Hopefully you have managed to deepen your understanding about linear regression by implementing the model, loss function, and the gradient descent algorithm, and putting everything together for training and testing.\n",
    "\n",
    "In the next lab exercise, we will delve a bit deeper at implementation level, and try to extend your model to handle more than one input variable. We will also start making your code a bit more efficient with vectorised implementations so that you can perform computations on multiple training instances simultaneously. This will hopefully help you get started on implementing Neural Networks (which we will unfortunately not cover in these lab exercises as it is part of your second coursework).  \n",
    "\n"
   ]
  }
 ]
}