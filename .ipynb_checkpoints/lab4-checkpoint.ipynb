{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/annnnnnnnnnie/ICMachineLearningLab/blob/master/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MBmOdXoenFw"
   },
   "source": [
    "# Lab 4: Simple Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3iS23ADfo4w"
   },
   "source": [
    "## Version history\n",
    "\n",
    "| Date | Author | Description |\n",
    "|:----:|:------:|:------------|\n",
    "2021-02-03 | Josiah Wang | First version | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EbbkgqOgZK_"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this lab exercise is for you to gain some experience implementing and training a simple linear regression model from scratch. This will help you improve your understanding of linear regression and machine learning optimisation. \n",
    "\n",
    "By the end of this lab exercise, you will have \n",
    "- implemented a simple linear regression model \n",
    "- defined and implemented a loss function\n",
    "- optimised the parameters of your model using gradient descent\n",
    "\n",
    "There will be a bit less coding required on your side in this exercise compared to previous exercises. The aim is for you to try to really understand linear regression at implementation level to complement the lectures, which will help you in future weeks as you move on to Neural Networks in your coursework assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0iaMjnIWEpe"
   },
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "In this tutorial, we will focus on the **regression task**. For simplicity, we will implement a *simple linear regression* model with one input variable and one output variable. More specifically, our task is to predict the value of $y$ given the input $x$.\n",
    "\n",
    "Let us develop our simple linear regressor with a simple toy example to make sure that our model works correctly. You can later apply it to a bigger dataset if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4w2N6AZsYJUd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# toy dataset\n",
    "x_train = np.array([1.0, 1.2, 2.0, 3.5, 4.0, 5.0])\n",
    "y_train = np.array([3.1, 3.5, 5.0, 7.9, 9.1, 10.9])\n",
    "x_test = np.array([2.5, 3.0, 4.5])\n",
    "y_test = np.array([6.0, 7.0, 10.1])\n",
    "\n",
    "# plot toy data\n",
    "plt.scatter(x_train, y_train, c=\"blue\")\n",
    "plt.scatter(x_test, y_test, c=\"red\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YagIkbxvaC-b"
   },
   "source": [
    "### Model\n",
    "\n",
    "As you should be able to see from the plot, the toy dataset can almost be perfectly modelled with a straight line. The model should also be able to pretty accurately predict the value of $y$ of the test set.\n",
    "\n",
    "Assuming you still remember your high school Maths, a straight line is represented as $y = wx + b$, where $w$ is the slope and $b$ the intercept/bias. Our objective is to find the line that best fits our training data. More specifically, we want our regressor to automatically learn the parameters $w$ and $b$ such that we can accurately predict the real-valued label ${\\hat y}$ given an example $x$. The objective is to get ${\\hat y}$ to be as close as possible to the values of $y$ of the training data (and presumably the true $y$).\n",
    "\n",
    "Let us now build our simple linear regression model. Complete the `forward()` method of the `SimpleLinearRegression` class below to return the value of the output $y$ given an input `x` and the current weight `w` and bias `b` of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JLNpOSjXUQrY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "class SimpleLinearRegression:\n",
    "    def __init__(self, random_generator=default_rng()):\n",
    "        # initialise the slope with a random value drawn from a standard normal \n",
    "        # distribution (mean=0, stddev=1)\n",
    "        self.w = random_generator.standard_normal()\n",
    "\n",
    "        # initialise bias to 0 \n",
    "        self.b = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Perform forward pass given an input x\n",
    "\n",
    "        Args:\n",
    "            x (float): input instance\n",
    "\n",
    "        Returns:\n",
    "            float: the output of the model given the current weights\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Complete this\n",
    "        return x * self.w + self.b\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        \"\"\" Placeholder for later\"\"\"\n",
    "        pass\n",
    "\n",
    "    def gradient(self, x, y):\n",
    "        \"\"\" Placeholder for later\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "## Quick test: This should return 8\n",
    "model = SimpleLinearRegression()\n",
    "model.w = 3\n",
    "model.b = 2\n",
    "x = 2\n",
    "y_hat = model.forward(x)\n",
    "print(y_hat) # should print 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9mtsqFXBNNa"
   },
   "source": [
    "### Loss function\n",
    "\n",
    "From the plot and the numbers, you may have manually worked out that the 'best' line would be $\\hat{y} \\approx 2x+1$, that is, the optimal parameter values are $w \\approx 2$ and $b \\approx 1$.\n",
    "\n",
    "What constitutes the 'best' line? We will first have to define what 'best' actually means. Intuitively, the 'best' line would be the one that goes through all training points as closely as possible.\n",
    "\n",
    "To enable our model to automatically learn what the parameter values of the 'best' line are from training examples, we will have to formally define that we mean by 'best'. We define this via a *loss function* (or cost function). For this tutorial, we will use the loss function as in the lectures - the **sum of squared differences** between the predicted label vs. the ground truth label across the training instances. \n",
    "\n",
    "$$L = \\frac{1}{2} \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 = \\frac{1}{2} \\sum_{i=1}^{N} \\left(wx^{(i)} + b - y^{(i)}\\right)^2$$\n",
    "\n",
    "Our objective is to select the parameters $\\theta = \\{ w, b \\}$ that **minimise** the loss (or error).\n",
    "\n",
    "$$\\theta = argmin_{\\theta} \\, L$$\n",
    "\n",
    "To make things easy, let us implement the loss function for a **single** instance $x$:\n",
    "\n",
    "$$L^{(i)} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$$\n",
    "\n",
    "Complete the `loss()` method of `SimpleLinearRegression` below to return the individual loss for an instance `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SZM0MAgiE2UZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "# Loss method for SimpleLinearRegression\n",
    "def loss(self, x, y):\n",
    "    \"\"\" Compute the loss for an input x\n",
    "\n",
    "    Args:\n",
    "        x (float): input instance\n",
    "        y (float): ground truth output\n",
    "\n",
    "    Returns:\n",
    "        float: the model loss for an instance x\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Complete this\n",
    "\n",
    "    return np.square(self.forward(x) - y)\n",
    "\n",
    "\n",
    "# A quick hack to bind this function as the SimpleLinearRegression.loss() method\n",
    "SimpleLinearRegression.loss = loss\n",
    "\n",
    "\n",
    "## Quick test: This should return 0.25\n",
    "model = SimpleLinearRegression()\n",
    "model.w = 3\n",
    "model.b = 2\n",
    "x = 2.0\n",
    "y = 8.5\n",
    "test_loss = model.loss(x, y)\n",
    "print(test_loss) # should print 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxwg9EPiGu2l"
   },
   "source": [
    "### Optimisation by brute force search\n",
    "\n",
    "Now, how do we get the model to automatically figure out the optimal parameters from training data? Remember that the optimal parameter values are the ones that minimise the loss function. A naive approach would be to compute the loss for different combinations of $w$ and $b$ and selecting the combination that results in the smallest loss.\n",
    "\n",
    "The code below will search for $w$ between $0$ and $4$, and for $b$ between $0$ and $2$ to find the best combination of $w$ and $b$. Examine the code, and try to understand what it is doing, then run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jyV8OCZSIdrj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(w=0.0, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.0, loss: 9.61\n",
      "    x: 1.2, y: 3.5, y_hat: 0.0, loss: 12.25\n",
      "    x: 2.0, y: 5.0, y_hat: 0.0, loss: 25.00\n",
      "    x: 3.5, y: 7.9, y_hat: 0.0, loss: 62.41\n",
      "    x: 4.0, y: 9.1, y_hat: 0.0, loss: 82.81\n",
      "    x: 5.0, y: 10.9, y_hat: 0.0, loss: 118.81\n",
      "    Loss = 155.4450\n",
      "\n",
      "(w=0.0, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.2, loss: 8.41\n",
      "    x: 1.2, y: 3.5, y_hat: 0.2, loss: 10.89\n",
      "    x: 2.0, y: 5.0, y_hat: 0.2, loss: 23.04\n",
      "    x: 3.5, y: 7.9, y_hat: 0.2, loss: 59.29\n",
      "    x: 4.0, y: 9.1, y_hat: 0.2, loss: 79.21\n",
      "    x: 5.0, y: 10.9, y_hat: 0.2, loss: 114.49\n",
      "    Loss = 147.6650\n",
      "\n",
      "(w=0.0, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.4, loss: 9.61\n",
      "    x: 2.0, y: 5.0, y_hat: 0.4, loss: 21.16\n",
      "    x: 3.5, y: 7.9, y_hat: 0.4, loss: 56.25\n",
      "    x: 4.0, y: 9.1, y_hat: 0.4, loss: 75.69\n",
      "    x: 5.0, y: 10.9, y_hat: 0.4, loss: 110.25\n",
      "    Loss = 140.1250\n",
      "\n",
      "(w=0.0, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 0.6, loss: 8.41\n",
      "    x: 2.0, y: 5.0, y_hat: 0.6, loss: 19.36\n",
      "    x: 3.5, y: 7.9, y_hat: 0.6, loss: 53.29\n",
      "    x: 4.0, y: 9.1, y_hat: 0.6, loss: 72.25\n",
      "    x: 5.0, y: 10.9, y_hat: 0.6, loss: 106.09\n",
      "    Loss = 132.8250\n",
      "\n",
      "(w=0.0, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.8, loss: 7.29\n",
      "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
      "    x: 3.5, y: 7.9, y_hat: 0.8, loss: 50.41\n",
      "    x: 4.0, y: 9.1, y_hat: 0.8, loss: 68.89\n",
      "    x: 5.0, y: 10.9, y_hat: 0.8, loss: 102.01\n",
      "    Loss = 125.7650\n",
      "\n",
      "(w=0.0, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.25\n",
      "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
      "    x: 3.5, y: 7.9, y_hat: 1.0, loss: 47.61\n",
      "    x: 4.0, y: 9.1, y_hat: 1.0, loss: 65.61\n",
      "    x: 5.0, y: 10.9, y_hat: 1.0, loss: 98.01\n",
      "    Loss = 118.9450\n",
      "\n",
      "(w=0.0, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.29\n",
      "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 1.2, loss: 44.89\n",
      "    x: 4.0, y: 9.1, y_hat: 1.2, loss: 62.41\n",
      "    x: 5.0, y: 10.9, y_hat: 1.2, loss: 94.09\n",
      "    Loss = 112.3650\n",
      "\n",
      "(w=0.0, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.41\n",
      "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 1.4, loss: 42.25\n",
      "    x: 4.0, y: 9.1, y_hat: 1.4, loss: 59.29\n",
      "    x: 5.0, y: 10.9, y_hat: 1.4, loss: 90.25\n",
      "    Loss = 106.0250\n",
      "\n",
      "(w=0.0, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.61\n",
      "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 1.6, loss: 39.69\n",
      "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
      "    x: 5.0, y: 10.9, y_hat: 1.6, loss: 86.49\n",
      "    Loss = 99.9250\n",
      "\n",
      "(w=0.0, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.89\n",
      "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 1.8, loss: 37.21\n",
      "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
      "    x: 5.0, y: 10.9, y_hat: 1.8, loss: 82.81\n",
      "    Loss = 94.0650\n",
      "\n",
      "(w=0.0, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.25\n",
      "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 2.0, loss: 34.81\n",
      "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
      "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
      "    Loss = 88.4450\n",
      "\n",
      "(w=0.2, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.2, loss: 8.41\n",
      "    x: 1.2, y: 3.5, y_hat: 0.2, loss: 10.63\n",
      "    x: 2.0, y: 5.0, y_hat: 0.4, loss: 21.16\n",
      "    x: 3.5, y: 7.9, y_hat: 0.7, loss: 51.84\n",
      "    x: 4.0, y: 9.1, y_hat: 0.8, loss: 68.89\n",
      "    x: 5.0, y: 10.9, y_hat: 1.0, loss: 98.01\n",
      "    Loss = 129.4688\n",
      "\n",
      "(w=0.2, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.4, loss: 9.36\n",
      "    x: 2.0, y: 5.0, y_hat: 0.6, loss: 19.36\n",
      "    x: 3.5, y: 7.9, y_hat: 0.9, loss: 49.00\n",
      "    x: 4.0, y: 9.1, y_hat: 1.0, loss: 65.61\n",
      "    x: 5.0, y: 10.9, y_hat: 1.2, loss: 94.09\n",
      "    Loss = 122.3568\n",
      "\n",
      "(w=0.2, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 0.6, loss: 8.18\n",
      "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
      "    x: 3.5, y: 7.9, y_hat: 1.1, loss: 46.24\n",
      "    x: 4.0, y: 9.1, y_hat: 1.2, loss: 62.41\n",
      "    x: 5.0, y: 10.9, y_hat: 1.4, loss: 90.25\n",
      "    Loss = 115.4848\n",
      "\n",
      "(w=0.2, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.8, loss: 7.08\n",
      "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
      "    x: 3.5, y: 7.9, y_hat: 1.3, loss: 43.56\n",
      "    x: 4.0, y: 9.1, y_hat: 1.4, loss: 59.29\n",
      "    x: 5.0, y: 10.9, y_hat: 1.6, loss: 86.49\n",
      "    Loss = 108.8528\n",
      "\n",
      "(w=0.2, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.05\n",
      "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 1.5, loss: 40.96\n",
      "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
      "    x: 5.0, y: 10.9, y_hat: 1.8, loss: 82.81\n",
      "    Loss = 102.4608\n",
      "\n",
      "(w=0.2, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.11\n",
      "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 1.7, loss: 38.44\n",
      "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
      "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
      "    Loss = 96.3088\n",
      "\n",
      "(w=0.2, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.24\n",
      "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 1.9, loss: 36.00\n",
      "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
      "    x: 5.0, y: 10.9, y_hat: 2.2, loss: 75.69\n",
      "    Loss = 90.3968\n",
      "\n",
      "(w=0.2, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.46\n",
      "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 2.1, loss: 33.64\n",
      "    x: 4.0, y: 9.1, y_hat: 2.2, loss: 47.61\n",
      "    x: 5.0, y: 10.9, y_hat: 2.4, loss: 72.25\n",
      "    Loss = 84.7248\n",
      "\n",
      "(w=0.2, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.76\n",
      "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 2.3, loss: 31.36\n",
      "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
      "    x: 5.0, y: 10.9, y_hat: 2.6, loss: 68.89\n",
      "    Loss = 79.2928\n",
      "\n",
      "(w=0.2, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.13\n",
      "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 2.5, loss: 29.16\n",
      "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
      "    x: 5.0, y: 10.9, y_hat: 2.8, loss: 65.61\n",
      "    Loss = 74.1008\n",
      "\n",
      "(w=0.2, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.59\n",
      "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 2.7, loss: 27.04\n",
      "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
      "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
      "    Loss = 69.1488\n",
      "\n",
      "(w=0.4, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.5, loss: 9.12\n",
      "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
      "    x: 3.5, y: 7.9, y_hat: 1.4, loss: 42.25\n",
      "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
      "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
      "    Loss = 105.8802\n",
      "\n",
      "(w=0.4, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 0.7, loss: 7.95\n",
      "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
      "    x: 3.5, y: 7.9, y_hat: 1.6, loss: 39.69\n",
      "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
      "    x: 5.0, y: 10.9, y_hat: 2.2, loss: 75.69\n",
      "    Loss = 99.4362\n",
      "\n",
      "(w=0.4, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.9, loss: 6.86\n",
      "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 1.8, loss: 37.21\n",
      "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
      "    x: 5.0, y: 10.9, y_hat: 2.4, loss: 72.25\n",
      "    Loss = 93.2322\n",
      "\n",
      "(w=0.4, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 1.1, loss: 5.86\n",
      "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 2.0, loss: 34.81\n",
      "    x: 4.0, y: 9.1, y_hat: 2.2, loss: 47.61\n",
      "    x: 5.0, y: 10.9, y_hat: 2.6, loss: 68.89\n",
      "    Loss = 87.2682\n",
      "\n",
      "(w=0.4, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.3, loss: 4.93\n",
      "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 2.2, loss: 32.49\n",
      "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
      "    x: 5.0, y: 10.9, y_hat: 2.8, loss: 65.61\n",
      "    Loss = 81.5442\n",
      "\n",
      "(w=0.4, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.5, loss: 4.08\n",
      "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 2.4, loss: 30.25\n",
      "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
      "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
      "    Loss = 76.0602\n",
      "\n",
      "(w=0.4, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.31\n",
      "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 2.6, loss: 28.09\n",
      "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
      "    x: 5.0, y: 10.9, y_hat: 3.2, loss: 59.29\n",
      "    Loss = 70.8162\n",
      "\n",
      "(w=0.4, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.62\n",
      "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 2.8, loss: 26.01\n",
      "    x: 4.0, y: 9.1, y_hat: 3.0, loss: 37.21\n",
      "    x: 5.0, y: 10.9, y_hat: 3.4, loss: 56.25\n",
      "    Loss = 65.8122\n",
      "\n",
      "(w=0.4, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 2.02\n",
      "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.0, loss: 24.01\n",
      "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
      "    x: 5.0, y: 10.9, y_hat: 3.6, loss: 53.29\n",
      "    Loss = 61.0482\n",
      "\n",
      "(w=0.4, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.49\n",
      "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.2, loss: 22.09\n",
      "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
      "    x: 5.0, y: 10.9, y_hat: 3.8, loss: 50.41\n",
      "    Loss = 56.5242\n",
      "\n",
      "(w=0.4, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 1.04\n",
      "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 3.4, loss: 20.25\n",
      "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
      "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
      "    Loss = 52.2402\n",
      "\n",
      "(w=0.6, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 0.7, loss: 7.73\n",
      "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 2.1, loss: 33.64\n",
      "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
      "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
      "    Loss = 84.6792\n",
      "\n",
      "(w=0.6, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 0.9, loss: 6.66\n",
      "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 2.3, loss: 31.36\n",
      "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
      "    x: 5.0, y: 10.9, y_hat: 3.2, loss: 59.29\n",
      "    Loss = 78.9032\n",
      "\n",
      "(w=0.6, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 1.1, loss: 5.66\n",
      "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 2.5, loss: 29.16\n",
      "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
      "    x: 5.0, y: 10.9, y_hat: 3.4, loss: 56.25\n",
      "    Loss = 73.3672\n",
      "\n",
      "(w=0.6, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.3, loss: 4.75\n",
      "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 2.7, loss: 27.04\n",
      "    x: 4.0, y: 9.1, y_hat: 3.0, loss: 37.21\n",
      "    x: 5.0, y: 10.9, y_hat: 3.6, loss: 53.29\n",
      "    Loss = 68.0712\n",
      "\n",
      "(w=0.6, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.5, loss: 3.92\n",
      "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 2.9, loss: 25.00\n",
      "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
      "    x: 5.0, y: 10.9, y_hat: 3.8, loss: 50.41\n",
      "    Loss = 63.0152\n",
      "\n",
      "(w=0.6, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.17\n",
      "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 3.1, loss: 23.04\n",
      "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
      "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
      "    Loss = 58.1992\n",
      "\n",
      "(w=0.6, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.50\n",
      "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.3, loss: 21.16\n",
      "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
      "    x: 5.0, y: 10.9, y_hat: 4.2, loss: 44.89\n",
      "    Loss = 53.6232\n",
      "\n",
      "(w=0.6, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 1.90\n",
      "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.5, loss: 19.36\n",
      "    x: 4.0, y: 9.1, y_hat: 3.8, loss: 28.09\n",
      "    x: 5.0, y: 10.9, y_hat: 4.4, loss: 42.25\n",
      "    Loss = 49.2872\n",
      "\n",
      "(w=0.6, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.39\n",
      "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 3.7, loss: 17.64\n",
      "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
      "    x: 5.0, y: 10.9, y_hat: 4.6, loss: 39.69\n",
      "    Loss = 45.1912\n",
      "\n",
      "(w=0.6, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 0.96\n",
      "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 3.9, loss: 16.00\n",
      "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
      "    x: 5.0, y: 10.9, y_hat: 4.8, loss: 37.21\n",
      "    Loss = 41.3352\n",
      "\n",
      "(w=0.6, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.61\n",
      "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 4.1, loss: 14.44\n",
      "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
      "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
      "    Loss = 37.7192\n",
      "\n",
      "(w=0.8, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.45\n",
      "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 2.8, loss: 26.01\n",
      "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
      "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
      "    Loss = 65.8658\n",
      "\n",
      "(w=0.8, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.48\n",
      "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 3.0, loss: 24.01\n",
      "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
      "    x: 5.0, y: 10.9, y_hat: 4.2, loss: 44.89\n",
      "    Loss = 60.7578\n",
      "\n",
      "(w=0.8, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.58\n",
      "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 3.2, loss: 22.09\n",
      "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
      "    x: 5.0, y: 10.9, y_hat: 4.4, loss: 42.25\n",
      "    Loss = 55.8898\n",
      "\n",
      "(w=0.8, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.76\n",
      "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 3.4, loss: 20.25\n",
      "    x: 4.0, y: 9.1, y_hat: 3.8, loss: 28.09\n",
      "    x: 5.0, y: 10.9, y_hat: 4.6, loss: 39.69\n",
      "    Loss = 51.2618\n",
      "\n",
      "(w=0.8, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 3.03\n",
      "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.6, loss: 18.49\n",
      "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
      "    x: 5.0, y: 10.9, y_hat: 4.8, loss: 37.21\n",
      "    Loss = 46.8738\n",
      "\n",
      "(w=0.8, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.37\n",
      "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.8, loss: 16.81\n",
      "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
      "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
      "    Loss = 42.7258\n",
      "\n",
      "(w=0.8, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.80\n",
      "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 4.0, loss: 15.21\n",
      "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
      "    x: 5.0, y: 10.9, y_hat: 5.2, loss: 32.49\n",
      "    Loss = 38.8178\n",
      "\n",
      "(w=0.8, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.30\n",
      "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 4.2, loss: 13.69\n",
      "    x: 4.0, y: 9.1, y_hat: 4.6, loss: 20.25\n",
      "    x: 5.0, y: 10.9, y_hat: 5.4, loss: 30.25\n",
      "    Loss = 35.1498\n",
      "\n",
      "(w=0.8, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.88\n",
      "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 4.4, loss: 12.25\n",
      "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
      "    x: 5.0, y: 10.9, y_hat: 5.6, loss: 28.09\n",
      "    Loss = 31.7218\n",
      "\n",
      "(w=0.8, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.55\n",
      "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 4.6, loss: 10.89\n",
      "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
      "    x: 5.0, y: 10.9, y_hat: 5.8, loss: 26.01\n",
      "    Loss = 28.5338\n",
      "\n",
      "(w=0.8, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.29\n",
      "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 4.8, loss: 9.61\n",
      "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
      "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
      "    Loss = 25.5858\n",
      "\n",
      "(w=1.0, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.29\n",
      "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 3.5, loss: 19.36\n",
      "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
      "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
      "    Loss = 49.4400\n",
      "\n",
      "(w=1.0, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.41\n",
      "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 3.7, loss: 17.64\n",
      "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
      "    x: 5.0, y: 10.9, y_hat: 5.2, loss: 32.49\n",
      "    Loss = 45.0000\n",
      "\n",
      "(w=1.0, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.61\n",
      "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 3.9, loss: 16.00\n",
      "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
      "    x: 5.0, y: 10.9, y_hat: 5.4, loss: 30.25\n",
      "    Loss = 40.8000\n",
      "\n",
      "(w=1.0, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.89\n",
      "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 4.1, loss: 14.44\n",
      "    x: 4.0, y: 9.1, y_hat: 4.6, loss: 20.25\n",
      "    x: 5.0, y: 10.9, y_hat: 5.6, loss: 28.09\n",
      "    Loss = 36.8400\n",
      "\n",
      "(w=1.0, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.25\n",
      "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 4.3, loss: 12.96\n",
      "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
      "    x: 5.0, y: 10.9, y_hat: 5.8, loss: 26.01\n",
      "    Loss = 33.1200\n",
      "\n",
      "(w=1.0, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.69\n",
      "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 4.5, loss: 11.56\n",
      "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
      "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
      "    Loss = 29.6400\n",
      "\n",
      "(w=1.0, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.21\n",
      "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 4.7, loss: 10.24\n",
      "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
      "    x: 5.0, y: 10.9, y_hat: 6.2, loss: 22.09\n",
      "    Loss = 26.4000\n",
      "\n",
      "(w=1.0, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.81\n",
      "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 4.9, loss: 9.00\n",
      "    x: 4.0, y: 9.1, y_hat: 5.4, loss: 13.69\n",
      "    x: 5.0, y: 10.9, y_hat: 6.4, loss: 20.25\n",
      "    Loss = 23.4000\n",
      "\n",
      "(w=1.0, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.49\n",
      "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 5.1, loss: 7.84\n",
      "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
      "    x: 5.0, y: 10.9, y_hat: 6.6, loss: 18.49\n",
      "    Loss = 20.6400\n",
      "\n",
      "(w=1.0, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.25\n",
      "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 5.3, loss: 6.76\n",
      "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
      "    x: 5.0, y: 10.9, y_hat: 6.8, loss: 16.81\n",
      "    Loss = 18.1200\n",
      "\n",
      "(w=1.0, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.09\n",
      "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 5.5, loss: 5.76\n",
      "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
      "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
      "    Loss = 15.8400\n",
      "\n",
      "(w=1.2, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.24\n",
      "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 4.2, loss: 13.69\n",
      "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
      "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
      "    Loss = 35.4018\n",
      "\n",
      "(w=1.2, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.46\n",
      "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 4.4, loss: 12.25\n",
      "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
      "    x: 5.0, y: 10.9, y_hat: 6.2, loss: 22.09\n",
      "    Loss = 31.6298\n",
      "\n",
      "(w=1.2, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.76\n",
      "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 4.6, loss: 10.89\n",
      "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
      "    x: 5.0, y: 10.9, y_hat: 6.4, loss: 20.25\n",
      "    Loss = 28.0978\n",
      "\n",
      "(w=1.2, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.13\n",
      "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 4.8, loss: 9.61\n",
      "    x: 4.0, y: 9.1, y_hat: 5.4, loss: 13.69\n",
      "    x: 5.0, y: 10.9, y_hat: 6.6, loss: 18.49\n",
      "    Loss = 24.8058\n",
      "\n",
      "(w=1.2, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.59\n",
      "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 5.0, loss: 8.41\n",
      "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
      "    x: 5.0, y: 10.9, y_hat: 6.8, loss: 16.81\n",
      "    Loss = 21.7538\n",
      "\n",
      "(w=1.2, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.12\n",
      "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 5.2, loss: 7.29\n",
      "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
      "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
      "    Loss = 18.9418\n",
      "\n",
      "(w=1.2, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.74\n",
      "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 5.4, loss: 6.25\n",
      "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
      "    x: 5.0, y: 10.9, y_hat: 7.2, loss: 13.69\n",
      "    Loss = 16.3698\n",
      "\n",
      "(w=1.2, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.44\n",
      "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 5.6, loss: 5.29\n",
      "    x: 4.0, y: 9.1, y_hat: 6.2, loss: 8.41\n",
      "    x: 5.0, y: 10.9, y_hat: 7.4, loss: 12.25\n",
      "    Loss = 14.0378\n",
      "\n",
      "(w=1.2, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.21\n",
      "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 5.8, loss: 4.41\n",
      "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
      "    x: 5.0, y: 10.9, y_hat: 7.6, loss: 10.89\n",
      "    Loss = 11.9458\n",
      "\n",
      "(w=1.2, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.07\n",
      "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 6.0, loss: 3.61\n",
      "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
      "    x: 5.0, y: 10.9, y_hat: 7.8, loss: 9.61\n",
      "    Loss = 10.0938\n",
      "\n",
      "(w=1.2, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 6.2, loss: 2.89\n",
      "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
      "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
      "    Loss = 8.4818\n",
      "\n",
      "(w=1.4, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.31\n",
      "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 4.9, loss: 9.00\n",
      "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
      "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
      "    Loss = 23.7512\n",
      "\n",
      "(w=1.4, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.62\n",
      "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 5.1, loss: 7.84\n",
      "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
      "    x: 5.0, y: 10.9, y_hat: 7.2, loss: 13.69\n",
      "    Loss = 20.6472\n",
      "\n",
      "(w=1.4, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 2.02\n",
      "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 5.3, loss: 6.76\n",
      "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
      "    x: 5.0, y: 10.9, y_hat: 7.4, loss: 12.25\n",
      "    Loss = 17.7832\n",
      "\n",
      "(w=1.4, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.49\n",
      "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 5.5, loss: 5.76\n",
      "    x: 4.0, y: 9.1, y_hat: 6.2, loss: 8.41\n",
      "    x: 5.0, y: 10.9, y_hat: 7.6, loss: 10.89\n",
      "    Loss = 15.1592\n",
      "\n",
      "(w=1.4, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 1.04\n",
      "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 5.7, loss: 4.84\n",
      "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
      "    x: 5.0, y: 10.9, y_hat: 7.8, loss: 9.61\n",
      "    Loss = 12.7752\n",
      "\n",
      "(w=1.4, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.67\n",
      "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 5.9, loss: 4.00\n",
      "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
      "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
      "    Loss = 10.6312\n",
      "\n",
      "(w=1.4, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.38\n",
      "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 6.1, loss: 3.24\n",
      "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
      "    x: 5.0, y: 10.9, y_hat: 8.2, loss: 7.29\n",
      "    Loss = 8.7272\n",
      "\n",
      "(w=1.4, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.18\n",
      "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 6.3, loss: 2.56\n",
      "    x: 4.0, y: 9.1, y_hat: 7.0, loss: 4.41\n",
      "    x: 5.0, y: 10.9, y_hat: 8.4, loss: 6.25\n",
      "    Loss = 7.0632\n",
      "\n",
      "(w=1.4, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.05\n",
      "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 6.5, loss: 1.96\n",
      "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
      "    x: 5.0, y: 10.9, y_hat: 8.6, loss: 5.29\n",
      "    Loss = 5.6392\n",
      "\n",
      "(w=1.4, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 6.7, loss: 1.44\n",
      "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
      "    x: 5.0, y: 10.9, y_hat: 8.8, loss: 4.41\n",
      "    Loss = 4.4552\n",
      "\n",
      "(w=1.4, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.03\n",
      "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 6.9, loss: 1.00\n",
      "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
      "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
      "    Loss = 3.5112\n",
      "\n",
      "(w=1.6, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.50\n",
      "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 5.6, loss: 5.29\n",
      "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
      "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
      "    Loss = 14.4882\n",
      "\n",
      "(w=1.6, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 1.90\n",
      "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 5.8, loss: 4.41\n",
      "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
      "    x: 5.0, y: 10.9, y_hat: 8.2, loss: 7.29\n",
      "    Loss = 12.0522\n",
      "\n",
      "(w=1.6, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.39\n",
      "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 6.0, loss: 3.61\n",
      "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
      "    x: 5.0, y: 10.9, y_hat: 8.4, loss: 6.25\n",
      "    Loss = 9.8562\n",
      "\n",
      "(w=1.6, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 0.96\n",
      "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 6.2, loss: 2.89\n",
      "    x: 4.0, y: 9.1, y_hat: 7.0, loss: 4.41\n",
      "    x: 5.0, y: 10.9, y_hat: 8.6, loss: 5.29\n",
      "    Loss = 7.9002\n",
      "\n",
      "(w=1.6, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.61\n",
      "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 6.4, loss: 2.25\n",
      "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
      "    x: 5.0, y: 10.9, y_hat: 8.8, loss: 4.41\n",
      "    Loss = 6.1842\n",
      "\n",
      "(w=1.6, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.34\n",
      "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 6.6, loss: 1.69\n",
      "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
      "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
      "    Loss = 4.7082\n",
      "\n",
      "(w=1.6, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.14\n",
      "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 6.8, loss: 1.21\n",
      "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
      "    x: 5.0, y: 10.9, y_hat: 9.2, loss: 2.89\n",
      "    Loss = 3.4722\n",
      "\n",
      "(w=1.6, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.03\n",
      "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 7.0, loss: 0.81\n",
      "    x: 4.0, y: 9.1, y_hat: 7.8, loss: 1.69\n",
      "    x: 5.0, y: 10.9, y_hat: 9.4, loss: 2.25\n",
      "    Loss = 2.4762\n",
      "\n",
      "(w=1.6, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 7.2, loss: 0.49\n",
      "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
      "    x: 5.0, y: 10.9, y_hat: 9.6, loss: 1.69\n",
      "    Loss = 1.7202\n",
      "\n",
      "(w=1.6, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.05\n",
      "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
      "    x: 3.5, y: 7.9, y_hat: 7.4, loss: 0.25\n",
      "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
      "    x: 5.0, y: 10.9, y_hat: 9.8, loss: 1.21\n",
      "    Loss = 1.2042\n",
      "\n",
      "(w=1.6, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.18\n",
      "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 7.6, loss: 0.09\n",
      "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
      "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
      "    Loss = 0.9282\n",
      "\n",
      "(w=1.8, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.80\n",
      "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 6.3, loss: 2.56\n",
      "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
      "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
      "    Loss = 7.6128\n",
      "\n",
      "(w=1.8, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.30\n",
      "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 6.5, loss: 1.96\n",
      "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
      "    x: 5.0, y: 10.9, y_hat: 9.2, loss: 2.89\n",
      "    Loss = 5.8448\n",
      "\n",
      "(w=1.8, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.88\n",
      "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 6.7, loss: 1.44\n",
      "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
      "    x: 5.0, y: 10.9, y_hat: 9.4, loss: 2.25\n",
      "    Loss = 4.3168\n",
      "\n",
      "(w=1.8, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.55\n",
      "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 6.9, loss: 1.00\n",
      "    x: 4.0, y: 9.1, y_hat: 7.8, loss: 1.69\n",
      "    x: 5.0, y: 10.9, y_hat: 9.6, loss: 1.69\n",
      "    Loss = 3.0288\n",
      "\n",
      "(w=1.8, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.29\n",
      "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 7.1, loss: 0.64\n",
      "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
      "    x: 5.0, y: 10.9, y_hat: 9.8, loss: 1.21\n",
      "    Loss = 1.9808\n",
      "\n",
      "(w=1.8, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.12\n",
      "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 7.3, loss: 0.36\n",
      "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
      "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
      "    Loss = 1.1728\n",
      "\n",
      "(w=1.8, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.02\n",
      "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 7.5, loss: 0.16\n",
      "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
      "    x: 5.0, y: 10.9, y_hat: 10.2, loss: 0.49\n",
      "    Loss = 0.6048\n",
      "\n",
      "(w=1.8, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
      "    x: 3.5, y: 7.9, y_hat: 7.7, loss: 0.04\n",
      "    x: 4.0, y: 9.1, y_hat: 8.6, loss: 0.25\n",
      "    x: 5.0, y: 10.9, y_hat: 10.4, loss: 0.25\n",
      "    Loss = 0.2768\n",
      "\n",
      "(w=1.8, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.07\n",
      "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 7.9, loss: 0.00\n",
      "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
      "    x: 5.0, y: 10.9, y_hat: 10.6, loss: 0.09\n",
      "    Loss = 0.1888\n",
      "\n",
      "(w=1.8, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.21\n",
      "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 8.1, loss: 0.04\n",
      "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
      "    x: 5.0, y: 10.9, y_hat: 10.8, loss: 0.01\n",
      "    Loss = 0.3408\n",
      "\n",
      "(w=1.8, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.44\n",
      "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 8.3, loss: 0.16\n",
      "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
      "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
      "    Loss = 0.7328\n",
      "\n",
      "(w=2.0, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.21\n",
      "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 7.0, loss: 0.81\n",
      "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
      "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
      "    Loss = 3.1250\n",
      "\n",
      "(w=2.0, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.81\n",
      "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 7.2, loss: 0.49\n",
      "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
      "    x: 5.0, y: 10.9, y_hat: 10.2, loss: 0.49\n",
      "    Loss = 2.0250\n",
      "\n",
      "(w=2.0, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.49\n",
      "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 7.4, loss: 0.25\n",
      "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
      "    x: 5.0, y: 10.9, y_hat: 10.4, loss: 0.25\n",
      "    Loss = 1.1650\n",
      "\n",
      "(w=2.0, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.25\n",
      "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 7.6, loss: 0.09\n",
      "    x: 4.0, y: 9.1, y_hat: 8.6, loss: 0.25\n",
      "    x: 5.0, y: 10.9, y_hat: 10.6, loss: 0.09\n",
      "    Loss = 0.5450\n",
      "\n",
      "(w=2.0, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.09\n",
      "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 7.8, loss: 0.01\n",
      "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
      "    x: 5.0, y: 10.9, y_hat: 10.8, loss: 0.01\n",
      "    Loss = 0.1650\n",
      "\n",
      "(w=2.0, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.01\n",
      "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
      "    x: 3.5, y: 7.9, y_hat: 8.0, loss: 0.01\n",
      "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
      "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
      "    Loss = 0.0250\n",
      "\n",
      "(w=2.0, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.01\n",
      "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 8.2, loss: 0.09\n",
      "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
      "    x: 5.0, y: 10.9, y_hat: 11.2, loss: 0.09\n",
      "    Loss = 0.1250\n",
      "\n",
      "(w=2.0, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.09\n",
      "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 8.4, loss: 0.25\n",
      "    x: 4.0, y: 9.1, y_hat: 9.4, loss: 0.09\n",
      "    x: 5.0, y: 10.9, y_hat: 11.4, loss: 0.25\n",
      "    Loss = 0.4650\n",
      "\n",
      "(w=2.0, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.25\n",
      "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 8.6, loss: 0.49\n",
      "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
      "    x: 5.0, y: 10.9, y_hat: 11.6, loss: 0.49\n",
      "    Loss = 1.0450\n",
      "\n",
      "(w=2.0, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.49\n",
      "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 8.8, loss: 0.81\n",
      "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
      "    x: 5.0, y: 10.9, y_hat: 11.8, loss: 0.81\n",
      "    Loss = 1.8650\n",
      "\n",
      "(w=2.0, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.81\n",
      "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 9.0, loss: 1.21\n",
      "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
      "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
      "    Loss = 2.9250\n",
      "\n",
      "(w=2.2, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.74\n",
      "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 7.7, loss: 0.04\n",
      "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
      "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
      "    Loss = 1.0248\n",
      "\n",
      "(w=2.2, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.44\n",
      "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 7.9, loss: 0.00\n",
      "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
      "    x: 5.0, y: 10.9, y_hat: 11.2, loss: 0.09\n",
      "    Loss = 0.5928\n",
      "\n",
      "(w=2.2, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.21\n",
      "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 8.1, loss: 0.04\n",
      "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
      "    x: 5.0, y: 10.9, y_hat: 11.4, loss: 0.25\n",
      "    Loss = 0.4008\n",
      "\n",
      "(w=2.2, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.07\n",
      "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
      "    x: 3.5, y: 7.9, y_hat: 8.3, loss: 0.16\n",
      "    x: 4.0, y: 9.1, y_hat: 9.4, loss: 0.09\n",
      "    x: 5.0, y: 10.9, y_hat: 11.6, loss: 0.49\n",
      "    Loss = 0.4488\n",
      "\n",
      "(w=2.2, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 8.5, loss: 0.36\n",
      "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
      "    x: 5.0, y: 10.9, y_hat: 11.8, loss: 0.81\n",
      "    Loss = 0.7368\n",
      "\n",
      "(w=2.2, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.02\n",
      "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 8.7, loss: 0.64\n",
      "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
      "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
      "    Loss = 1.2648\n",
      "\n",
      "(w=2.2, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.12\n",
      "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 8.9, loss: 1.00\n",
      "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
      "    x: 5.0, y: 10.9, y_hat: 12.2, loss: 1.69\n",
      "    Loss = 2.0328\n",
      "\n",
      "(w=2.2, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.29\n",
      "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 9.1, loss: 1.44\n",
      "    x: 4.0, y: 9.1, y_hat: 10.2, loss: 1.21\n",
      "    x: 5.0, y: 10.9, y_hat: 12.4, loss: 2.25\n",
      "    Loss = 3.0408\n",
      "\n",
      "(w=2.2, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.55\n",
      "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 9.3, loss: 1.96\n",
      "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
      "    x: 5.0, y: 10.9, y_hat: 12.6, loss: 2.89\n",
      "    Loss = 4.2888\n",
      "\n",
      "(w=2.2, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.88\n",
      "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 9.5, loss: 2.56\n",
      "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
      "    x: 5.0, y: 10.9, y_hat: 12.8, loss: 3.61\n",
      "    Loss = 5.7768\n",
      "\n",
      "(w=2.2, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.30\n",
      "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 9.7, loss: 3.24\n",
      "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
      "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
      "    Loss = 7.5048\n",
      "\n",
      "(w=2.4, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.38\n",
      "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 8.4, loss: 0.25\n",
      "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
      "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
      "    Loss = 1.3122\n",
      "\n",
      "(w=2.4, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.18\n",
      "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
      "    x: 3.5, y: 7.9, y_hat: 8.6, loss: 0.49\n",
      "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
      "    x: 5.0, y: 10.9, y_hat: 12.2, loss: 1.69\n",
      "    Loss = 1.5482\n",
      "\n",
      "(w=2.4, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.05\n",
      "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 8.8, loss: 0.81\n",
      "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
      "    x: 5.0, y: 10.9, y_hat: 12.4, loss: 2.25\n",
      "    Loss = 2.0242\n",
      "\n",
      "(w=2.4, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 9.0, loss: 1.21\n",
      "    x: 4.0, y: 9.1, y_hat: 10.2, loss: 1.21\n",
      "    x: 5.0, y: 10.9, y_hat: 12.6, loss: 2.89\n",
      "    Loss = 2.7402\n",
      "\n",
      "(w=2.4, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.03\n",
      "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 9.2, loss: 1.69\n",
      "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
      "    x: 5.0, y: 10.9, y_hat: 12.8, loss: 3.61\n",
      "    Loss = 3.6962\n",
      "\n",
      "(w=2.4, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.14\n",
      "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 9.4, loss: 2.25\n",
      "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
      "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
      "    Loss = 4.8922\n",
      "\n",
      "(w=2.4, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.34\n",
      "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 9.6, loss: 2.89\n",
      "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
      "    x: 5.0, y: 10.9, y_hat: 13.2, loss: 5.29\n",
      "    Loss = 6.3282\n",
      "\n",
      "(w=2.4, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.61\n",
      "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 9.8, loss: 3.61\n",
      "    x: 4.0, y: 9.1, y_hat: 11.0, loss: 3.61\n",
      "    x: 5.0, y: 10.9, y_hat: 13.4, loss: 6.25\n",
      "    Loss = 8.0042\n",
      "\n",
      "(w=2.4, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 0.96\n",
      "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 10.0, loss: 4.41\n",
      "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
      "    x: 5.0, y: 10.9, y_hat: 13.6, loss: 7.29\n",
      "    Loss = 9.9202\n",
      "\n",
      "(w=2.4, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.39\n",
      "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 10.2, loss: 5.29\n",
      "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
      "    x: 5.0, y: 10.9, y_hat: 13.8, loss: 8.41\n",
      "    Loss = 12.0762\n",
      "\n",
      "(w=2.4, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 1.90\n",
      "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 10.4, loss: 6.25\n",
      "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
      "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
      "    Loss = 14.4722\n",
      "\n",
      "(w=2.6, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.14\n",
      "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
      "    x: 3.5, y: 7.9, y_hat: 9.1, loss: 1.44\n",
      "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
      "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
      "    Loss = 3.9872\n",
      "\n",
      "(w=2.6, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.03\n",
      "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
      "    x: 3.5, y: 7.9, y_hat: 9.3, loss: 1.96\n",
      "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
      "    x: 5.0, y: 10.9, y_hat: 13.2, loss: 5.29\n",
      "    Loss = 4.8912\n",
      "\n",
      "(w=2.6, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 9.5, loss: 2.56\n",
      "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
      "    x: 5.0, y: 10.9, y_hat: 13.4, loss: 6.25\n",
      "    Loss = 6.0352\n",
      "\n",
      "(w=2.6, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.05\n",
      "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 9.7, loss: 3.24\n",
      "    x: 4.0, y: 9.1, y_hat: 11.0, loss: 3.61\n",
      "    x: 5.0, y: 10.9, y_hat: 13.6, loss: 7.29\n",
      "    Loss = 7.4192\n",
      "\n",
      "(w=2.6, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.18\n",
      "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 9.9, loss: 4.00\n",
      "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
      "    x: 5.0, y: 10.9, y_hat: 13.8, loss: 8.41\n",
      "    Loss = 9.0432\n",
      "\n",
      "(w=2.6, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.38\n",
      "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 10.1, loss: 4.84\n",
      "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
      "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
      "    Loss = 10.9072\n",
      "\n",
      "(w=2.6, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.67\n",
      "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 10.3, loss: 5.76\n",
      "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
      "    x: 5.0, y: 10.9, y_hat: 14.2, loss: 10.89\n",
      "    Loss = 13.0112\n",
      "\n",
      "(w=2.6, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 1.04\n",
      "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 10.5, loss: 6.76\n",
      "    x: 4.0, y: 9.1, y_hat: 11.8, loss: 7.29\n",
      "    x: 5.0, y: 10.9, y_hat: 14.4, loss: 12.25\n",
      "    Loss = 15.3552\n",
      "\n",
      "(w=2.6, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.49\n",
      "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 10.7, loss: 7.84\n",
      "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
      "    x: 5.0, y: 10.9, y_hat: 14.6, loss: 13.69\n",
      "    Loss = 17.9392\n",
      "\n",
      "(w=2.6, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 2.02\n",
      "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 10.9, loss: 9.00\n",
      "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
      "    x: 5.0, y: 10.9, y_hat: 14.8, loss: 15.21\n",
      "    Loss = 20.7632\n",
      "\n",
      "(w=2.6, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.62\n",
      "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 11.1, loss: 10.24\n",
      "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
      "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
      "    Loss = 23.8272\n",
      "\n",
      "(w=2.8, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.02\n",
      "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
      "    x: 3.5, y: 7.9, y_hat: 9.8, loss: 3.61\n",
      "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
      "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
      "    Loss = 9.0498\n",
      "\n",
      "(w=2.8, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.00\n",
      "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
      "    x: 3.5, y: 7.9, y_hat: 10.0, loss: 4.41\n",
      "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
      "    x: 5.0, y: 10.9, y_hat: 14.2, loss: 10.89\n",
      "    Loss = 10.6218\n",
      "\n",
      "(w=2.8, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.07\n",
      "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 10.2, loss: 5.29\n",
      "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
      "    x: 5.0, y: 10.9, y_hat: 14.4, loss: 12.25\n",
      "    Loss = 12.4338\n",
      "\n",
      "(w=2.8, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.21\n",
      "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 10.4, loss: 6.25\n",
      "    x: 4.0, y: 9.1, y_hat: 11.8, loss: 7.29\n",
      "    x: 5.0, y: 10.9, y_hat: 14.6, loss: 13.69\n",
      "    Loss = 14.4858\n",
      "\n",
      "(w=2.8, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.44\n",
      "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 10.6, loss: 7.29\n",
      "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
      "    x: 5.0, y: 10.9, y_hat: 14.8, loss: 15.21\n",
      "    Loss = 16.7778\n",
      "\n",
      "(w=2.8, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.74\n",
      "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 10.8, loss: 8.41\n",
      "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
      "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
      "    Loss = 19.3098\n",
      "\n",
      "(w=2.8, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.12\n",
      "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 11.0, loss: 9.61\n",
      "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
      "    x: 5.0, y: 10.9, y_hat: 15.2, loss: 18.49\n",
      "    Loss = 22.0818\n",
      "\n",
      "(w=2.8, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.59\n",
      "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 11.2, loss: 10.89\n",
      "    x: 4.0, y: 9.1, y_hat: 12.6, loss: 12.25\n",
      "    x: 5.0, y: 10.9, y_hat: 15.4, loss: 20.25\n",
      "    Loss = 25.0938\n",
      "\n",
      "(w=2.8, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.13\n",
      "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 11.4, loss: 12.25\n",
      "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
      "    x: 5.0, y: 10.9, y_hat: 15.6, loss: 22.09\n",
      "    Loss = 28.3458\n",
      "\n",
      "(w=2.8, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.76\n",
      "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 11.6, loss: 13.69\n",
      "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
      "    x: 5.0, y: 10.9, y_hat: 15.8, loss: 24.01\n",
      "    Loss = 31.8378\n",
      "\n",
      "(w=2.8, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.46\n",
      "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 11.8, loss: 15.21\n",
      "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
      "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
      "    Loss = 35.5698\n",
      "\n",
      "(w=3.0, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.01\n",
      "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
      "    x: 3.5, y: 7.9, y_hat: 10.5, loss: 6.76\n",
      "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
      "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
      "    Loss = 16.5000\n",
      "\n",
      "(w=3.0, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.09\n",
      "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
      "    x: 3.5, y: 7.9, y_hat: 10.7, loss: 7.84\n",
      "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
      "    x: 5.0, y: 10.9, y_hat: 15.2, loss: 18.49\n",
      "    Loss = 18.7400\n",
      "\n",
      "(w=3.0, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.25\n",
      "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 10.9, loss: 9.00\n",
      "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
      "    x: 5.0, y: 10.9, y_hat: 15.4, loss: 20.25\n",
      "    Loss = 21.2200\n",
      "\n",
      "(w=3.0, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.49\n",
      "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 11.1, loss: 10.24\n",
      "    x: 4.0, y: 9.1, y_hat: 12.6, loss: 12.25\n",
      "    x: 5.0, y: 10.9, y_hat: 15.6, loss: 22.09\n",
      "    Loss = 23.9400\n",
      "\n",
      "(w=3.0, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.81\n",
      "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 11.3, loss: 11.56\n",
      "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
      "    x: 5.0, y: 10.9, y_hat: 15.8, loss: 24.01\n",
      "    Loss = 26.9000\n",
      "\n",
      "(w=3.0, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.21\n",
      "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 11.5, loss: 12.96\n",
      "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
      "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
      "    Loss = 30.1000\n",
      "\n",
      "(w=3.0, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.69\n",
      "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 11.7, loss: 14.44\n",
      "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
      "    x: 5.0, y: 10.9, y_hat: 16.2, loss: 28.09\n",
      "    Loss = 33.5400\n",
      "\n",
      "(w=3.0, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.25\n",
      "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 11.9, loss: 16.00\n",
      "    x: 4.0, y: 9.1, y_hat: 13.4, loss: 18.49\n",
      "    x: 5.0, y: 10.9, y_hat: 16.4, loss: 30.25\n",
      "    Loss = 37.2200\n",
      "\n",
      "(w=3.0, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.89\n",
      "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 12.1, loss: 17.64\n",
      "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
      "    x: 5.0, y: 10.9, y_hat: 16.6, loss: 32.49\n",
      "    Loss = 41.1400\n",
      "\n",
      "(w=3.0, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.61\n",
      "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 12.3, loss: 19.36\n",
      "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
      "    x: 5.0, y: 10.9, y_hat: 16.8, loss: 34.81\n",
      "    Loss = 45.3000\n",
      "\n",
      "(w=3.0, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.41\n",
      "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 12.5, loss: 21.16\n",
      "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
      "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
      "    Loss = 49.7000\n",
      "\n",
      "(w=3.2, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
      "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.12\n",
      "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
      "    x: 3.5, y: 7.9, y_hat: 11.2, loss: 10.89\n",
      "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
      "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
      "    Loss = 26.3378\n",
      "\n",
      "(w=3.2, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.29\n",
      "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
      "    x: 3.5, y: 7.9, y_hat: 11.4, loss: 12.25\n",
      "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
      "    x: 5.0, y: 10.9, y_hat: 16.2, loss: 28.09\n",
      "    Loss = 29.2458\n",
      "\n",
      "(w=3.2, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.55\n",
      "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 11.6, loss: 13.69\n",
      "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
      "    x: 5.0, y: 10.9, y_hat: 16.4, loss: 30.25\n",
      "    Loss = 32.3938\n",
      "\n",
      "(w=3.2, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.88\n",
      "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 11.8, loss: 15.21\n",
      "    x: 4.0, y: 9.1, y_hat: 13.4, loss: 18.49\n",
      "    x: 5.0, y: 10.9, y_hat: 16.6, loss: 32.49\n",
      "    Loss = 35.7818\n",
      "\n",
      "(w=3.2, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.30\n",
      "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 12.0, loss: 16.81\n",
      "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
      "    x: 5.0, y: 10.9, y_hat: 16.8, loss: 34.81\n",
      "    Loss = 39.4098\n",
      "\n",
      "(w=3.2, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.80\n",
      "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 12.2, loss: 18.49\n",
      "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
      "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
      "    Loss = 43.2778\n",
      "\n",
      "(w=3.2, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.37\n",
      "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 12.4, loss: 20.25\n",
      "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
      "    x: 5.0, y: 10.9, y_hat: 17.2, loss: 39.69\n",
      "    Loss = 47.3858\n",
      "\n",
      "(w=3.2, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 3.03\n",
      "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 12.6, loss: 22.09\n",
      "    x: 4.0, y: 9.1, y_hat: 14.2, loss: 26.01\n",
      "    x: 5.0, y: 10.9, y_hat: 17.4, loss: 42.25\n",
      "    Loss = 51.7338\n",
      "\n",
      "(w=3.2, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.76\n",
      "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 12.8, loss: 24.01\n",
      "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
      "    x: 5.0, y: 10.9, y_hat: 17.6, loss: 44.89\n",
      "    Loss = 56.3218\n",
      "\n",
      "(w=3.2, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.58\n",
      "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 13.0, loss: 26.01\n",
      "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
      "    x: 5.0, y: 10.9, y_hat: 17.8, loss: 47.61\n",
      "    Loss = 61.1498\n",
      "\n",
      "(w=3.2, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.48\n",
      "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 13.2, loss: 28.09\n",
      "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
      "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
      "    Loss = 66.2178\n",
      "\n",
      "(w=3.4, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
      "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.34\n",
      "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
      "    x: 3.5, y: 7.9, y_hat: 11.9, loss: 16.00\n",
      "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
      "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
      "    Loss = 38.5632\n",
      "\n",
      "(w=3.4, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.61\n",
      "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
      "    x: 3.5, y: 7.9, y_hat: 12.1, loss: 17.64\n",
      "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
      "    x: 5.0, y: 10.9, y_hat: 17.2, loss: 39.69\n",
      "    Loss = 42.1392\n",
      "\n",
      "(w=3.4, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 0.96\n",
      "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 12.3, loss: 19.36\n",
      "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
      "    x: 5.0, y: 10.9, y_hat: 17.4, loss: 42.25\n",
      "    Loss = 45.9552\n",
      "\n",
      "(w=3.4, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.39\n",
      "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 12.5, loss: 21.16\n",
      "    x: 4.0, y: 9.1, y_hat: 14.2, loss: 26.01\n",
      "    x: 5.0, y: 10.9, y_hat: 17.6, loss: 44.89\n",
      "    Loss = 50.0112\n",
      "\n",
      "(w=3.4, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 1.90\n",
      "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 12.7, loss: 23.04\n",
      "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
      "    x: 5.0, y: 10.9, y_hat: 17.8, loss: 47.61\n",
      "    Loss = 54.3072\n",
      "\n",
      "(w=3.4, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.50\n",
      "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 12.9, loss: 25.00\n",
      "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
      "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
      "    Loss = 58.8432\n",
      "\n",
      "(w=3.4, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.3, loss: 3.17\n",
      "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 13.1, loss: 27.04\n",
      "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
      "    x: 5.0, y: 10.9, y_hat: 18.2, loss: 53.29\n",
      "    Loss = 63.6192\n",
      "\n",
      "(w=3.4, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.5, loss: 3.92\n",
      "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 13.3, loss: 29.16\n",
      "    x: 4.0, y: 9.1, y_hat: 15.0, loss: 34.81\n",
      "    x: 5.0, y: 10.9, y_hat: 18.4, loss: 56.25\n",
      "    Loss = 68.6352\n",
      "\n",
      "(w=3.4, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 5.7, loss: 4.75\n",
      "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 13.5, loss: 31.36\n",
      "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
      "    x: 5.0, y: 10.9, y_hat: 18.6, loss: 59.29\n",
      "    Loss = 73.8912\n",
      "\n",
      "(w=3.4, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 5.9, loss: 5.66\n",
      "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 13.7, loss: 33.64\n",
      "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
      "    x: 5.0, y: 10.9, y_hat: 18.8, loss: 62.41\n",
      "    Loss = 79.3872\n",
      "\n",
      "(w=3.4, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 6.1, loss: 6.66\n",
      "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 13.9, loss: 36.00\n",
      "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
      "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
      "    Loss = 85.1232\n",
      "\n",
      "(w=3.6, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
      "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.67\n",
      "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
      "    x: 3.5, y: 7.9, y_hat: 12.6, loss: 22.09\n",
      "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
      "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
      "    Loss = 53.1762\n",
      "\n",
      "(w=3.6, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 1.04\n",
      "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
      "    x: 3.5, y: 7.9, y_hat: 12.8, loss: 24.01\n",
      "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
      "    x: 5.0, y: 10.9, y_hat: 18.2, loss: 53.29\n",
      "    Loss = 57.4202\n",
      "\n",
      "(w=3.6, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.49\n",
      "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 13.0, loss: 26.01\n",
      "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
      "    x: 5.0, y: 10.9, y_hat: 18.4, loss: 56.25\n",
      "    Loss = 61.9042\n",
      "\n",
      "(w=3.6, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 2.02\n",
      "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 13.2, loss: 28.09\n",
      "    x: 4.0, y: 9.1, y_hat: 15.0, loss: 34.81\n",
      "    x: 5.0, y: 10.9, y_hat: 18.6, loss: 59.29\n",
      "    Loss = 66.6282\n",
      "\n",
      "(w=3.6, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.62\n",
      "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 13.4, loss: 30.25\n",
      "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
      "    x: 5.0, y: 10.9, y_hat: 18.8, loss: 62.41\n",
      "    Loss = 71.5922\n",
      "\n",
      "(w=3.6, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.3, loss: 3.31\n",
      "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 13.6, loss: 32.49\n",
      "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
      "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
      "    Loss = 76.7962\n",
      "\n",
      "(w=3.6, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.5, loss: 4.08\n",
      "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 13.8, loss: 34.81\n",
      "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
      "    x: 5.0, y: 10.9, y_hat: 19.2, loss: 68.89\n",
      "    Loss = 82.2402\n",
      "\n",
      "(w=3.6, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 5.7, loss: 4.93\n",
      "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 14.0, loss: 37.21\n",
      "    x: 4.0, y: 9.1, y_hat: 15.8, loss: 44.89\n",
      "    x: 5.0, y: 10.9, y_hat: 19.4, loss: 72.25\n",
      "    Loss = 87.9242\n",
      "\n",
      "(w=3.6, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 5.9, loss: 5.86\n",
      "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 14.2, loss: 39.69\n",
      "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
      "    x: 5.0, y: 10.9, y_hat: 19.6, loss: 75.69\n",
      "    Loss = 93.8482\n",
      "\n",
      "(w=3.6, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 6.1, loss: 6.86\n",
      "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
      "    x: 3.5, y: 7.9, y_hat: 14.4, loss: 42.25\n",
      "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
      "    x: 5.0, y: 10.9, y_hat: 19.8, loss: 79.21\n",
      "    Loss = 100.0122\n",
      "\n",
      "(w=3.6, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 6.3, loss: 7.95\n",
      "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
      "    x: 3.5, y: 7.9, y_hat: 14.6, loss: 44.89\n",
      "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
      "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
      "    Loss = 106.4162\n",
      "\n",
      "(w=3.8, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
      "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.12\n",
      "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
      "    x: 3.5, y: 7.9, y_hat: 13.3, loss: 29.16\n",
      "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
      "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
      "    Loss = 70.1768\n",
      "\n",
      "(w=3.8, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.59\n",
      "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
      "    x: 3.5, y: 7.9, y_hat: 13.5, loss: 31.36\n",
      "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
      "    x: 5.0, y: 10.9, y_hat: 19.2, loss: 68.89\n",
      "    Loss = 75.0888\n",
      "\n",
      "(w=3.8, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.13\n",
      "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 13.7, loss: 33.64\n",
      "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
      "    x: 5.0, y: 10.9, y_hat: 19.4, loss: 72.25\n",
      "    Loss = 80.2408\n",
      "\n",
      "(w=3.8, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.76\n",
      "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 13.9, loss: 36.00\n",
      "    x: 4.0, y: 9.1, y_hat: 15.8, loss: 44.89\n",
      "    x: 5.0, y: 10.9, y_hat: 19.6, loss: 75.69\n",
      "    Loss = 85.6328\n",
      "\n",
      "(w=3.8, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.46\n",
      "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 14.1, loss: 38.44\n",
      "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
      "    x: 5.0, y: 10.9, y_hat: 19.8, loss: 79.21\n",
      "    Loss = 91.2648\n",
      "\n",
      "(w=3.8, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.24\n",
      "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 14.3, loss: 40.96\n",
      "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
      "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
      "    Loss = 97.1368\n",
      "\n",
      "(w=3.8, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.11\n",
      "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 14.5, loss: 43.56\n",
      "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
      "    x: 5.0, y: 10.9, y_hat: 20.2, loss: 86.49\n",
      "    Loss = 103.2488\n",
      "\n",
      "(w=3.8, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 6.0, loss: 6.05\n",
      "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
      "    x: 3.5, y: 7.9, y_hat: 14.7, loss: 46.24\n",
      "    x: 4.0, y: 9.1, y_hat: 16.6, loss: 56.25\n",
      "    x: 5.0, y: 10.9, y_hat: 20.4, loss: 90.25\n",
      "    Loss = 109.6008\n",
      "\n",
      "(w=3.8, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 6.2, loss: 7.08\n",
      "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
      "    x: 3.5, y: 7.9, y_hat: 14.9, loss: 49.00\n",
      "    x: 4.0, y: 9.1, y_hat: 16.8, loss: 59.29\n",
      "    x: 5.0, y: 10.9, y_hat: 20.6, loss: 94.09\n",
      "    Loss = 116.1928\n",
      "\n",
      "(w=3.8, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 6.4, loss: 8.18\n",
      "    x: 2.0, y: 5.0, y_hat: 9.4, loss: 19.36\n",
      "    x: 3.5, y: 7.9, y_hat: 15.1, loss: 51.84\n",
      "    x: 4.0, y: 9.1, y_hat: 17.0, loss: 62.41\n",
      "    x: 5.0, y: 10.9, y_hat: 20.8, loss: 98.01\n",
      "    Loss = 123.0248\n",
      "\n",
      "(w=3.8, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.8, loss: 7.29\n",
      "    x: 1.2, y: 3.5, y_hat: 6.6, loss: 9.36\n",
      "    x: 2.0, y: 5.0, y_hat: 9.6, loss: 21.16\n",
      "    x: 3.5, y: 7.9, y_hat: 15.3, loss: 54.76\n",
      "    x: 4.0, y: 9.1, y_hat: 17.2, loss: 65.61\n",
      "    x: 5.0, y: 10.9, y_hat: 21.0, loss: 102.01\n",
      "    Loss = 130.0968\n",
      "\n",
      "(w=4.0, b=0.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
      "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.69\n",
      "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
      "    x: 3.5, y: 7.9, y_hat: 14.0, loss: 37.21\n",
      "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
      "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
      "    Loss = 89.5650\n",
      "\n",
      "(w=4.0, b=0.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
      "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.25\n",
      "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
      "    x: 3.5, y: 7.9, y_hat: 14.2, loss: 39.69\n",
      "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
      "    x: 5.0, y: 10.9, y_hat: 20.2, loss: 86.49\n",
      "    Loss = 95.1450\n",
      "\n",
      "(w=4.0, b=0.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
      "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.89\n",
      "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
      "    x: 3.5, y: 7.9, y_hat: 14.4, loss: 42.25\n",
      "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
      "    x: 5.0, y: 10.9, y_hat: 20.4, loss: 90.25\n",
      "    Loss = 100.9650\n",
      "\n",
      "(w=4.0, b=0.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
      "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.61\n",
      "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
      "    x: 3.5, y: 7.9, y_hat: 14.6, loss: 44.89\n",
      "    x: 4.0, y: 9.1, y_hat: 16.6, loss: 56.25\n",
      "    x: 5.0, y: 10.9, y_hat: 20.6, loss: 94.09\n",
      "    Loss = 107.0250\n",
      "\n",
      "(w=4.0, b=0.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
      "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.41\n",
      "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
      "    x: 3.5, y: 7.9, y_hat: 14.8, loss: 47.61\n",
      "    x: 4.0, y: 9.1, y_hat: 16.8, loss: 59.29\n",
      "    x: 5.0, y: 10.9, y_hat: 20.8, loss: 98.01\n",
      "    Loss = 113.3250\n",
      "\n",
      "(w=4.0, b=1.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
      "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.29\n",
      "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
      "    x: 3.5, y: 7.9, y_hat: 15.0, loss: 50.41\n",
      "    x: 4.0, y: 9.1, y_hat: 17.0, loss: 62.41\n",
      "    x: 5.0, y: 10.9, y_hat: 21.0, loss: 102.01\n",
      "    Loss = 119.8650\n",
      "\n",
      "(w=4.0, b=1.2)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
      "    x: 1.2, y: 3.5, y_hat: 6.0, loss: 6.25\n",
      "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
      "    x: 3.5, y: 7.9, y_hat: 15.2, loss: 53.29\n",
      "    x: 4.0, y: 9.1, y_hat: 17.2, loss: 65.61\n",
      "    x: 5.0, y: 10.9, y_hat: 21.2, loss: 106.09\n",
      "    Loss = 126.6450\n",
      "\n",
      "(w=4.0, b=1.4)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
      "    x: 1.2, y: 3.5, y_hat: 6.2, loss: 7.29\n",
      "    x: 2.0, y: 5.0, y_hat: 9.4, loss: 19.36\n",
      "    x: 3.5, y: 7.9, y_hat: 15.4, loss: 56.25\n",
      "    x: 4.0, y: 9.1, y_hat: 17.4, loss: 68.89\n",
      "    x: 5.0, y: 10.9, y_hat: 21.4, loss: 110.25\n",
      "    Loss = 133.6650\n",
      "\n",
      "(w=4.0, b=1.6)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
      "    x: 1.2, y: 3.5, y_hat: 6.4, loss: 8.41\n",
      "    x: 2.0, y: 5.0, y_hat: 9.6, loss: 21.16\n",
      "    x: 3.5, y: 7.9, y_hat: 15.6, loss: 59.29\n",
      "    x: 4.0, y: 9.1, y_hat: 17.6, loss: 72.25\n",
      "    x: 5.0, y: 10.9, y_hat: 21.6, loss: 114.49\n",
      "    Loss = 140.9250\n",
      "\n",
      "(w=4.0, b=1.8)\n",
      "    x: 1.0, y: 3.1, y_hat: 5.8, loss: 7.29\n",
      "    x: 1.2, y: 3.5, y_hat: 6.6, loss: 9.61\n",
      "    x: 2.0, y: 5.0, y_hat: 9.8, loss: 23.04\n",
      "    x: 3.5, y: 7.9, y_hat: 15.8, loss: 62.41\n",
      "    x: 4.0, y: 9.1, y_hat: 17.8, loss: 75.69\n",
      "    x: 5.0, y: 10.9, y_hat: 21.8, loss: 118.81\n",
      "    Loss = 148.4250\n",
      "\n",
      "(w=4.0, b=2.0)\n",
      "    x: 1.0, y: 3.1, y_hat: 6.0, loss: 8.41\n",
      "    x: 1.2, y: 3.5, y_hat: 6.8, loss: 10.89\n",
      "    x: 2.0, y: 5.0, y_hat: 10.0, loss: 25.00\n",
      "    x: 3.5, y: 7.9, y_hat: 16.0, loss: 65.61\n",
      "    x: 4.0, y: 9.1, y_hat: 18.0, loss: 79.21\n",
      "    x: 5.0, y: 10.9, y_hat: 22.0, loss: 123.21\n",
      "    Loss = 156.1650\n",
      "\n",
      "BEST:\n",
      "w=2.0, b=1.0, loss=0.0250\n"
     ]
    }
   ],
   "source": [
    "model = SimpleLinearRegression()\n",
    "\n",
    "# to store all losses for later use\n",
    "losses = []\n",
    "\n",
    "# the parameters to search\n",
    "weights = np.arange(0, 4.1, 0.2) \n",
    "biases = np.arange(0, 2.1, 0.2)\n",
    "\n",
    "# for storing the loss in a matrix for visualisation later\n",
    "loss_matrix = np.zeros((len(weights), len(biases)))\n",
    "\n",
    "# compute loss for each (w,b) combination\n",
    "for i, w in enumerate(weights):\n",
    "    for j, b in enumerate(biases):\n",
    "        print(f\"(w={w:.1f}, b={b:.1f})\")\n",
    "        \n",
    "        # setup weights of model\n",
    "        model.w = w\n",
    "        model.b = b\n",
    "\n",
    "        sum_loss = 0\n",
    "        # for each example\n",
    "        for (x, y) in zip(x_train, y_train):\n",
    "            # compute the loss for this example\n",
    "            single_loss = model.loss(x, y)\n",
    "\n",
    "            # and add it to the sum\n",
    "            sum_loss += single_loss\n",
    "\n",
    "            # print out the values just to make sure everything is working correctly\n",
    "            y_hat = model.forward(x)\n",
    "            print(f\"    x: {x}, y: {y}, y_hat: {y_hat:.1f}, loss: {single_loss:.2f}\")\n",
    "\n",
    "        # print out the sum of individual losses\n",
    "        # I multiplied by 0.5 to be consistent with the equation earlier, \n",
    "        # but this is not necessary in practice as this is a constant\n",
    "        print(f\"    Loss = {(0.5 * sum_loss):.4f}\\n\")\n",
    "\n",
    "        # store the losses and the corresponding (w,b) for later use\n",
    "        losses.append((0.5*sum_loss, w, b))\n",
    "\n",
    "        # store the losses in a matrix form for visualisation later\n",
    "        loss_matrix[i,j] = 0.5 * sum_loss\n",
    "\n",
    "# find combination with minimum loss\n",
    "(min_loss, best_w, best_b) = min(losses, key=lambda x:x[0])\n",
    "print(\"BEST:\")\n",
    "print(f\"w={best_w}, b={best_b}, loss={min_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdvNc6WQOwhv"
   },
   "source": [
    "Plotting the loss values as a surface graph gives you a picture of the \"optimisation landscape\" for the parameter values. The loss is minimum at $w=2$, $b=1$ (it might be hard to see this clearly from the 3D diagram, but you can trust the numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LVruAU86OwIA"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "# enable 3D\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    " # generate combinations of weights and biases\n",
    "(w_list, b_list) = np.meshgrid(weights, biases)\n",
    "\n",
    "# plot loss across weights and bias values\n",
    "surf = ax.plot_surface(w_list.T, b_list.T, loss_matrix,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('b')\n",
    "ax.set_zlabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nWYSrkBcWzs"
   },
   "source": [
    "### Optimisation with derivatives\n",
    "\n",
    "Obviously, the brute force search method above is really computationally expensive (very slow!), especially when there are more parameters and more values to search. A more efficient approach is to start from a random set of parameter values, and then try to \"move down\" the loss graph to the point with a minimum loss. So, in the plot above, you can start at some random $w$ and $b$, and try to move towards the minimum point.\n",
    "\n",
    "The problem is: from where you are, you actually do not know where the minimum point is. You only know that there *is* a minimum point. The question is: how do you know towards which direction you should move (and how far) so that you can get to the minimum point as fast as possible? The answer is downwards (obviously!) and where the slope is steepest. \n",
    "\n",
    "Fortunately, calculus saves us from guessing, and provides us a way to compute the direction of the slope at any point. This is called the *derivative* (or gradient). Intuitively, the derivative $\\frac{\\partial L}{\\partial w}$ tells us by how much $L$ changes when $w$ changes. Similarly, $\\frac{\\partial L}{\\partial b}$ tells us by how much $L$ changes when $b$ changes. So if we move in these directions, we hope that we can ultimately reach a minimum point.\n",
    "\n",
    "If you are well-versed with calculus, you can compute $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ by hand. Otherwise, you can get help from a derivative calculator (e.g. https://www.derivative-calculator.net)\n",
    "\n",
    "As presented in the lectures, the partial derivatives of the loss function with respect to $w$ and to $b$ are:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w} = \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x$ \n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$ \n",
    "\n",
    "Thus, for a *single* point, the partial derivatives are:\n",
    "\n",
    "$\\frac{\\partial L^{(i)}}{\\partial w} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x$ \n",
    "\n",
    "$\\frac{\\partial L^{(i)}}{\\partial b} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$ \n",
    "\n",
    "Now, complete the `gradient()` method for `SimpleLinearRegression` below to compute the partial derivatives wrt $w$ and $b$ at a given point. To make life easier, just compute and return both $\\frac{\\partial L^{(i)}}{\\partial w}$ and $\\frac{\\partial L^{(i)}}{\\partial b}$ at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sBXmI71Ewc9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "def gradient(self, x, y):\n",
    "    \"\"\" Compute partial derivatives wrt w and b\n",
    "\n",
    "    Args:\n",
    "        x (float): input instance\n",
    "        y (float): ground truth output\n",
    "\n",
    "    Returns:\n",
    "        tuple: (float, float)\n",
    "            - the first element will be dL/dw\n",
    "            - the second element will be dL/db\n",
    "    \"\"\"\n",
    "    # TODO: Complete this\n",
    "    dLdw = x * (self.forward(x) - y)\n",
    "    dLdb = (self.forward(x) - y)\n",
    "\n",
    "    return (dLdw, dLdb)\n",
    "    \n",
    "\n",
    "# A quick hack to bind this function as the SimpleLinearRegression.gradient() method\n",
    "SimpleLinearRegression.gradient = gradient\n",
    "\n",
    "\n",
    "## Quick test: This should return (6.0, 3.0)\n",
    "model = SimpleLinearRegression()\n",
    "model.w = 3\n",
    "model.b = 2\n",
    "x = 2.0\n",
    "y = 5.0\n",
    "(dLdw, dLdb) = model.gradient(x, y)\n",
    "print(dLdw) # should print 6.0\n",
    "print(dLdb) # should print 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBw1pRVN-heL"
   },
   "source": [
    "So, in this example, $\\frac{\\partial L}{\\partial w}=6.0$ suggests that when $w$ increases by a very tiny amount, $L$ will increase by 6.0 times that amount.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCNctFEY0yiT"
   },
   "source": [
    "#### Gradient descent\n",
    "\n",
    "Now that we have our gradients, let us try to optimise the parameters $w$ and $b$ of our model to minimise the loss. We will use the gradient descent algorithm for this, as discussed in the lectures. \n",
    "\n",
    "You may reimplement the code provided in the lectures, replacing some of the code by reusing the `forward()`, `loss()` and `gradient()` methods of `SimpleLinearRegression` that you implemented earlier. This will help make your code more modular and readable, and help to improve your understanding of gradient descent at a more abstract level.\n",
    "\n",
    "You should be able to obtain $w \\approx 2$ and $b \\approx 1$ by the end of training if you have implemented everything correctly. Also experiment with the learning rate and the number of epochs and observe the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLdqp4ESCpsR"
   },
   "outputs": [],
   "source": [
    "model = SimpleLinearRegression()\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    error = 0.0\n",
    "    grad_w = 0.0\n",
    "    grad_b = 0.0\n",
    "    for (x, y) in zip(x_train, y_train):\n",
    "        ### TODO: Complete this\n",
    "        ### 1. Compute the gradients for w and b for this example\n",
    "        dLdw, dLdb = model.gradient(x, y)\n",
    "\n",
    "        ### 2. Add the gradients to grad_w and grad_b\n",
    "        grad_w = dLdw\n",
    "        grad_b = dLdb\n",
    "\n",
    "        ### 3. Add the \"local\" loss to the global error (Loss) for analysis\n",
    "        error = model.loss(x, y)\n",
    "\n",
    "    # TODO: Update the weights using the (summed) gradients\n",
    "    model.w = ???\n",
    "    model.b = ???\n",
    "    \n",
    "    print(f\"Epoch: {epoch}\\t w: {model.w:.2f}\\t b: {model.b:.2f}\\t L: {error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad3Q827dMohe"
   },
   "source": [
    "### Predictions\n",
    "\n",
    "Now that your model is trained, you can use it to predict some unknown test instances. Complete the code below to predict the output of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTqEAz6GMoRH"
   },
   "outputs": [],
   "source": [
    "y_predictions = np.zeros((len(y_test),))\n",
    "for (i, x) in enumerate(x_test):\n",
    "    # TODO: Complete this\n",
    "    y_predictions[i] = ????\n",
    "\n",
    "print(y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2ANPLrkLVaY"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Finally, let us evaluate the linear regression model you developed. Unlike classification, we will need a different metric for regression. Recall from Lecture 3, a common evaluation metric for regression is the Mean Squared Error (MSE). We will use that for this tutorial.\n",
    "\n",
    "Complete the `mse()` function below to compute the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fvD2vNXLx6f"
   },
   "outputs": [],
   "source": [
    "def mse(y_gold, y_prediction):\n",
    "    \"\"\" Compute the MSE given the ground truth and predictions\n",
    "\n",
    "    Args:\n",
    "        y_gold (np.ndarray): the correct ground truth values of y\n",
    "        y_prediction (np.ndarray): the predicted values of y\n",
    "\n",
    "    Returns:\n",
    "        float : MSE\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(y_gold) == len(y_prediction)  \n",
    "    \n",
    "    # TODO: Complete this\n",
    "    return ????\n",
    "\n",
    "\n",
    "# Compute the MSE on model predictions on our toy test data\n",
    "# You should be able to obtain a very small MSE rate\n",
    "print(mse(y_test, y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6emtY9qqwGSe"
   },
   "source": [
    "## Iris Dataset (Extra exercise)\n",
    "\n",
    "Here is an extra optional exercise for you: try to get your simple linear regression model working on a (slightly) larger and noisier dataset. \n",
    "\n",
    "For this, we will convert the Iris dataset to use as a regression task. More specifically, our task is to predict the *petal width* of a flower (`y`) given the *sepal length* as input (`x`). The code below will give you the dataset in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1yTkyHzwnUp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Download iris data if it does not exist\n",
    "if not os.path.exists(\"iris.data\"):\n",
    "    !wget -O iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
    "\n",
    "def read_dataset_as_regression(filepath):\n",
    "    \"\"\" Read in the dataset from the specified filepath\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The filepath to the dataset file\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple of (x, y), each being a numpy array. \n",
    "               - x is a numpy array with shape (N, ), \n",
    "                   where N is the number of instances\n",
    "               - y is a numpy array with shape (N, ), where each element is a \n",
    "                real-valued float, and N is the number of instances\n",
    "    \"\"\"\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for line in open(filepath):\n",
    "        if line.strip() != \"\": # handle empty rows in file\n",
    "            row = line.strip().split(\",\")\n",
    "            # extract columns 0 as x.\n",
    "            x.append(float(row[0])) \n",
    "\n",
    "            # extract column 3 as y\n",
    "            y.append(float(row[3]))\n",
    "\n",
    "    return (np.array(x), np.array(y))\n",
    "\n",
    "\n",
    "def split_dataset(x, y, test_proportion, random_generator=default_rng()):\n",
    "    \"\"\" Split dataset into training and test sets, according to the given \n",
    "        test set proportion.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
    "        y (np.ndarray): Output label, numpy array with shape (N,)\n",
    "        test_proprotion (float): the desired proportion of test examples \n",
    "                                 (0.0-1.0)\n",
    "        random_generator (np.random.Generator): A random generator\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple of (x_train, x_test, y_train, y_test) \n",
    "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
    "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
    "               - y_train (np.ndarray): Training labels, shape (N_train, )\n",
    "               - y_test (np.ndarray): Test labels, shape (N_test, )\n",
    "    \"\"\"\n",
    "\n",
    "    shuffled_indices = random_generator.permutation(len(x))\n",
    "    n_test = round(len(x) * test_proportion)\n",
    "    n_train = len(x) - n_test\n",
    "    x_train = x[shuffled_indices[:n_train]]\n",
    "    y_train = y[shuffled_indices[:n_train]]\n",
    "    x_test = x[shuffled_indices[n_train:]]\n",
    "    y_test = y[shuffled_indices[n_train:]]\n",
    "    return (x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "(x, y) = read_dataset_as_regression(\"iris.data\")\n",
    "print(x.shape)  # (150,) \n",
    "print(y.shape)  # (150,)\n",
    "\n",
    "seed = 60012\n",
    "rg = default_rng(seed)\n",
    "x_train, x_test, y_train, y_test = split_dataset(x, y, \n",
    "                                                 test_proportion=0.2, \n",
    "                                                 random_generator=rg)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XM2yXt_0RMO"
   },
   "source": [
    "As usual, it's always a good idea to examine your data before you start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTF3kSAV0ddn"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, c=\"blue\", edgecolor='k')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYm-2dB41JYI"
   },
   "source": [
    "### Model\n",
    "\n",
    "You can copy your `SimpleLinearRegression` class from earlier, with the`forward()`, `loss()` and `gradient()` methods that you implemented earlier. This time you can put them all together in the same place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfdsAHaV1IuR"
   },
   "outputs": [],
   "source": [
    "class SimpleLinearRegression:\n",
    "    def __init__(self, random_generator=default_rng()):\n",
    "        self.w = random_generator.standard_normal()\n",
    "        self.b = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return ????\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        return ????\n",
    "\n",
    "    def gradient(self, x, y):\n",
    "        return ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6zXgIed3gTR"
   },
   "source": [
    "### Optimisation\n",
    "\n",
    "Again, you should not need to change much of your gradient descent implementation from earlier, so just copy it over. You may, however, need to tweak the learning rate and the number of epochs to obtain a reasonable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Txe_vse3iIH"
   },
   "outputs": [],
   "source": [
    "model = SimpleLinearRegression()\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    error = 0.0\n",
    "    grad_w = 0.0\n",
    "    grad_b = 0.0\n",
    "    for (x, y) in zip(x_train, y_train):\n",
    "        ### TODO: Complete this\n",
    "        ### 1. Compute the gradients for w and b for this example\n",
    "        ???\n",
    "\n",
    "        ### 2. Add the gradients to grad_w and grad_b\n",
    "        grad_w = ???\n",
    "        grad_b = ???\n",
    "\n",
    "        ### 3. Add the \"local\" loss to the global error (Loss) for analysis\n",
    "        error = ???\n",
    "\n",
    "    # TODO: Update the weights using the (summed) gradients\n",
    "    model.w = ???\n",
    "    model.b = ???\n",
    "    \n",
    "    print(f\"Epoch: {epoch}\\t w: {model.w:.2f}\\t b: {model.b:.2f}\\t L: {error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P7tQX-L31LB"
   },
   "source": [
    "### Visualising your trained model\n",
    "\n",
    "You can visualise your model by plotting the line on the graph.\n",
    "\n",
    "We will also plot the test instances to get a rough idea of how well we expect it to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUmFEg7s33Ve"
   },
   "outputs": [],
   "source": [
    "# Plot training instances\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, c=\"blue\", edgecolor='k')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "# Draw the line representing the model\n",
    "xmin = x_train.min()\n",
    "ymin = model.forward(xmin)\n",
    "xmax = x_train.max()\n",
    "ymax = model.forward(xmax)\n",
    "plt.plot([xmin, xmax], [ymin, ymax], 'r-')\n",
    "\n",
    "# Plot test instances\n",
    "plt.scatter(x_test, y_test, c=\"red\", edgecolor='k')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XY37EHKe5JZT"
   },
   "source": [
    "### Predictions and evaluation\n",
    "\n",
    "Finally, predict the test instances given the model.\n",
    "\n",
    "Then evaluate the model with MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHx0gaAr5OfN"
   },
   "outputs": [],
   "source": [
    "y_predictions = np.zeros((len(y_test),))\n",
    "for (i, x) in enumerate(x_test):\n",
    "    y_predictions[i] = ????\n",
    "\n",
    "print(y_predictions)\n",
    "print(y_test)\n",
    "\n",
    "print(mse(y_test, y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNMajcyH7Vv5"
   },
   "source": [
    "## Summary\n",
    "\n",
    "Hopefully you have managed to deepen your understanding about linear regression by implementing the model, loss function, and the gradient descent algorithm, and putting everything together for training and testing.\n",
    "\n",
    "In the next lab exercise, we will delve a bit deeper at implementation level, and try to extend your model to handle more than one input variable. We will also start making your code a bit more efficient with vectorised implementations so that you can perform computations on multiple training instances simultaneously. This will hopefully help you get started on implementing Neural Networks (which we will unfortunately not cover in these lab exercises as it is part of your second coursework).  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "lab4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
