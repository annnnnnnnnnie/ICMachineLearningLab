{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/annnnnnnnnnie/ICMachineLearningLab/blob/master/lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MBmOdXoenFw"
   },
   "source": [
    "# Lab 1: Building a Machine Learning Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3iS23ADfo4w"
   },
   "source": [
    "## Version history\n",
    "\n",
    "| Date | Author | Description |\n",
    "|:----:|:------:|:------------|\n",
    "2021-01-12 | Josiah Wang | First version | \n",
    "2021-01-18 | Josiah Wang | Updated the description of the nearest neighbour classifier to make it clearer   |\n",
    "2021-01-19 | Josiah Wang | Clarified the terms categories and classes, both referring to classification labels |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EbbkgqOgZK_"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this lab exercise is to give you some practical experience in building a full machine learning pipeline in Python and NumPy.\n",
    "\n",
    "By the end of this lab exercise, you will have constructed a full pipeline for a classification task: \n",
    "- reading and processing the dataset\n",
    "- examining your data/features\n",
    "- building a baseline classifier\n",
    "- building a simple nearest neighbour classifier\n",
    "- evaluating the performance of the classifier with the accuracy metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1PH_TJZhf-c"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We will work with the Iris dataset in this lab exercise. This is a classic dataset from 1936 often used for teaching machine learning techniques.\n",
    "\n",
    "Let's first download this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NdL1OHXvifcA"
   },
   "outputs": [],
   "source": [
    "# Download iris data if it does not exist\n",
    "# ! runs a bash command, so !ls will run the ls command on the virtual machine\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"iris.data\"):\n",
    "    !wget -O iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fywysYw13usm"
   },
   "source": [
    "### Examining the dataset\n",
    "\n",
    "Now try to take a quick look at `iris.data`. The simple code snippet below will allow you to read from a file line by line. \n",
    "\n",
    "Note that we will use the terms *categories* and *classes* interchangably to mean classification labels. *Class* may also occasionally refer to the OOP sense of classes, which are different from classification labels.\n",
    "\n",
    "Try to answer these questions while you examine the data:\n",
    "- How many attributes/features are there?\n",
    "- What kind of features are they? Categorical? Integers? Continuous real numbers?\n",
    "- How many categories/classes are used in this dataset?\n",
    "- How many instances are there in total?\n",
    "- How many instances are there *per category*? (Is this a balanced dataset?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lgPVrvw_jngI"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'iris.data'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_20592/1043192304.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mcollections\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mCounter\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"iris.data\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m     \u001B[0mfirst\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[0msecond\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'iris.data'"
     ]
    }
   ],
   "source": [
    "# Examine the file to understand its structure\n",
    "#for line in open(\"iris.data\"):\n",
    "#    print(line.strip())\n",
    "\n",
    "# How many attributes.features are there? \n",
    "## 4 features\n",
    "# What kind of features are they?\n",
    "## Continuous real numbers\n",
    "# How many classes?\n",
    "## 3 classes\n",
    "# How many instances?\n",
    "## 150 instances\n",
    "# Balanced?\n",
    "## Yes\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "with open(\"iris.data\") as dataset:\n",
    "    first = []\n",
    "    second = []\n",
    "    third = []\n",
    "    fourth = []\n",
    "    label = []\n",
    "    for line in dataset:\n",
    "        line = line.strip().split(',')\n",
    "        if line[0]:\n",
    "            first.append(line[0])\n",
    "            second.append(line[1])\n",
    "            third.append(line[2])\n",
    "            fourth.append(line[3])\n",
    "            label.append(line[4])\n",
    "    first.sort()\n",
    "    second.sort()\n",
    "    third.sort()\n",
    "    fourth.sort()\n",
    "    print(first)\n",
    "    print(second)\n",
    "    print(third)\n",
    "    print(fourth)\n",
    "    print(Counter(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtLcMgSN3XrI"
   },
   "source": [
    "### Read and store the instances and labels\n",
    "\n",
    "Now, we will try to properly read in the data and store them into the variables `x` and `y`.\n",
    "\n",
    "Depending on your algorithm, you can sometimes just use the string labels directly as `y` (i.e. `Iris-setosa`, `Iris-versicolor`, `Iris-virginica`). It is, however, often converted to integers (`0`, `1`, `2`).\n",
    "\n",
    "Complete the function `read_dataset()` below that takes in the filepath (a string), and returns a tuple containing the numpy arrays `x` and `y` and also the list of class labels. \n",
    "\n",
    "- `x` is a numpy array with shape `(N, K)`, where `N` is the number of examples and `K` the number of attributes/features.\n",
    "\n",
    "- `y` is a numpy array with shape `(N, )`, where each element in the array is an integer from 0 to `C`, and `C` is the number of categories/classes.\n",
    "\n",
    "- `classes` is a numpy array with shape `(C, )`, containing the label for each of the class (category).\n",
    "\n",
    "Hint: For computing `y` from the string labels, use `np.unique()` with the keyword argument `return_inverse=True` to easily get the unique class labels and the mapping to the labels at the same time. Or you can also just write this from scratch in Python - I don't mind!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oC9S6oGZkWH7"
   },
   "outputs": [],
   "source": [
    "# Let's warm up with a little Python file reading exercise!\n",
    "# Complete the function below to read the file and store the instances as \n",
    "# feature vectors x and labels y\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def read_dataset(filepath):\n",
    "    \"\"\" Read in the dataset from the specified filepath\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The filepath to the dataset file\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple of (x, y, classes), each being a numpy array. \n",
    "               - x is a numpy array with shape (N, K), \n",
    "                   where N is the number of instances\n",
    "                   K is the number of features/attributes\n",
    "               - y is a numpy array with shape (N, ), and each element should be \n",
    "                   an integer from 0 to C-1 where C is the number of classes \n",
    "               - classes : a numpy array with shape (C, ), which contains the \n",
    "                   unique class labels corresponding to the integers in y\n",
    "    \"\"\"\n",
    "    with open(filepath) as dataset:\n",
    "        data = filter(lambda l: l[0], map(lambda l: l.strip().split(','), dataset))\n",
    "        x_raw = []\n",
    "        y_raw = []\n",
    "        \n",
    "        for line in data:\n",
    "            x_raw.append(list(map(float, line[:-1])))\n",
    "            y_raw.append(line[-1])\n",
    "        \n",
    "        classes, big_y = np.unique(np.array(y_raw), return_inverse=True)\n",
    "        big_x = np.array(x_raw)\n",
    "        \n",
    "        return big_x, big_y, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGa0842IOAki"
   },
   "source": [
    "We are expecting `N` to be 150. So if you happen to end up with 151, you will need to check why and update your code accordingly! (It likely has something to do with a blank line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "filfeUOAb8eT"
   },
   "outputs": [],
   "source": [
    "(x, y, classes) = read_dataset(\"iris.data\")\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_jsScXRsYxt"
   },
   "source": [
    "## Understanding the features\n",
    "\n",
    "Usually, I would also encourage you to examine the raw data itself (e.g. images of the flowers). This is so that you can gain insights into what might be useful to distinguish between the classes (colour? size?) Unfortunately, we are not provided with these, but only pre-processed features. So we can skip the explicit feature encoding step and just use the pre-processed features directly.\n",
    "\n",
    "The dataset itself does not give you more information about the four features, but they actually represent the: \n",
    "1. sepal length (cm)\n",
    "2. sepal width (cm)\n",
    "3. petal length (cm)\n",
    "4. petal width (cm)\n",
    "\n",
    "Now, to gain a better understanding of the features, try to compute some statistics for each of them. For example,\n",
    "- What is the minimum and maximum value for each feature?\n",
    "- What is the mean, median and standard deviation for each?\n",
    "\n",
    "It is also a good idea to obtain the statistics above separately for each class. So you can try to find each attribute's range, mean, median, and standard deviation separately for class 0, class 1 and class 2. You may discover some patterns and get some ideas about what features will be useful for certain classes. For example, perhaps you might notice that one of the features have a completely different range for class 0 vs class 2? \n",
    "\n",
    "Tip: `x[y==2]` gives you all the feature vectors belonging to class 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0wKamlxdNyb"
   },
   "outputs": [],
   "source": [
    "# Free-style coding! Practice your NumPy skills here by computing some \n",
    "# statistics for the features\n",
    "\n",
    "# Example: compute the min for each attribute\n",
    "print(x.min(axis=0))\n",
    "print(x.max(axis=0))\n",
    "print()\n",
    "for c in range(len(classes)):\n",
    "    print(x[y==c].min(axis=0))\n",
    "    print(x[y==c].max(axis=0))\n",
    "    print(x[y==c].std(axis=0))\n",
    "    print(x[y==c].mean(axis=0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNKLNI2-cARf"
   },
   "source": [
    "### Visualising the features\n",
    "\n",
    "Now let's try to *really* understand the features by examining how they correlate. A very useful thing you can do is to visualise your features. This may give you better insights that you might have missed when looking at just the numbers. \n",
    "\n",
    "We will use an external Python library called `matplotlib` for plotting. If you are working locally on your computer, you may need to install the library with `pip3 install matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g0pIs3bjw3J"
   },
   "source": [
    "### Plotting a scatter plot for two features\n",
    "\n",
    "Let us first try to examine whether the features are actually any good for classifying the flowers. Let's try to plot a scatter plot for the first two features (sepal length vs sepal width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZYyPqKCj61d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_names = [\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\"]\n",
    "                        \n",
    "plt.figure()\n",
    "plt.scatter(x[:,0], x[:,1], c=y, cmap=plt.cm.Set1, edgecolor='k')\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKfTuBm-kaiS"
   },
   "source": [
    "If you look carefully at the scatter plot above, you may find that with only these two features, you can actually already separate one of the classes (in red, this is actually \"setosa\") from the other two classes with a straight line (a linear classifier). So these kinds of observation will be useful to inform your machine learning design.\n",
    "\n",
    "Now, letâ€™s try visualising the remaining two features (petal length vs petal width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weu5VfHMmV5s"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x[:,2], x[:,3], c=y, cmap=plt.cm.Set1, edgecolor='k')\n",
    "plt.xlabel(feature_names[2])\n",
    "plt.ylabel(feature_names[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRaBst-fmb8d"
   },
   "source": [
    "Woah! You should see something even better! The first class (in red) forms its own tight cluster, while the other two are just about separable. So using just these two features might even be enough for our classifier!\n",
    "\n",
    "Such visualisation activities can actually be very useful for you to decide on what features to use!\n",
    "\n",
    "If you want, you can try further combinations/views, for example sepal width and petal width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3WYf3LhvGoX"
   },
   "source": [
    "### Plotting histograms\n",
    "\n",
    "While the statistics you computed earlier (min, max, median, etc.) might be useful, sometimes you can get more insights by **visualising** the value of the features itself.\n",
    "\n",
    "So let's say we want to check the values of petal width (since it seems like a good feature), separately for the three classes. We can plot a histogram of the petal width distribution for each of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjzPBokrvRW-"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3) # plot subfigures in a 1x3 grid\n",
    "\n",
    "ax[0].hist(x[y==0, 2], color='r')\n",
    "ax[0].set(title=classes[0])\n",
    "\n",
    "ax[1].hist(x[y==1, 2], color='b')\n",
    "ax[1].set(title=classes[1])\n",
    "\n",
    "ax[2].hist(x[y==2, 2], color='g')\n",
    "ax[2].set(title=classes[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaqBSm7ywh0w"
   },
   "source": [
    "You can see that \"setosa\" can clearly be distinguished from the other two classes by petal width. For \"versicolor\" and \"virginica\", there is a bit of an overlap when the petal width is between around 4.5-5.1. So there will be a bit of uncertainty here.\n",
    "\n",
    "You can also use a Python library called [Pandas](https://pandas.pydata.org) to help with analysing your dataset and features. We won't cover this in this lab tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ewHfcgINbLb"
   },
   "source": [
    "## Pre-process your data\n",
    "\n",
    "Now, remember that you will need a **test set** that is separate from the **training set**.\n",
    "\n",
    "While some datasets provide pre-splitted training and test datasets, others do not. \n",
    "\n",
    "The Iris dataset has not been pre-splitted. So you will have to split this yourself.\n",
    "\n",
    "So divide your dataset at random into training and testing.\n",
    "\n",
    "Make sure that you do not end up with one class all being in the test set!\n",
    "\n",
    "I used NumPy's `np.random` in the code below, but you are free to use Python's `random` if you prefer and modify accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLyRKQ4cPsx3"
   },
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "def split_dataset(x, y, test_proportion, random_generator=default_rng()):\n",
    "    \"\"\" Split dataset into training and test sets, according to the given \n",
    "        test set proportion.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
    "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
    "        test_proportion (float): the desired proportion of test examples \n",
    "                                 (0.0-1.0)\n",
    "        random_generator (np.random.Generator): A random generator\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple of (x_train, x_test, y_train, y_test) \n",
    "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
    "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
    "               - y_train (np.ndarray): Training labels, shape (N_train, )\n",
    "               - y_test (np.ndarray): Test labels, shape (N_train, )\n",
    "    \"\"\"\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if random_generator.random() < test_proportion:\n",
    "            x_test.append(x[i])\n",
    "            y_test.append(y[i])\n",
    "        else:\n",
    "            x_train.append(x[i])\n",
    "            y_train.append(y[i])\n",
    "    return np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsO28HzedhvT"
   },
   "source": [
    "And we'll now just split the dataset into 80% for training and 20% for testing.\n",
    "\n",
    "Remember to initialise the random number generator with a seed number (just choose any number).\n",
    "\n",
    "Tip: Random numbers are not really random in computers. They are **pseudo-random** because you can reproduce the same 'random' sequence with a fixed seed number. Using seed numbers is important in your scientific experiments so that you can reproduce your experimental results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVWFWhzodSmS"
   },
   "outputs": [],
   "source": [
    "seed = 600123\n",
    "rg = default_rng(seed)\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_dataset(x, y, \n",
    "                                                 test_proportion=0.2, \n",
    "                                                 random_generator=rg)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(set(y_train)) # Sanity check to ensure all labels are in the training set\n",
    "print(set(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg6lFRv3eOZr"
   },
   "source": [
    "## Random Baseline Classifier\n",
    "\n",
    "Now that the data is ready, we can finally construct your classifier.\n",
    "\n",
    "We will always need to compare the performance of our dataset against something. So let us first construct a lower-bound, baseline classifier that predicts a class label at random.\n",
    "\n",
    "We will create a class called `RandomClassifier`, which has two methods:\n",
    "- `fit()` to train the classifier given the training examples `x` and their corresponding labels `y\n",
    "- `predict()` to predict a set of labels given some examples `x`.\n",
    "\n",
    "Obviously, there is no need to 'train' a random classifier. Instead, we only need to store the unique class labels that the classifier can use to make a random prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mek8A-ldhTSY"
   },
   "outputs": [],
   "source": [
    "class RandomClassifier:\n",
    "    def __init__(self, random_generator=default_rng()):\n",
    "        self.random_generator = random_generator\n",
    "        self.unique_y = []\n",
    "        self.counts = 0\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\" Fit the training data to the classifier.\n",
    "\n",
    "        Args:\n",
    "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
    "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
    "        \"\"\"\n",
    "        self.unique_y, self.counts = np.unique(y, return_counts=True)\n",
    "        return\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\" Perform prediction given some examples.\n",
    "\n",
    "        Args:\n",
    "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
    "\n",
    "        Returns:\n",
    "        y (np.ndarray): Predicted class labels, numpy array with shape (N,)\n",
    "        \"\"\"        \n",
    "        prediction = []\n",
    "        table = np.repeat(self.unique_y, self.counts)\n",
    "        for _ in x:\n",
    "            prediction.append(table[self.random_generator.integers(len(table))])\n",
    "        return np.array(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPJI6UGxoW4l"
   },
   "outputs": [],
   "source": [
    "random_classifier = RandomClassifier(rg)\n",
    "random_classifier.fit(x_train, y_train)\n",
    "random_predictions = random_classifier.predict(x_test)\n",
    "print(len(random_predictions))\n",
    "print(len(y_test))\n",
    "print(random_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4yZIOgepd2L"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now, let's try to evaluate our random classifier. Let's try to implement the accuracy metric as discussed in our lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfjzLLwUqDC-"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(y_gold, y_prediction):\n",
    "    \"\"\" Compute the accuracy given the ground truth and predictions\n",
    "\n",
    "    Args:\n",
    "    y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
    "    y_prediction (np.ndarray): the predicted labels\n",
    "\n",
    "    Returns:\n",
    "    float : the accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(y_gold) == len(y_prediction)  \n",
    "    \n",
    "    correct_count = 0\n",
    "    total = len(y_gold)\n",
    "    for i in range(len(y_gold)):\n",
    "        if y_gold[i] == y_prediction[i]:\n",
    "            correct_count += 1\n",
    "    \n",
    "    return correct_count/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCuFiRqRuM91"
   },
   "outputs": [],
   "source": [
    "accuracy = compute_accuracy(y_test, random_predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LNLsjjiwPJi"
   },
   "source": [
    "## Nearest Neighbour Classifier\n",
    "\n",
    "Let's now try to construct a better classifier.\n",
    "\n",
    "Let's build a classifier which takes the test examples, and for each predicts the class label of the nearest training example according to the Euclidean distance metric $d(x^{(i)}, x^{(q)})=\\sqrt{\\sum_f^F (x_f^{(i)} - x_f^{(q)})^2}$.\n",
    "\n",
    "We will construct a class called `NearestNeighbourClassifier`. To make our framework reusable, this class will have the same methods as `RandomClassifier`: `fit()` and `predict()`.\n",
    "\n",
    "Again, because this is a 'lazy learner', our focus will be on the `predict()` method. The `fit()` method will only need to store the instances and the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtnx_HjTyA2N"
   },
   "outputs": [],
   "source": [
    "class NearestNeighbourClassifier:\n",
    "    def __init__(self):\n",
    "        self.x = np.array([])\n",
    "        self.y = np.array([])\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\" Fit the training data to the classifier.\n",
    "\n",
    "        Args:\n",
    "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
    "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
    "        \"\"\"\n",
    "        self.x = np.copy(x)\n",
    "        self.y = np.copy(y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\" Perform prediction given some examples.\n",
    "\n",
    "        Args:\n",
    "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
    "\n",
    "        Returns:\n",
    "        y (np.ndarray): Predicted class labels, numpy array with shape (N,)\n",
    "        \"\"\" \n",
    "        # TODO: Complete this method to predict the class of the \n",
    "        # nearest neighbour given a set of test instance\n",
    "        prediction = []\n",
    "        for one_entry in x:\n",
    "            p = self.predict_one(one_entry)\n",
    "            prediction.append(p)\n",
    "        return np.array(prediction)\n",
    "    \n",
    "    \n",
    "    def predict_one(self, x):\n",
    "        current_min = float('inf')\n",
    "        current_argmin = 0\n",
    "        for i in range(len(self.x)):\n",
    "            dist = NearestNeighbourClassifier.euclidean_distance(x, self.x[i])\n",
    "            if dist < current_min:\n",
    "                current_min = dist\n",
    "                current_argmin = i\n",
    "        return self.y[current_argmin]\n",
    "    \n",
    "    @staticmethod\n",
    "    def euclidean_distance(x1, x2):\n",
    "        return np.sqrt(np.sum(np.square(x1-x2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBpJ8EjVy-bi"
   },
   "outputs": [],
   "source": [
    "nn_classifier = NearestNeighbourClassifier()\n",
    "nn_classifier.fit(x_train, y_train)\n",
    "nn_predictions = nn_classifier.predict(x_test)\n",
    "print(nn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MC4ONOgF6u7c"
   },
   "source": [
    "## Evaluation (again)\n",
    "\n",
    "And let's evaluate our new nearest neighbour classifier. You should be able to achieve >90% accuracy. Not bad for such a simple classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWRWsYurz1om"
   },
   "outputs": [],
   "source": [
    "accuracy = compute_accuracy(y_test, nn_predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNMajcyH7Vv5"
   },
   "source": [
    "## Summary\n",
    "\n",
    "And congratulations! You will have constructed a full pipeline for a classification task, and have built and evaluated your first classifier! \n",
    "\n",
    "I hope that this tutorial have given you a practical understanding of the pipeline at implementation level. I hope you have also gained an appreciation on why it is important to examine your data and features before embarking on any ML project. Hopefully you have also brushed up your Python+NumPy skills while you are at it!\n",
    "\n",
    "We will be coming back to these experiments in later tutorials and construct more complex classifiers. Till then, happy learning!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqle7HFj1FyV"
   },
   "source": [
    "## Bonus activity (Optional)\n",
    "\n",
    "We concluded from our visualisations that perhaps you may only need the last two features (i.e. petal width and petal height) to construct our classifier. Let's test our intuition and see whether we can achieve a good enough accuracy with only the last two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPr1YEfJ3g8Z"
   },
   "outputs": [],
   "source": [
    "x_sub_train = x_train[:,-2:] # TODO: Extract only the last two columns of x_train\n",
    "x_sub_test = x_test[:,-2:] # TODO: Extract only the last two columns of x_test\n",
    "print(x_sub_train.shape)\n",
    "print(x_sub_test.shape)\n",
    "\n",
    "nn_classifier = NearestNeighbourClassifier()\n",
    "nn_classifier.fit(x_sub_train, y_train)\n",
    "nn_predictions = nn_classifier.predict(x_sub_test)\n",
    "print(nn_predictions)\n",
    "\n",
    "accuracy = compute_accuracy(y_test, nn_predictions)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNc3LZOS4qcA"
   },
   "source": [
    "I actually achieved a similar accuracy as before. Two of the predictions changed - one from correct to incorrect, the other from incorrect to correct. You can perhaps examine these feature vectors to try to figure out why?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "lab1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}